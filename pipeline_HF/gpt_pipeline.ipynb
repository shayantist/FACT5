{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pyPcREqUfSd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd3ce6d-9cc7-4346-9085-9751331b070d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For transformer models\n",
        "!pip install -q accelerate\n",
        "# !pip install -q bitsandbytes\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "# !pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "# For sentence similarity\n",
        "!pip install sentence_transformers\n",
        "\n",
        "# For web queries\n",
        "!pip install googlesearch-python\n",
        "\n",
        "# For Retrieval Augmentated Generation (RAG) since HF doesn't have great support for it\n",
        "!pip install langchain\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCUhIrM43cdB",
        "outputId": "98b29747-119c-49d8-be05-41606598c851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/21.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(\n",
        "    api_key = OPENAI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "xON-OssXfjFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_atomization_template = \"\"\"\n",
        "You are a helpful assistant. Your task is to break down a set of statements given after <<<>>> into a minimal number of atomic claims.\n",
        "These atomic claims need to be comprehensible, coherent, and context-independent.\n",
        "\n",
        "Segmentation Criteria:\n",
        "1. Each atomic claim should focus on a single idea or concept regarding the truthfulness and/or plausibility of the statement.\n",
        "   - e.g., Gen Z is divided 50-50 on the \"issue\" -> specify what \"issue\" is divisive\n",
        "2. Atomic claims should be independent of each other and not rely heavily on the context of the original statement.\n",
        "3. Aim for clarity and coherence in the segmented atomic claims.\n",
        "4. If a statement cannot be broken down further, return the entire statement as one atomic claim.\n",
        "\n",
        "###\n",
        "Here are some examples:\n",
        "Statements: The Green New Deal proposed by Rep. Alexandria Ocasio-Cortez aims to achieve net-zero greenhouse gas emissions and create millions of green jobs.\n",
        "Atomic Claims: ['The Green New Deal was proposed by Rep. Alexandria Ocasio-Cortez.', 'The Green New Deal aims to achieve net-zero greenhouse gas emissions.', 'The Green New Deal aims to create millions of green jobs.']\n",
        "Statements: The Inflation Reduction Act, signed into law by President Biden in August 2022, is the largest investment in climate change mitigation in U.S. history.\n",
        "Atomic Claims: ['The Inflation Reduction Act was signed into law by President Biden in August 2022.', 'The Inflation Reduction Act is the largest investment in climate change mitigation in U.S. history.']\n",
        "Statements: President Trump's policies led to the highest inflation rate in over 40 years, resulting in economic hardship for millions of American families.\n",
        "Atomic Claims: [\"President Trump's policies led to the highest inflation rate in over 40 years.\", \"The high inflation rate caused by President Trump's policies resulted in economic hardship for millions of American families.\"]\n",
        "###\n",
        "\n",
        "You will only respond with the atomic claims in the format of a single, one-dimensional list square-brackets of string objects.\n",
        "Do not provide any explanations or notes.\n",
        "\n",
        "<<<\n",
        "Statements: {statements}\n",
        ">>>\n",
        "Atomic Claims: []\"\"\"\n",
        "\n",
        "def generate_atomic_claims(statements, num_examples=3):\n",
        "  \"\"\"\n",
        "  Generates atomic claims for the input statements.\n",
        "\n",
        "  Args:\n",
        "      claim (str): The input statements.\n",
        "      num_examples (int, optional): The number of few-shot examples to include in the prompt. Defaults to 3.\n",
        "\n",
        "  Returns:\n",
        "      str: The generated atomic claims.\n",
        "  \"\"\"\n",
        "  if num_examples > 0: # Populate the prompt with few-shot examples (w/ proper formatting)\n",
        "      examples_text = \"\"\n",
        "      best_examples = select_best_examples(statements, examples[\"claim_atomization_examples\"], \"statement\", num_examples)\n",
        "\n",
        "      # Add each example to the prompt\n",
        "      for example in best_examples:\n",
        "          examples_text += f\"Statements: {example['statement']}\\n\"\n",
        "          examples_text += f\"Atomic Claims: {example['atomic_claims']}\\n\"\n",
        "\n",
        "      # Finally, fill in the prompt template with the examples and the input statements\n",
        "      prompt = claim_atomization_template.format(examples=examples_text, statements=statements)\n",
        "  else: # Otherwise leave the examples section of the prompt template blank and only include the input statements\n",
        "      prompt = claim_atomization_template.format(examples=\"\", statements=statements)\n",
        "\n",
        "  #\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\",\n",
        "      \"content\": \"You are a helpful assistant dividing up a statement into independent claims.\"}, # type task\n",
        "      {\"role\": \"user\",\n",
        "      \"content\": prompt} # prompt\n",
        "    ],\n",
        "    temperature = 0\n",
        "  )\n",
        "  # Decode the generated text\n",
        "  output_text = response.choices[0].message.content\n",
        "  #return output_text\n",
        "  # Extract only the list of claims from the model's output\n",
        "  try:\n",
        "      # Assuming output format directly returns Python list\n",
        "      atomic_claims = eval(output_text.split('Atomic Claims:')[-1].strip())\n",
        "      # POST-PROCESSING ERROR HANDLING: If list contains lists, return a flattened list\n",
        "      if isinstance(atomic_claims[0], list):\n",
        "          atomic_claims = [item for sublist in atomic_claims for item in sublist]\n",
        "      return atomic_claims\n",
        "  except:\n",
        "      print(f\"Error parsing model output: {output_text}\")\n",
        "      return [\"Error parsing model output\"]\n",
        "\n",
        "\n",
        "statements = '''\n",
        "\"In New York, there are no barriers to law enforcement to work with the federal government on immigration laws, and there are 100 crimes where migrants can be handed over.\"\n",
        "'''\n",
        "atomic_claims = generate_atomic_claims(statements, 0)\n",
        "print(f'statement: {statements}')\n",
        "print(f'atomic claims: {atomic_claims}')\n"
      ],
      "metadata": {
        "id": "efOMk3zSljAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question Generation\n",
        "\n",
        "# Define prompt template\n",
        "question_generation_template = \"\"\"\n",
        "You are a helpful assistant. Let's break down the claim to questions step by step.\n",
        "Your task is to provide a set of unique, independent questions to search on the web to verify the claim given after <<<>>>.\n",
        "\n",
        "Question generation criteria:\n",
        "1. Each question should be related to the claim, context-independent, and be understood without access to the claim.\n",
        "2. Each question should be able to be fact-checked by a True/False.\n",
        "3. Be as specific and concise as possible. Try to minimize the number of questions.\n",
        "4. Include enough details (e.g., pronoun specification, pronoun disambiguation) to ensure that the claim can be verified.\n",
        "\n",
        "###\n",
        "Here are some examples:\n",
        "Claim: The high inflation rate caused by President Trump's policies resulted in economic hardship for millions of American families.\n",
        "Questions: [\"How did the high inflation rate under President Trump's policies affect American families?\", \"What economic challenges did millions of American families face due to the inflation rate under President Trump's policies?\", 'What are the consequences of economic hardship caused by inflation under President Trump?']\n",
        "Claim: President Trump's policies led to the highest inflation rate in over 40 years.\n",
        "Questions: [\"What impact did President Trump's policies have on the inflation rate?\", \"How does the inflation rate during President Trump's term compare to previous years?\", \"What is the significance of the inflation rate under President Trump's policies?\"]\n",
        "###\n",
        "\n",
        "You will only respond with the generated questions in the format of a single, one-dimensional list in square-brackets. Do not provide any explanations or notes.\n",
        "\n",
        "###\n",
        "Here are some examples:\n",
        "{examples}###\n",
        "\n",
        "<<<\n",
        "Claim: {claim}\n",
        ">>>\n",
        "Questions: []\"\"\"\n",
        "\n",
        "def generate_questions(claim, num_examples=3):\n",
        "    \"\"\"\n",
        "    Generates questions to verify the factuality of the input claim.\n",
        "\n",
        "    Args:\n",
        "        claim (str): The input claim.\n",
        "        num_examples (int, optional): The number of few-shot examples to include in the prompt. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated questions.\n",
        "    \"\"\"\n",
        "    if num_examples > 0: # Populate the prompt with few-shot examples (w/ proper formatting)\n",
        "        examples_text = \"\"\n",
        "        best_examples = select_best_examples(claim, examples[\"question_generation_examples\"], \"claim\", num_examples)\n",
        "\n",
        "        # Add each example to the prompt\n",
        "        for example in best_examples:\n",
        "            examples_text += f\"Claim: {example['claim']}\\n\"\n",
        "            examples_text += f\"Questions: {example['questions']}\\n\"\n",
        "\n",
        "        # Finally, fill in the prompt template with the examples and the input claim\n",
        "        prompt = question_generation_template.format(examples=examples_text, claim=claim)\n",
        "    else: # Otherwise leave the examples section of the prompt template blank and only include the input claim\n",
        "        prompt = question_generation_template.format(examples=\"\", claim=claim)\n",
        "\n",
        "    # Print the entire prompt for debugging purposes\n",
        "    # print(prompt)\n",
        "\n",
        "    # gpt output\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\",\n",
        "      \"content\": \"You are a helpful assistant dividing up a statement into independent claims.\"}, # type task\n",
        "      {\"role\": \"user\",\n",
        "      \"content\": prompt} # prompt\n",
        "    ],\n",
        "    temperature = 0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    output_text = response.choices[0].message.content\n",
        "\n",
        "    # Extract only the list of questions from the model's output\n",
        "    try:\n",
        "        # Assuming output format directly returns Python list\n",
        "        questions = eval(output_text.split('Questions:')[-1].strip())\n",
        "        return questions\n",
        "    except:\n",
        "        print(f\"Error parsing model output: {output_text}\")\n",
        "        return [\"Error parsing model output\"]\n",
        "\n",
        "# Example usage for question generation\n",
        "claim_question = dict()\n",
        "for i, claim in enumerate(atomic_claims):\n",
        "  questions = generate_questions(claim, 0)\n",
        "  claim_question[claim] = questions\n",
        "  print(f\"Claim: {claim}\")\n",
        "  print(f\"Questions: {claim_question[claim]}\")\n"
      ],
      "metadata": {
        "id": "iqsh52A1tqOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Web Querying & Scraping\n",
        "import json\n",
        "import requests\n",
        "import pprint\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_website_name(title):\n",
        "    \"\"\"Extracts the website name from a given title using regex\"\"\"\n",
        "    delimiters = [' \\| ', ' - ']\n",
        "    for delimiter in delimiters:\n",
        "        parts = re.split(delimiter, title)\n",
        "        if len(parts) > 1:\n",
        "            return parts[-1].strip()  # Return the last part of the split title\n",
        "    # If no delimiters are found, return the original title\n",
        "    return title.strip()\n",
        "\n",
        "def scrape_text_from_website(url):\n",
        "    \"\"\"Scrapes text and metadata from a given website URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove script and style tags\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            # Extract all text from the website\n",
        "            text = soup.get_text()\n",
        "\n",
        "            # Clean up whitespace\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to retrieve content from the URL: {url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during website scraping: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_search_results(question, scrape_website=False):\n",
        "    \"\"\"\n",
        "    Fetches search results for a given question using an API.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to search for.\n",
        "        scrape_website (bool, optional): Whether to scrape the website content. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of organic search results.\n",
        "    \"\"\"\n",
        "    api_key = \"e9b35305c3b0a79189b7c2dc4c37adbc587d1e65\"  # Replace with your actual API key\n",
        "\n",
        "    headers = {\n",
        "        \"X-API-KEY\": api_key,\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    payload = json.dumps({\"q\": question})\n",
        "    try:\n",
        "        response = requests.post(\"https://google.serper.dev/search\", headers=headers, data=payload)\n",
        "        result = json.loads(response.text)\n",
        "\n",
        "        # Extract the organic search results and transform them into our desired format\n",
        "        results = []\n",
        "        for item in result['organic']:\n",
        "            results.append({\n",
        "                \"title\": item.get('title', ''),\n",
        "                \"source_title\": extract_website_name(item.get('title', '')),\n",
        "                \"date_published\": item.get('date', ''),\n",
        "                \"relevant_excerpt\": item.get('snippet', ''),\n",
        "                \"text\": scrape_text_from_website(item.get('link', '')) if scrape_website else '',\n",
        "                \"search_position\": item.get('position', -1),\n",
        "                \"url\": item.get('link', ''),\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch information: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage:\n",
        "question = question = '''\n",
        "In New York, are there barriers to law enforcement to work with the federal government on immigration laws?\n",
        "'''\n",
        "search_results = fetch_search_results(question)\n",
        "search_results\n",
        "\n",
        "# Pretty print the results\n",
        "# pprint.pprint(search_results)"
      ],
      "metadata": {
        "id": "aCYr74RW3kul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RAG Retriever\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "\n",
        "import copy\n",
        "\n",
        "# Initialize embedding model for retrieval (sentence similarity)\n",
        "BATCH_SIZE = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "retriever_model_id='sentence-transformers/all-MiniLM-L6-v2'\n",
        "retriever_model = HuggingFaceEmbeddings(\n",
        "    model_name=retriever_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': BATCH_SIZE},\n",
        ")\n",
        "\n",
        "def retrieve_relevant_documents_using_rag(search_results, content_key, question, chunk_size=512, chunk_overlap=128, top_k=10):\n",
        "    \"\"\"\n",
        "    Takes in search results and a query question, processes and splits the documents,\n",
        "    and retrieves relevant documents using a RAG approach.\n",
        "\n",
        "    Args:\n",
        "        search_results (list of dict): A list of dictionaries containing web-scraped data.\n",
        "        question (str): The query question for retrieving relevant documents.\n",
        "        chunk_size (int): The maximum size of the text chunks.\n",
        "        chunk_overlap (int): The overlap between consecutive text chunks.\n",
        "        content_key (str): The key in the dictionary containing the text content.\n",
        "        top_k (int): The number of relevant documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of relevant document chunks.\n",
        "    \"\"\"\n",
        "    # Create LangChain documents from search results\n",
        "    documents = []\n",
        "    for result in search_results:\n",
        "        page_content = result.pop(content_key, None)  # Extract the text content, remaining keys are metadata\n",
        "        if page_content is not None:\n",
        "            documents.append(Document(page_content=page_content, metadata=result))\n",
        "\n",
        "    # Split documents into smaller chunks (if needed, based on document size)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Initialize ChromaDB vector store to index the document chunks\n",
        "    db = Chroma.from_documents(\n",
        "        documents=split_documents,\n",
        "        embedding=retriever_model,\n",
        "    )\n",
        "\n",
        "    # Retrieve the most relevant chunks for the given question\n",
        "    relevant_docs = db.max_marginal_relevance_search(question, k=top_k)\n",
        "\n",
        "    return relevant_docs\n",
        "\n",
        "question = '''\n",
        "In New York, are there barriers to law enforcement to work with the federal government on immigration laws?\n",
        "'''\n",
        "relevant_docs = retrieve_relevant_documents_using_rag(copy.deepcopy(search_results), 'relevant_excerpt', question)\n",
        "relevant_docs"
      ],
      "metadata": {
        "id": "Y0wLmC-73F5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs"
      ],
      "metadata": {
        "id": "08oEQ97jGaXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RAG-based Question Answering\n",
        "\n",
        "# Define prompt template\n",
        "answer_synthesis_template = \"\"\"\n",
        "You are a helpful assistant. Let's think step by step. Your task is to synthesize the documents (along with their source metadata) provided below to answer the question given after <<<>>>.\n",
        "\n",
        "Answer criteria:\n",
        "1. Start your output with \"Answer:\"\n",
        "2  Only use the documents below to answer the question.\n",
        "3. Cite the relevant documents as JSON (including the source URL) after your answer starting with \"Sources:\"\n",
        "\n",
        "If you cannot answer the question given the relevant documents, just say that you don't have enough information to answer the question. Do not make up an answer or sources.\n",
        "\n",
        "Here are the relevant documents:\n",
        "{documents}\n",
        "\n",
        "<<<\n",
        "Question: {question}\n",
        ">>>\n",
        "Answer: \"\"\"\n",
        "\n",
        "def synthesize_answer(relevant_docs, question, return_sources=True):\n",
        "    \"\"\"\n",
        "    Synthesizes an answer to a given question using the relevant documents.\n",
        "\n",
        "    Args:\n",
        "        relevant_docs (list of dict): A list of relevant document chunks.\n",
        "        question (str): The question to answer.\n",
        "\n",
        "    Returns:\n",
        "        str: The synthesized answer.\n",
        "    \"\"\"\n",
        "    # Format the relevant documents for the prompt\n",
        "    documents_text = \"\"\n",
        "    for doc in relevant_docs:\n",
        "        documents_text += f\"Title: {doc.metadata.get('source_title', '')}\\n\"\n",
        "        documents_text += f\"URL: {doc.metadata.get('url', '')}\\n\"\n",
        "        documents_text += f\"Text: {doc.page_content.strip()}\\n\\n\"\n",
        "        documents_text += f\"Date Published: {doc.metadata.get('date_published', '')}\\n\"\n",
        "\n",
        "    # Fill in the prompt template with the relevant documents and the question\n",
        "    prompt = answer_synthesis_template.format(documents=documents_text, question=question)\n",
        "    # print('*'*20)\n",
        "    # print(documents_text)\n",
        "    # print('*'*20)\n",
        "    prompt = prompt.replace('\\n\\n', '\\n')\n",
        "\n",
        "    # Print the entire prompt for debugging purposes\n",
        "    # print(prompt)\n",
        "    # print('-'*20)\n",
        "\n",
        "    # gpt output\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\",\n",
        "      \"content\": \"You are a helpful assistant dividing up a statement into independent claims.\"}, # type task\n",
        "      {\"role\": \"user\",\n",
        "      \"content\": prompt} # prompt\n",
        "    ],\n",
        "    temperature = 0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    output_text = response.choices[0].message.content\n",
        "    # Extract only the answer from the model's output\n",
        "    try:\n",
        "        answer = output_text.split('Answer:')[-1].split('Sources:')[0].strip()\n",
        "        sources = output_text.split('Sources:')[-1].strip()\n",
        "        if return_sources: return answer, sources\n",
        "        return answer\n",
        "    except:\n",
        "        print(f\"Error parsing model output: {output_text}\")\n",
        "        return \"Error parsing model output\""
      ],
      "metadata": {
        "id": "6gOyIez85oP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Claim Classification\n",
        "\n",
        "# Define prompt template for reasoning and classification\n",
        "# 1. -- from CRITIC\n",
        "claim_classification_template = \"\"\"\n",
        "You are a logical reasoning assistant.\n",
        "\n",
        "Given the original claim, a set of questions to help verify the claim, and their answers, reason step-by-step to come to a verdict on whether the claim is true or false. Think step-by-step about your reasoning process.\n",
        "Return the verdict after \"Verdict:\" and provide a clear explanation after \"Reasoning:\"\n",
        "For the verdict, only classify the claim as \"True\", \"False\", or \"Unverifiable.\"\n",
        "\n",
        "Reasoning Criteria:\n",
        "1. Reason the claim over both plausibility and truthfulness.\n",
        "2. Make sure to only reference to the question and answer pairs for your explanations.\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "{questions_and_answers}\n",
        "\n",
        "Verdict: \"\"\"\n",
        "\n",
        "def classify_claim(claim, questions, answers, return_reasoning=True):\n",
        "    \"\"\"\n",
        "    Uses a chain-of-thought approach to classify the original claim as true or false based on the answers to generated questions.\n",
        "\n",
        "    Args:\n",
        "        claim (str): The original claim.\n",
        "        questions (list): List of questions related to the claim.\n",
        "        answers (list): List of answers corresponding to the questions.\n",
        "\n",
        "    Returns:\n",
        "        str: The conclusion whether the claim is true or false with reasoning.\n",
        "    \"\"\"\n",
        "    # Format the questions and answers into a single string\n",
        "    questions_and_answers = \"\"\n",
        "    for question, answer in zip(questions, answers):\n",
        "        questions_and_answers += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "\n",
        "    # Fill in the prompt template with the claim and formatted questions and answers\n",
        "    prompt = claim_classification_template.format(claim=claim, questions_and_answers=questions_and_answers)\n",
        "\n",
        "    # Print the entire prompt for debugging purposes\n",
        "    # print(prompt)\n",
        "\n",
        "    # gpt output\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\",\n",
        "      \"content\": \"You are a helpful assistant dividing up a statement into independent claims.\"}, # type task\n",
        "      {\"role\": \"user\",\n",
        "      \"content\": prompt} # prompt\n",
        "    ],\n",
        "    temperature = 0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    output_text = response.choices[0].message.content\n",
        "\n",
        "    # Extract the verdict and reasoning separately from the model's output\n",
        "    try:\n",
        "        verdict = output_text.split('Verdict:')[-1].split('Reasoning:')[0].strip()\n",
        "        reasoning = output_text.split('Reasoning:')[-1].strip()\n",
        "        if return_reasoning: return verdict, reasoning\n",
        "        return verdict\n",
        "    except:\n",
        "        raise ValueError(f\"Error parsing model output: {output_text}\")\n",
        "\n",
        "    return output_text, verdict"
      ],
      "metadata": {
        "id": "X_bLNrBT6NaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "def generate_fact_score(verdicts):\n",
        "    label = None\n",
        "    perc_unverified = 0\n",
        "    v_cleaned = verdicts\n",
        "    if 'Unverifiable' in verdicts:\n",
        "        v_cleaned = verdicts[:]\n",
        "        v_cleaned.remove('Unverifiable')\n",
        "        perc_unverified = Counter(verdicts)['Unverifiable'] / len(verdicts)\n",
        "    perc_true = Counter(verdicts)['True'] / len(verdicts)\n",
        "    perc_false = Counter(verdicts)['False'] / len(verdicts)\n",
        "    perc = [perc_true, perc_false, perc_unverified]\n",
        "    winner = np.argwhere(perc == np.amax(perc))\n",
        "    # print(perc, winner)\n",
        "\n",
        "    if len(winner) == 3: # three-way tie\n",
        "        label = \"Unverifiable\"\n",
        "\n",
        "    elif len(winner) == 2: # two-way tie\n",
        "        if 0 in winner and 1 in winner: # half true\n",
        "            label = 'Half True'\n",
        "        elif 0 in winner and 2 in winner: # true & unverifable\n",
        "            label = \"Unverifiable\"\n",
        "        elif 1 in winner and 2 in winner: # false & unverifable\n",
        "            label = \"Unverifiable\"\n",
        "\n",
        "    elif len(winner) == 1:\n",
        "        if 0 in winner:\n",
        "            if perc_true == 1: # all true\n",
        "                label = \"True\"\n",
        "            elif Counter(v_cleaned)['True'] / len(v_cleaned) > 0.5: # mostly true\n",
        "                label = \"Mostly True\"\n",
        "        elif 1 in winner:\n",
        "            if perc_false == 1: # all false\n",
        "                label = \"Pants on Fire\"\n",
        "            elif Counter(v_cleaned)['False'] / len(v_cleaned) > 0.5: # mostly false\n",
        "                label = \"Mostly False\"\n",
        "        elif 2 in winner:\n",
        "            label = 'Unverifiable'\n",
        "    return label"
      ],
      "metadata": {
        "id": "tlX892CznuzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_statement_gpt(statement):\n",
        "    atomic_claims = generate_atomic_claims(statement,0)\n",
        "    print(len(atomic_claims))\n",
        "    output_dict = []\n",
        "    verdicts = []\n",
        "    for i, claim in enumerate(atomic_claims):\n",
        "        claim_output = {}\n",
        "        claim_output['claim'] = claim\n",
        "        questions = generate_questions(claim,0)\n",
        "        claim_output['qa-pairs'] = {}\n",
        "        claim_output['qa-pairs']['questions'] = questions\n",
        "        answers = []\n",
        "        sources = []\n",
        "        for j, question in enumerate(questions):\n",
        "            print(question)\n",
        "            search_results = fetch_search_results(question,0)\n",
        "            relevant_docs = retrieve_relevant_documents_using_rag(search_results, 'relevant_excerpt', question)\n",
        "            answer, source = synthesize_answer(relevant_docs, question)\n",
        "            # output[f'claim_{i}']['questions'][f'answer_{j}'] = question\n",
        "            answers.append(answer)\n",
        "            sources.append(source)\n",
        "        claim_output['qa-pairs']['answers'] = answers\n",
        "        claim_output['qa-pairs']['sources'] = sources\n",
        "        verdict, reasoning = classify_claim(claim, questions, answers)\n",
        "        verdicts.append(verdict)\n",
        "        claim_output['verdict'] = verdict\n",
        "        claim_output['reasoning'] = reasoning\n",
        "        output_dict.append(claim_output)\n",
        "    return verdicts, output_dict, generate_fact_score(verdicts),"
      ],
      "metadata": {
        "id": "PGQWeZyfCnC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/courses/AY 23-24/24 SP/COMS-W3997 LLM Foundations & Ethics/LLM Project/Code/data/pilot.csv'\n",
        "import pandas as pd\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "A8yjCIO97JEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['statement'] = df['statement'].astype(str)"
      ],
      "metadata": {
        "id": "Gxdtk_1q72SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check_samples = []\n",
        "for i in df['statement'][:10]:\n",
        "    fact_check_samples.append(verify_statement_gpt(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9_keiIR2qIya",
        "outputId": "139bd4a4-a591-4bf5-fb9c-98d147197315"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "What percentage of Generation Z supports Hamas?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the support for Hamas among Generation Z evenly split?\n",
            "What percentage of Generation Z supports Israel?\n",
            "Is the support for Israel among Generation Z evenly split?\n",
            "1\n",
            "Has the trade deficit with China increased during President Trump's tenure?\n",
            "What was the trend in the trade deficit between the US and China during President Trump's administration?\n",
            "3\n",
            "What are the details of the new abortion ban affecting millions of Arizonans?\n",
            "How will the new abortion ban in Arizona impact the lives of its residents?\n",
            "What makes the new abortion ban in Arizona considered extreme and dangerous?\n",
            "Does the abortion ban provide protections for women when their health is at risk?\n",
            "What are the health risks for women under the abortion ban?\n",
            "How does the abortion ban address situations where a woman's health is endangered?\n",
            "Does the abortion ban provide protections for women in cases of rape or incest?\n",
            "What are the implications of the abortion ban on women involved in tragic cases of rape or incest?\n",
            "Error parsing model output: ```python\n",
            "[\"After a 2022 law, the vast majority of colleges in New York State do not have on-campus poll sites.\"]\n",
            "```\n",
            "1\n",
            "2\n",
            "Has the number of voters registering without a photo ID increased in Arizona according to Social Security Administration data?\n",
            "Is there a rise in voters registering without a photo ID in Texas as per Social Security Administration data?\n",
            "Does Social Security Administration data indicate an increase in voters registering without a photo ID in Pennsylvania?\n",
            "Has there been an increase in voters registering without a photo ID in Arizona, Texas, and Pennsylvania?\n",
            "Is there evidence that migrants who entered the United States illegally are registering to vote in Arizona, Texas, and Pennsylvania?\n",
            "2\n",
            "Did immigrants in the U.S. illegally storm New York City Hall on a Tuesday?\n",
            "What incidents involving immigrants storming New York City Hall have been reported?\n",
            "Did immigrants demand housing in luxury hotels instead of city shelters?\n",
            "Error parsing model output: ```python\n",
            "[\n",
            "    \"This year, the typical family’s tax bill is thousands of dollars lower because of the Trump Tax Cuts.\",\n",
            "    \"The Trump Tax Cuts doubled the Standard Deduction.\",\n",
            "    \"The Trump Tax Cuts doubled the child tax cuts.\",\n",
            "    \"The Trump Tax Cuts lowered income tax rates for everyone.\",\n",
            "    \"Joe Biden plans to implement colossal tax hikes.\"\n",
            "]\n",
            "```\n",
            "1\n",
            "Error parsing model output: ```python\n",
            "[]\n",
            "```\n",
            "Error parsing model output\n",
            "3\n",
            "Is there a global decline in women's fertility rates?\n",
            "What are the current trends in women's fertility worldwide?\n",
            "What is the current state of reproductive health?\n",
            "Is there a crisis in reproductive health?\n",
            "How is the crisis in reproductive health related to the epidemic of chronic disease?\n",
            "What evidence links reproductive health issues to chronic diseases?\n",
            "Error parsing model output: ```python\n",
            "[\"Billionaires pay an average federal tax rate of about 8% less than a teacher.\", \"Billionaires pay an average federal tax rate of about 8% less than a firefighter.\", \"Billionaires pay an average federal tax rate of about 8% less than a sanitation worker.\", \"Billionaires pay an average federal tax rate of about 8% less than a nurse.\"]\n",
            "```\n",
            "1\n",
            "2\n",
            "How many Americans are protected from being denied health insurance due to preexisting conditions under the Affordable Care Act?\n",
            "What provisions does the Affordable Care Act have regarding preexisting conditions?\n",
            "Does Trump want to repeal the Affordable Care Act?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1838bcfb1453>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfact_check_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfact_check_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverify_statement_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-ebcafde47713>\u001b[0m in \u001b[0;36mverify_statement_gpt\u001b[0;34m(statement)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_relevant_documents_using_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relevant_excerpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthesize_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-85c5b7d429ac>\u001b[0m in \u001b[0;36mfetch_search_results\u001b[0;34m(question, scrape_website)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://google.serper.dev/search\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4_sample = pd.DataFrame(data = {\n",
        "    \"verdicts\": [x[0] for x in fact_check_samples],\n",
        "    \"fact_score\": [x[2] for x in fact_check_samples],\n",
        "    \"output\": [x[1] for x in fact_check_samples]})"
      ],
      "metadata": {
        "id": "riVE7-XZshnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4_sample.to_excel('/content/drive/MyDrive/courses/AY 23-24/24 SP/COMS-W3997 LLM Foundations & Ethics/LLM Project/gpt4-samples.xlsx')"
      ],
      "metadata": {
        "id": "kNRmJd1Jsxj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}