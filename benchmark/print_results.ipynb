{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../pipeline_v2/')\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(seed, model, num_samples=50):\n",
    "    if os.path.exists(f'results_v2_{model}.pkl'):\n",
    "        df = pd.read_pickle(f'results_v2_{model}.pkl')\n",
    "    elif os.path.exists(f'./benchmark/results_v2_{model}.pkl'):\n",
    "        df = pd.read_pickle(f'./benchmark/results_v2_{model}.pkl')\n",
    "    else:\n",
    "        raise ValueError(f\"results_v2_{model}.pkl not found\")\n",
    "\n",
    "    sample_df = df.sample(num_samples, random_state=seed).reset_index(drop=True)\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model = 'gemini'):\n",
    "    texts = []\n",
    "    sample_df = load_data(42, model)\n",
    "    for i in range(len(sample_df)):\n",
    "        text_printed = \"\"\"\n",
    "    # Instructions\n",
    "    ## üìù For Each Statement\n",
    "    ### Step 1: Review the Analysis\n",
    "    - ‚úÖ Check the LLM's verdict\n",
    "    - üí≠ Examine reasoning provided\n",
    "    - üìë If needed, expand individual claims below to examine evidence\n",
    "\n",
    "    ### Step 2: Rate Agreement Level\n",
    "    Choose one:\n",
    "    - üåü **STRONGLY AGREE**: Perfect verdict & reasoning\n",
    "    - ‚úÖ **AGREE**: Mostly correct analysis\n",
    "    - ‚ö†Ô∏è **DISAGREE**: Significant issues found\n",
    "    - ‚ùå **STRONGLY DISAGREE**: Completely incorrect\n",
    "\n",
    "    ### Step 3: If Disagreeing, Select Why\n",
    "    - üîç **IRRELEVANT/INCORRECT EVIDENCE**: Wrong evidence retrieved\n",
    "    - ü§î **INCORRECT ANALYSIS**: Evidence interpreted incorrectly\n",
    "\n",
    "    ### RESULTS:\n",
    "        \"\"\"\n",
    "        row = sample_df.iloc[i]\n",
    "        results = row[f'{model}_pipeline_results']\n",
    "        result = results[0]\n",
    "        claims = result['claims']\n",
    "        statement_text = f\"On {row['statement_date']}, {row['statement_originator']} claimed: {row['statement']}\"\n",
    "        reasoning = result['reasoning'].replace('\\n', ' ')\n",
    "        \n",
    "        text_printed += f\"Statement Evaluation {i + 1}\\n\"\n",
    "        text_printed += f\"Statement: {statement_text}\\n\"\n",
    "        text_printed += f\"Overall Verdict: {row['pipeline_pass3_verdict']}\\n\"\n",
    "        text_printed += f\"Overall Confidence: {result['confidence']}\\n\"\n",
    "        text_printed += f\"Overall Reasoning: {reasoning}\\n\\n\"\n",
    "\n",
    "        text_printed += f\"Claims Extracted & Independently Verified:\\n\"\n",
    "\n",
    "        for i, claim in enumerate(claims):\n",
    "            text_printed += f\"Claim {i + 1}/{len(claims)}: {claim.text}\\n\"\n",
    "            text_printed += f\"Verdict: {claim.verdict}\\n\"\n",
    "            text_printed += f\"Confidence: {claim.confidence}\\n\"\n",
    "            text_printed += f\"Reasoning: {claim.reasoning}\\n\"\n",
    "\n",
    "            for component in claim.components:\n",
    "                answer_text = component.answer.text\n",
    "                answer_text = answer_text.replace('\\n', ' ')\n",
    "                text_printed += f\"Question: {component.question}\\n\"\n",
    "                text_printed += f\"Answer: {component.answer.text}\\n\"\n",
    "\n",
    "                if component.answer.citations:\n",
    "                    text_printed += f\"Explicit citations by the model:\\n\"\n",
    "                    for j, citation in enumerate(component.answer.citations, 1):\n",
    "                        if citation: \n",
    "                            text_printed += f\"[{j}] {citation.snippet}\\n\"\n",
    "                            text_printed += f\"{citation.source_title}  - {citation.source_url}\\n\"\n",
    "                \n",
    "                text_printed += '\\n'\n",
    "                if component.answer.retrieved_docs:\n",
    "                    text_printed += f\"Documents used to synthesize answer (implicit citations):\\n\"\n",
    "                    for k, doc in enumerate(component.answer.retrieved_docs, 1):\n",
    "                        text_printed += f\"[{k}] {doc.content if doc.content else ''}\\n\"\n",
    "                        text_printed += f\"{doc.metadata.get('title', '') if doc.metadata else ''} - {doc.metadata.get('url', '') if doc.metadata else ''}\\n\"\n",
    "                text_printed += '\\n'\n",
    "        text_printed += \"\"\"Return a Python list containing three things and NOTHING Else: \n",
    "        (1) agreement level (one of the following: STRONGLY AGREE, AGREE, DISAGREE, STRONGLY DISAGREE), \n",
    "        (2) if disagreeing, select why (IRRELEVANT/INCORRECT EVIDENCE, INCORRECT ANALYSIS, or None)\n",
    "        (3) other elaboration, reasoning, or comments\"\"\"\n",
    "        texts.append(text_printed)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/3ph_k68x1kn9trwd194mr64m0000gn/T/ipykernel_58384/3116672654.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "# prompt = prompt_template.format(context=None, question=query_text)\n",
    "model = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113859378a694df6b26ab09cb90ef372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "gemini_texts = generate_text(model = 'gemini')\n",
    "gemini_evals = []\n",
    "for i in tqdm(range(len(gemini_texts))):\n",
    "    response_text = model.invoke(gemini_texts[i])\n",
    "    gemini_evals.append(response_text)\n",
    "    with open(\"llama_eval_gemini.pkl\", 'wb') as f:\n",
    "        pickle.dump(gemini_evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee99f2c3742644a7a36baaafd2a68001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "mistral_texts = generate_text(model = 'mistral')\n",
    "mistral_evals = []\n",
    "for i in tqdm(range(len(mistral_texts))):\n",
    "    response_text = model.invoke(mistral_texts[i])\n",
    "    mistral_evals.append(response_text)\n",
    "    with open(\"llama_eval_mistral.pkl\", 'wb') as f:\n",
    "        pickle.dump(mistral_evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import ast\n",
    "with open('llama_eval_gemini.pkl', 'rb') as f:\n",
    "    gemini_copy = pickle.load(f)\n",
    "with open('llama_eval_mistral.pkl', 'rb') as f:\n",
    "    mistral_copy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama_eval_gemini.pkl', 'wb') as f:\n",
    "    pickle.dump(gemini_copy, f)\n",
    "with open('llama_eval_mistral.pkl', 'wb') as f:\n",
    "    pickle.dump(mistral_copy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at idx 2, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 3, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 5, invalid syntax. Perhaps you forgot a comma? (<unknown>, line 2)\n",
      "Error at idx 10, unterminated string literal (detected at line 4) (<unknown>, line 4)\n",
      "Error at idx 16, invalid syntax (<unknown>, line 0)\n",
      "Error at idx 29, invalid syntax (<unknown>, line 0)\n",
      "Error at idx 30, invalid syntax (<unknown>, line 0)\n",
      "Error at idx 33, unterminated string literal (detected at line 2) (<unknown>, line 2)\n",
      "Error at idx 39, invalid syntax (<unknown>, line 0)\n",
      "Error at idx 41, invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "Error at idx 44, invalid syntax (<unknown>, line 0)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "mistral_copy = mistral_evals[:]\n",
    "for i, m in enumerate(mistral_copy):\n",
    "    m = mistral_copy[i]\n",
    "    if type(m) == list:\n",
    "        continue\n",
    "    start = m.find('[')\n",
    "    end = m.find(']')+1\n",
    "    m = m[start:end]\n",
    "    # m = m.replace(\"['\",'[\"').replace(\"']\",'\"]').replace(\", '\", ', \"').replace(\"',\", '\",')\n",
    "    # m = m.replace(\"[\\n\",'[\"').replace(\"\\n]\",'\"]').replace(\",\\n\", '\",').replace(', ', ', \"')\n",
    "    try:\n",
    "        m = ast.literal_eval(m)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {i}, {e}\")\n",
    "    mistral_copy[i] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at idx 3, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 5, unterminated string literal (detected at line 2) (<unknown>, line 2)\n",
      "Error at idx 6, unterminated string literal (detected at line 2) (<unknown>, line 2)\n",
      "Error at idx 7, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 14, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 16, unterminated string literal (detected at line 2) (<unknown>, line 2)\n",
      "Error at idx 22, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 24, malformed node or string on line 2: <ast.Name object at 0x16a592fb0>\n",
      "Error at idx 28, unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "Error at idx 32, invalid syntax. Perhaps you forgot a comma? (<unknown>, line 2)\n",
      "Error at idx 44, invalid syntax (<unknown>, line 0)\n",
      "Error at idx 47, unterminated string literal (detected at line 1) (<unknown>, line 1)\n"
     ]
    }
   ],
   "source": [
    "gemini_copy = gemini_evals[:]\n",
    "for i, m in enumerate(gemini_copy):\n",
    "    m = gemini_copy[i]\n",
    "    start = m.find('[')\n",
    "    end = m.find(']')+1\n",
    "    m = m[start:end]\n",
    "    m = m.replace(\"['\",'[\"').replace(\"']\",'\"]').replace(\", '\", ', \"').replace(\"',\", '\",')\n",
    "    try:\n",
    "        m = ast.literal_eval(m)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {i}, {e}\")\n",
    "    gemini_copy[i] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_copy = gemini_evals[:]\n",
    "for i, m in enumerate(gemini_copy):\n",
    "    corrected = m\n",
    "    if m[0] != None:\n",
    "        continue\n",
    "    while type(corrected) != list or m[0] == None: \n",
    "        print(m)\n",
    "        print(f'index {i}: enter correction:')\n",
    "        x = input()\n",
    "        corrected = ast.literal_eval(x)\n",
    "    gemini_copy[i] = corrected\n",
    "    print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
