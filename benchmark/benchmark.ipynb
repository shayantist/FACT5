{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### install necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining labels\n",
    "class_labels = [\"unverifiable\", \"false\", \"mostly false\", \"half true\", \"mostly true\", \"true\"]\n",
    "class_2_index = {label: i for i, label in enumerate(class_labels)}\n",
    "index_2_class = {i: label for i, label in enumerate(class_labels)}\n",
    "num_labels = [class_2_index[label] for label in class_labels] # [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions to generate confusion matrix and metrics\n",
    "def generate_cm(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate the confusion matrix with the cost matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "def generate_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate the weighted F1 score\n",
    "    '''\n",
    "    weighted_precision = precision_score(y_true, y_pred, average='weighted', sample_weight=None, labels=num_labels)\n",
    "    weighted_recall = recall_score(y_true, y_pred, average='weighted', sample_weight=None, labels=num_labels)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted', sample_weight=None, labels=num_labels)\n",
    "    metrics = pd.DataFrame({'Precision': weighted_precision, 'Recall': weighted_recall, 'F1': weighted_f1}, index=class_labels)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the data\n",
    "df = pd.read_csv('../data/[FINAL] Pilot - Pilot Claims copy.csv')\n",
    "statements = df['statement'].to_list()\n",
    "statement_dates = df['statement_date'].to_list()\n",
    "statement_originators = df['statement_originator'].to_list()\n",
    "gold = df['verdict'].to_list()\n",
    "\n",
    "'''\n",
    "Creates a list of strings that contain the statement, source, and date in the following format:\n",
    "Statement: {statement}\n",
    "According to: {source}\n",
    "Date: {date}\n",
    "'''\n",
    "statements_agg = [f\"\"\"Statement: {statement} \\n According to: {source} \\n Date: {date}\"\"\" for statement, source, date in zip(statements, statement_originators, statement_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate results\n",
    "def generate_results(statements: list, \n",
    "                     old_file: str = None, \n",
    "                     new_file: str = None):\n",
    "    '''\n",
    "    generate and store results for a list of statements\n",
    "    make sure to:\n",
    "    (1) load the old results if they exist\n",
    "    (2) initialize the pipeline and define LM before running\n",
    "    (3) run %%capture to suppress output\n",
    "\n",
    "    '''\n",
    "    # load results if it exists\n",
    "    results = []\n",
    "    if os.path.exists(old_file):\n",
    "        with open(old_file, 'rb') as f:\n",
    "            \n",
    "            results = pickle.load(f)\n",
    "\n",
    "    for index, statement in enumerate(tqdm(statements)):\n",
    "        if len(results) <= index+1 and type(results[index]) != int:\n",
    "            continue\n",
    "        verdict = None\n",
    "\n",
    "        # retry 5 times if there is an error\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                verdict, confidence, reasoning, claims = pipeline.fact_check(statement)\n",
    "            except Exception as e:\n",
    "                print(f\"Error {e}: retrying for statement {index}, attempt {i+1}\")\n",
    "                continue \n",
    "            break   \n",
    "        \n",
    "        if verdict is None:\n",
    "            results.append(index)\n",
    "        else:\n",
    "            results.append((verdict, confidence, reasoning, claims))\n",
    "        with open(new_file, 'wb') as f:\n",
    "            pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate results dataframe\n",
    "def generate_results_df(results_filename: str,\n",
    "                        df_filename: str):\n",
    "    '''\n",
    "    convert results to a dataframe and save it\n",
    "    '''\n",
    "    if os.path.exists(results_filename):\n",
    "        with open(results_filename, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    results_df = pd.DataFrame(data=results, columns=['verdict', 'confidence', 'reasoning', 'claims'])\n",
    "    results_df.to_csv(df_filename)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### load fact-checking pipeline\n",
    "import dotenv\n",
    "import sys\n",
    "import dspy\n",
    "import os\n",
    "sys.path.append('../pipeline_v2/')\n",
    "import main\n",
    "dotenv.load_dotenv('../.env')\n",
    "\n",
    "# initialize search provider\n",
    "main.NUM_SEARCH_RESULTS = 10 # Number of search results to retrieve\n",
    "main.SCRAPE_TIMEOUT = 5 # Timeout for scraping a webpage (in seconds)\n",
    "search_provider = main.SearchProvider(provider=\"duckduckgo\")\n",
    "\n",
    "# initialize DSPy\n",
    "lm = dspy.LM('gemini/gemini-1.5-flash', api_key=os.getenv('GOOGLE_GEMINI_API_KEY'), cache=False)\n",
    "#lm = dspy.LM('ollama_chat/mistral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# initialize pipeline\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "main.VERBOSE = True # Print intermediate results\n",
    "main.INTERACTIVE = False # Allow the user to provide feedback\n",
    "main.USE_BM25 = True # Use BM25 for retrieval (in addition to cosine similarity)\n",
    "main.BM25_WEIGHT = 0.5 # Weight for BM25 in the hybrid retrieval\n",
    "\n",
    "pipeline = main.FactCheckPipeline(\n",
    "    search_provider=search_provider,\n",
    "    model_name=lm,\n",
    "    embedding_model=embedding_model,\n",
    "    retriever_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supported', 'unsupported']\n"
     ]
    }
   ],
   "source": [
    "print(main.VERDICTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import dotenv\n",
    "import sys\n",
    "import dspy\n",
    "import os\n",
    "sys.path.append('../pipeline_v2/')\n",
    "import main \n",
    "dotenv.load_dotenv('../.env')\n",
    "\n",
    "# Initialize search provider\n",
    "main.NUM_SEARCH_RESULTS = 10 # Number of search results to retrieve\n",
    "main.SCRAPE_TIMEOUT = 5 # Timeout for scraping a webpage (in seconds)\n",
    "search_provider = main.SearchProvider(provider=\"duckduckgo\")\n",
    "\n",
    "# Initialize DSPy\n",
    "lm = dspy.LM('ollama_chat/mistral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# Initialize pipeline\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "main.VERBOSE = False # Print intermediate results\n",
    "main.INTERACTIVE = False # Allow the user to provide feedback\n",
    "main.USE_BM25 = True # Use BM25 for retrieval (in addition to cosine similarity)\n",
    "main.BM25_WEIGHT = 0.5 # Weight for BM25 in the hybrid retrieval\n",
    "\n",
    "pipeline = main.FactCheckPipeline(\n",
    "    search_provider=search_provider,\n",
    "    model_name=lm,\n",
    "    embedding_model=embedding_model,\n",
    "    retriever_k=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolitiFact Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil.parser as dparser\n",
    "def scrape_politifact(url):\n",
    "    \"\"\"Scrapes text and metadata from a given website URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            article_element = soup.find('article', class_='m-textblock')\n",
    "            statement = soup.find('div', class_='m-statement__quote').get_text()\n",
    "            statement = re.sub(r'\\s+', ' ', statement).strip()\n",
    "\n",
    "            verdict = soup.find('div', class_='m-statement__meter').find('img').get('alt').upper().replace('-',' ')\n",
    "            if verdict == 'BARELY TRUE':\n",
    "                verdict = 'MOSTLY FALSE'\n",
    "            author = soup.find('div',class_=\"m-statement__author\")\n",
    "            statement_originator = author.find('a',class_='m-statement__name').get_text()\n",
    "            statement_originator = re.sub(r'\\s+', ' ', statement_originator).strip()\n",
    "            \n",
    "            statement_date = author.find('div',class_='m-statement__desc').get_text()\n",
    "            statement_date = re.sub(r'\\s+', ' ', statement_date).strip()\n",
    "            statement_date = dparser.parse(statement_date,fuzzy=True).strftime('%m/%d/%Y')\n",
    "            if article_element:\n",
    "                section_element = article_element.find('section', class_='o-pick')\n",
    "                if section_element:\n",
    "                    section_element.decompose()\n",
    "                article_text = article_element.get_text()\n",
    "                article_text = article_text.replace(u'\\xa0', u' ')\n",
    "                reasoning = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "            return verdict, statement_originator, statement, statement_date, reasoning, url\n",
    "    except Exception as e:\n",
    "        print(f\"Error during website scraping: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "factcheck_urls = [\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=half-true\",\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=true\",\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=false\",\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=mostly-true\",\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=barely-true\",\n",
    "    \"https://www.politifact.com/factchecks/list/?ruling=pants-fire\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during website scraping: bad month number 27; must be 1-12: dicho el Septiembre 27, 2024 en un anuncio de campaña:\n",
      "Error during website scraping: hour must be in 0..23: stated on October 7, 2024 in an interview on CBS's \"60 Minutes\":\n",
      "Error during website scraping: bad month number 19; must be 1-12: dicho el Septiembre 19, 2024 en una audiencia en la Cámara de Representantes:\n",
      "Error during website scraping: bad month number 22; must be 1-12: dicho el Agosto 22, 2024 en su discurso de aceptación en la Convención Nacional Demócrata:\n",
      "Error during website scraping: bad month number 25; must be 1-12: dicho el Febrero 25, 2025 en una publicación en Facebook:\n",
      "Error during website scraping: bad month number 17; must be 1-12: dicho el Febrero 17, 2025 en una publicación en Facebook:\n",
      "Error during website scraping: bad month number 17; must be 1-12: dicho el Febrero 17, 2025 en una publicación en Facebook:\n",
      "Error during website scraping: Unknown string format: stated on January 11, 2025 in a Jan 11, 2025, video:\n",
      "Error during website scraping: hour must be in 0..23: stated on October 7, 2024 in an interview on \"60 Minutes\":\n",
      "Error during website scraping: Unknown string format: stated on February 18, 2025 in remarks to reporters at Mar-a-Lago:\n",
      "Error during website scraping: bad month number 30; must be 1-12: dicho el Noviembre 30, 2024 en una publicación en Facebook:\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for ruling in factcheck_urls:\n",
    "    response = requests.get(ruling, timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_element = soup.find_all('li', class_='o-listicle__item')\n",
    "    for i in range(30):\n",
    "        ref = article_element[i].find('div', class_='m-statement__quote').find('a').get('href')\n",
    "        url = f'https://www.politifact.com{ref}'\n",
    "        news = scrape_politifact(url)\n",
    "        data.append(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVeriTeC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('data_dev.json', 'r') as file:\n",
    "    averitec = json.load(file)\n",
    "\n",
    "averitec_20 = averitec[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_dev_1.json\", \"w\") as final:\n",
    "\tjson.dump(averitec_20, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "averitec_results = []\n",
    "for s in statements:\n",
    "    verdict, confidence, reasoning, claims = pipeline.fact_check(s)\n",
    "    questions = [claim.components[j].question for claim in claims for j in range(len(claim.components))] # List of questions\n",
    "    answers = [claim.components[j].answer.text for claim in claims for j in range(len(claim.components))] # List of corresponding answers\n",
    "    urls = ['x' for i in range(len(questions))] # List of corresponding URLs\n",
    "    pred_label = verdict\n",
    "    print('='*20)\n",
    "    print(pred_label)\n",
    "    averitec_results.append(format_prediction(questions, answers, urls, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pilot_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-AggreFact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>doc</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>contamination_identifier</th>\n",
       "      <th>gemini_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>Using acrylic paint is a fun and economical wa...</td>\n",
       "      <td>You can use a single acrylic paint color or mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>LLM-AggreFact:PuKGySjD1LKX354etBOla3Ke9VjVBmYR...</td>\n",
       "      <td>[{'verdict': 'supported', 'confidence': 1.0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>Recipe by rjholtz\\nUpdated on April 23, 2023\\n...</td>\n",
       "      <td>You can also grill tilapia in foil or use othe...</td>\n",
       "      <td>1</td>\n",
       "      <td>LLM-AggreFact:nGjDY4zLhqIU5KzXn5YqfKdQDL72ru6u...</td>\n",
       "      <td>[{'verdict': 'supported', 'confidence': 0.95, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>Listen to Chuck Swindoll’s overview of First J...</td>\n",
       "      <td>The author of the book of First John is not id...</td>\n",
       "      <td>1</td>\n",
       "      <td>LLM-AggreFact:3EjFNtDyEESiwE3PKH4eILjTpzVzoQRv...</td>\n",
       "      <td>[{'verdict': 'supported', 'confidence': 0.9, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>Agender is defined as not having a gender. Som...</td>\n",
       "      <td>Agender people may use gender-neutral pronouns...</td>\n",
       "      <td>0</td>\n",
       "      <td>LLM-AggreFact:kgftCc3SYJipb7CnIJHAuTjCgljJ4LhQ...</td>\n",
       "      <td>[{'verdict': 'unsupported', 'confidence': 0.6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ClaimVerify</td>\n",
       "      <td>2000, Comedy/Drama, 1h 46m\\nCritics Consensus\\...</td>\n",
       "      <td>The cast of O Brother, Where Art Thou? include...</td>\n",
       "      <td>0</td>\n",
       "      <td>LLM-AggreFact:31kK2h5aC3tGW4bFrV3z5yXj7vznzrlV...</td>\n",
       "      <td>[{'verdict': 'partially supported', 'confidenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset                                                doc  \\\n",
       "0  ClaimVerify  Using acrylic paint is a fun and economical wa...   \n",
       "1  ClaimVerify  Recipe by rjholtz\\nUpdated on April 23, 2023\\n...   \n",
       "2  ClaimVerify  Listen to Chuck Swindoll’s overview of First J...   \n",
       "3  ClaimVerify  Agender is defined as not having a gender. Som...   \n",
       "4  ClaimVerify  2000, Comedy/Drama, 1h 46m\\nCritics Consensus\\...   \n",
       "\n",
       "                                               claim  label  \\\n",
       "0  You can use a single acrylic paint color or mi...      1   \n",
       "1  You can also grill tilapia in foil or use othe...      1   \n",
       "2  The author of the book of First John is not id...      1   \n",
       "3  Agender people may use gender-neutral pronouns...      0   \n",
       "4  The cast of O Brother, Where Art Thou? include...      0   \n",
       "\n",
       "                            contamination_identifier  \\\n",
       "0  LLM-AggreFact:PuKGySjD1LKX354etBOla3Ke9VjVBmYR...   \n",
       "1  LLM-AggreFact:nGjDY4zLhqIU5KzXn5YqfKdQDL72ru6u...   \n",
       "2  LLM-AggreFact:3EjFNtDyEESiwE3PKH4eILjTpzVzoQRv...   \n",
       "3  LLM-AggreFact:kgftCc3SYJipb7CnIJHAuTjCgljJ4LhQ...   \n",
       "4  LLM-AggreFact:31kK2h5aC3tGW4bFrV3z5yXj7vznzrlV...   \n",
       "\n",
       "                                      gemini_results  \n",
       "0  [{'verdict': 'supported', 'confidence': 1.0, '...  \n",
       "1  [{'verdict': 'supported', 'confidence': 0.95, ...  \n",
       "2  [{'verdict': 'supported', 'confidence': 0.9, '...  \n",
       "3  [{'verdict': 'unsupported', 'confidence': 0.6,...  \n",
       "4  [{'verdict': 'partially supported', 'confidenc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc79120912486d9797d37258386110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 0 because 3/3 trials completed\n",
      "Skipping row 1 because 3/3 trials completed\n",
      "Skipping row 2 because 3/3 trials completed\n",
      "Skipping row 3 because 3/3 trials completed\n",
      "Skipping row 4 because 3/3 trials completed\n",
      "Skipping row 5 because 3/3 trials completed\n",
      "Skipping row 6 because 3/3 trials completed\n",
      "Skipping row 7 because 3/3 trials completed\n",
      "Skipping row 8 because 3/3 trials completed\n",
      "Skipping row 9 because 3/3 trials completed\n",
      "Skipping row 10 because 3/3 trials completed\n",
      "Skipping row 11 because 3/3 trials completed\n",
      "Skipping row 12 because 3/3 trials completed\n",
      "Skipping row 13 because 3/3 trials completed\n",
      "Skipping row 14 because 3/3 trials completed\n",
      "Skipping row 15 because 3/3 trials completed\n",
      "Skipping row 16 because 3/3 trials completed\n",
      "Skipping row 17 because 3/3 trials completed\n",
      "Skipping row 18 because 3/3 trials completed\n",
      "Skipping row 19 because 3/3 trials completed\n",
      "Skipping row 20 because 3/3 trials completed\n",
      "Skipping row 21 because 3/3 trials completed\n",
      "Skipping row 22 because 3/3 trials completed\n",
      "Skipping row 23 because 3/3 trials completed\n",
      "Skipping row 24 because 3/3 trials completed\n",
      "Skipping row 25 because 3/3 trials completed\n",
      "Skipping row 26 because 3/3 trials completed\n",
      "Skipping row 27 because 3/3 trials completed\n",
      "Skipping row 28 because 3/3 trials completed\n",
      "Skipping row 29 because 3/3 trials completed\n",
      "Skipping row 30 because 3/3 trials completed\n",
      "Skipping row 31 because 3/3 trials completed\n",
      "Skipping row 32 because 3/3 trials completed\n",
      "Skipping row 33 because 3/3 trials completed\n",
      "Skipping row 34 because 3/3 trials completed\n",
      "Skipping row 35 because 3/3 trials completed\n",
      "Skipping row 36 because 3/3 trials completed\n",
      "Skipping row 37 because 3/3 trials completed\n",
      "Skipping row 38 because 3/3 trials completed\n",
      "Skipping row 39 because 3/3 trials completed\n",
      "Skipping row 40 because 3/3 trials completed\n",
      "Skipping row 41 because 3/3 trials completed\n",
      "Skipping row 42 because 3/3 trials completed\n",
      "Skipping row 43 because 3/3 trials completed\n",
      "Skipping row 44 because 3/3 trials completed\n",
      "Skipping row 45 because 3/3 trials completed\n",
      "Skipping row 46 because 3/3 trials completed\n",
      "Skipping row 47 because 3/3 trials completed\n",
      "Skipping row 48 because 3/3 trials completed\n",
      "Skipping row 49 because 3/3 trials completed\n",
      "Skipping row 50 because 3/3 trials completed\n",
      "Skipping row 51 because 3/3 trials completed\n",
      "Skipping row 52 because 3/3 trials completed\n",
      "Skipping row 53 because 3/3 trials completed\n",
      "Skipping row 54 because 3/3 trials completed\n",
      "Skipping row 55 because 3/3 trials completed\n",
      "Skipping row 56 because 3/3 trials completed\n",
      "Skipping row 57 because 3/3 trials completed\n",
      "Skipping row 58 because 3/3 trials completed\n",
      "Skipping row 59 because 3/3 trials completed\n",
      "Skipping row 60 because 3/3 trials completed\n",
      "Skipping row 61 because 3/3 trials completed\n",
      "Skipping row 62 because 3/3 trials completed\n",
      "Skipping row 63 because 3/3 trials completed\n",
      "Skipping row 64 because 3/3 trials completed\n",
      "Skipping row 65 because 3/3 trials completed\n",
      "Skipping row 66 because 3/3 trials completed\n",
      "Skipping row 67 because 3/3 trials completed\n",
      "Skipping row 68 because 3/3 trials completed\n",
      "Skipping row 69 because 3/3 trials completed\n",
      "Skipping row 70 because 3/3 trials completed\n",
      "Skipping row 71 because 3/3 trials completed\n",
      "Skipping row 72 because 3/3 trials completed\n",
      "Skipping row 73 because 3/3 trials completed\n",
      "Skipping row 74 because 3/3 trials completed\n",
      "Skipping row 75 because 3/3 trials completed\n",
      "Skipping row 76 because 3/3 trials completed\n",
      "Skipping row 77 because 3/3 trials completed\n",
      "Skipping row 78 because 3/3 trials completed\n",
      "Skipping row 79 because 3/3 trials completed\n",
      "Skipping row 80 because 3/3 trials completed\n",
      "Skipping row 81 because 3/3 trials completed\n",
      "Skipping row 82 because 3/3 trials completed\n",
      "Skipping row 83 because 3/3 trials completed\n",
      "Skipping row 84 because 3/3 trials completed\n",
      "Skipping row 85 because 3/3 trials completed\n",
      "Skipping row 86 because 3/3 trials completed\n",
      "Skipping row 87 because 3/3 trials completed\n",
      "Skipping row 88 because 3/3 trials completed\n",
      "Skipping row 89 because 3/3 trials completed\n",
      "Skipping row 90 because 3/3 trials completed\n",
      "Skipping row 91 because 3/3 trials completed\n",
      "Skipping row 92 because 3/3 trials completed\n",
      "Skipping row 93 because 3/3 trials completed\n",
      "Skipping row 94 because 3/3 trials completed\n",
      "Skipping row 95 because 3/3 trials completed\n",
      "Skipping row 96 because 3/3 trials completed\n",
      "Skipping row 97 because 3/3 trials completed\n",
      "Skipping row 98 because 3/3 trials completed\n",
      "Skipping row 99 because 3/3 trials completed\n",
      "Skipping row 100 because 3/3 trials completed\n",
      "Skipping row 101 because 3/3 trials completed\n",
      "Skipping row 102 because 3/3 trials completed\n",
      "Skipping row 103 because 3/3 trials completed\n",
      "Skipping row 104 because 3/3 trials completed\n",
      "Skipping row 105 because 3/3 trials completed\n",
      "Skipping row 106 because 3/3 trials completed\n",
      "Skipping row 107 because 3/3 trials completed\n",
      "Skipping row 108 because 3/3 trials completed\n",
      "Skipping row 109 because 3/3 trials completed\n",
      "Skipping row 110 because 3/3 trials completed\n",
      "Skipping row 111 because 3/3 trials completed\n",
      "Skipping row 112 because 3/3 trials completed\n",
      "Skipping row 113 because 3/3 trials completed\n",
      "Skipping row 114 because 3/3 trials completed\n",
      "Skipping row 115 because 3/3 trials completed\n",
      "Skipping row 116 because 3/3 trials completed\n",
      "Skipping row 117 because 3/3 trials completed\n",
      "Skipping row 118 because 3/3 trials completed\n",
      "Skipping row 119 because 3/3 trials completed\n",
      "Skipping row 120 because 3/3 trials completed\n",
      "Skipping row 121 because 3/3 trials completed\n",
      "Skipping row 122 because 3/3 trials completed\n",
      "Skipping row 123 because 3/3 trials completed\n",
      "Skipping row 124 because 3/3 trials completed\n",
      "Skipping row 125 because 3/3 trials completed\n",
      "Skipping row 126 because 3/3 trials completed\n",
      "Skipping row 127 because 3/3 trials completed\n",
      "Skipping row 128 because 3/3 trials completed\n",
      "Skipping row 129 because 3/3 trials completed\n",
      "Skipping row 130 because 3/3 trials completed\n",
      "Skipping row 131 because 3/3 trials completed\n",
      "Skipping row 132 because 3/3 trials completed\n",
      "Skipping row 133 because 3/3 trials completed\n",
      "Skipping row 134 because 3/3 trials completed\n",
      "Skipping row 135 because 3/3 trials completed\n",
      "Skipping row 136 because 3/3 trials completed\n",
      "Skipping row 137 because 3/3 trials completed\n",
      "Skipping row 138 because 3/3 trials completed\n",
      "Skipping row 139 because 3/3 trials completed\n",
      "Skipping row 140 because 3/3 trials completed\n",
      "Skipping row 141 because 3/3 trials completed\n",
      "Skipping row 142 because 3/3 trials completed\n",
      "Skipping row 143 because 3/3 trials completed\n",
      "Skipping row 144 because 3/3 trials completed\n",
      "Skipping row 145 because 3/3 trials completed\n",
      "Skipping row 146 because 3/3 trials completed\n",
      "Skipping row 147 because 3/3 trials completed\n",
      "Skipping row 148 because 3/3 trials completed\n",
      "Skipping row 149 because 3/3 trials completed\n",
      "Skipping row 150 because 3/3 trials completed\n",
      "Skipping row 151 because 3/3 trials completed\n",
      "Skipping row 152 because 3/3 trials completed\n",
      "Skipping row 153 because 3/3 trials completed\n",
      "Skipping row 154 because 3/3 trials completed\n",
      "Skipping row 155 because 3/3 trials completed\n",
      "Skipping row 156 because 3/3 trials completed\n",
      "Skipping row 157 because 3/3 trials completed\n",
      "Skipping row 158 because 3/3 trials completed\n",
      "Skipping row 159 because 3/3 trials completed\n",
      "Skipping row 160 because 3/3 trials completed\n",
      "Skipping row 161 because 3/3 trials completed\n",
      "Skipping row 162 because 3/3 trials completed\n",
      "Skipping row 163 because 3/3 trials completed\n",
      "Skipping row 164 because 3/3 trials completed\n",
      "Skipping row 165 because 3/3 trials completed\n",
      "Skipping row 166 because 3/3 trials completed\n",
      "Skipping row 167 because 3/3 trials completed\n",
      "Skipping row 168 because 3/3 trials completed\n",
      "Skipping row 169 because 3/3 trials completed\n",
      "Skipping row 170 because 3/3 trials completed\n",
      "Skipping row 171 because 3/3 trials completed\n",
      "Skipping row 172 because 3/3 trials completed\n",
      "Skipping row 173 because 3/3 trials completed\n",
      "Skipping row 174 because 3/3 trials completed\n",
      "Skipping row 175 because 3/3 trials completed\n",
      "Skipping row 176 because 3/3 trials completed\n",
      "Skipping row 177 because 3/3 trials completed\n",
      "Skipping row 178 because 3/3 trials completed\n",
      "Skipping row 179 because 3/3 trials completed\n",
      "Skipping row 180 because 3/3 trials completed\n",
      "Skipping row 181 because 3/3 trials completed\n",
      "Skipping row 182 because 3/3 trials completed\n",
      "Skipping row 183 because 3/3 trials completed\n",
      "Skipping row 184 because 3/3 trials completed\n",
      "Skipping row 185 because 3/3 trials completed\n",
      "Skipping row 186 because 3/3 trials completed\n",
      "Skipping row 187 because 3/3 trials completed\n",
      "Skipping row 188 because 3/3 trials completed\n",
      "Skipping row 189 because 3/3 trials completed\n",
      "Skipping row 190 because 3/3 trials completed\n",
      "Skipping row 191 because 3/3 trials completed\n",
      "Skipping row 192 because 3/3 trials completed\n",
      "Skipping row 193 because 3/3 trials completed\n",
      "Skipping row 194 because 3/3 trials completed\n",
      "Skipping row 195 because 3/3 trials completed\n",
      "Skipping row 196 because 3/3 trials completed\n",
      "Skipping row 197 because 3/3 trials completed\n",
      "Skipping row 198 because 3/3 trials completed\n",
      "Skipping row 199 because 3/3 trials completed\n",
      "Skipping row 200 because 3/3 trials completed\n",
      "Skipping row 201 because 3/3 trials completed\n",
      "Skipping row 202 because 3/3 trials completed\n",
      "Skipping row 203 because 3/3 trials completed\n",
      "Skipping row 204 because 3/3 trials completed\n",
      "Skipping row 205 because 3/3 trials completed\n",
      "Skipping row 206 because 3/3 trials completed\n",
      "Skipping row 207 because 3/3 trials completed\n",
      "Running row 208 because 1/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3218637a5761482e80ef3311b6589136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mPassage 3 pay structures companies\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mPassage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe amount of pay from company to company does not vary too much, but you do have a wide variety of compensation methods. There are various combinations of hourly and commission pay rates, which depending on what type of work you specialize in can vary your bottom line considerably.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Passage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim by explicitly stating that Passage 3 mentions hourly rates, commission-based rates, and combinations thereof as pay structures used by different companies.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mPassage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe amount of pay from company to company does not vary too much, but you do have a wide variety of compensation methods. There are various combinations of hourly and commission pay rates, which depending on what type of work you specialize in can vary your bottom line considerably.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mPassage 3 pay structures companies\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Passage 3 pay structures companies']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mPassage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe amount of pay from company to company does not vary too much, but you do have a wide variety of compensation methods. There are various combinations of hourly and commission pay rates, which depending on what type of work you specialize in can vary your bottom line considerably.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Passage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim by explicitly stating that Passage 3 mentions hourly rates, commission-based rates, and combinations thereof as pay structures used by different companies.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mPassage 3 indicates that different companies utilize varying pay structures, including hourly rates, commission-based rates, or a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat types of pay structures are mentioned in Passage 3 as being used by different companies?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mPassage 3 mentions that different companies utilize various combinations of hourly and commission pay rates for automotive technicians [1].  The specific combination used can greatly affect the technician's overall earnings [1]. \n",
      "\n",
      "Reasoning: Passage 3 states that there are \"various combinations of hourly and commission pay rates\" used by different companies in the automotive tech industry.  The passage highlights that these combinations can significantly impact earnings depending on the type of work performed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe amount of pay from company to company does not vary too much, but you do have a wide variety of compensation methods. There are various combinations of hourly and commission pay rates, which depending on what type of work you specialize in can vary your bottom line considerably.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mPassage 3 provides information on the varying pay structures among different companies, with some offering hourly and commission pay rates, while others may use a combination of both.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Passage 3 mentions different companies using hourly rates, commission-based rates, or a combination of both for automotive technicians.  This aligns perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 209 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4214ea561f254c28b4e05b593d8790d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to mutable sign, spring, and seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat are the characteristics of a 'mutable sign' in the context of astrology or seasonal changes?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['definition mutable sign astrology', 'mutable signs and seasonal transitions']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mHow does the identified 'it' signify the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"[identified 'it'] and end of spring\", \"[identified 'it'] seasonal transition\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to mutable sign, spring, and seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcontextual meaning of 'it' in relation to mutable sign, spring, and seasonal change\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to mutable sign, spring, and seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the provided text, 'it' refers to the astrological sign Gemini [1].  This passage describes Gemini as a mutable sign that precedes summer, signifying change after the spring season of Taurus [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"it\" in several contexts.  Document [1], passage 2, refers to \"it\" as Gemini, a mutable sign preceding summer, announcing change after the spring of Taurus.  Therefore, \"it\" in this context refers to the astrological sign Gemini.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the characteristics of a 'mutable sign' in the context of astrology or seasonal changes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition mutable sign astrology', 'mutable signs and seasonal transitions']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mdefinition mutable sign astrology\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mmutable signs and seasonal transitions\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the characteristics of a 'mutable sign' in the context of astrology or seasonal changes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition mutable sign astrology', 'mutable signs and seasonal transitions']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn astrology, mutable signs, such as Gemini, are associated with periods of transition and change [1, 3].  Gemini, for example, precedes summer and marks the end of spring, signifying a shift in the season and in life on Earth [1, 3]. \n",
      "\n",
      "Reasoning: Document [1] and [3] mention Gemini as a mutable sign that precedes summer, indicating change and the transition from spring (Taurus) to summer.  This suggests that mutable signs, in the context of astrology and seasonal changes, are associated with periods of transition and adaptation.  The provided text focuses on Gemini as an example, but doesn't explicitly define the broader characteristics of mutable signs beyond this transitional nature.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the identified 'it' signify the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"[identified 'it'] and end of spring\", \"[identified 'it'] seasonal transition\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[identified 'it'] and end of spring\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[identified 'it'] seasonal transition\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the identified 'it' signify the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"[identified 'it'] and end of spring\", \"[identified 'it'] seasonal transition\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the zodiac, Gemini follows Taurus, which represents spring [1, 3].  Gemini is described as a mutable sign that \"announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change\" [1, 3]. Therefore, Gemini signifies the transition from spring to summer. \n",
      "\n",
      "Reasoning: The provided text mentions Gemini as a mutable sign in the zodiac that follows Taurus (which represents spring).  It states that Gemini \"announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\"  This indicates that Gemini's position signifies the end of spring and the beginning of a new season (summer).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text identifies \"it\" as the astrological sign Gemini.  The question-answer pairs establish that Gemini is a mutable sign, meaning it represents a period of transition and change.  Specifically, Gemini's position in the zodiac follows Taurus (spring), marking the end of spring and the beginning of summer.  Therefore, the claim that Gemini is a mutable sign indicating the end of spring and the coming change of seasons is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the provided text, 'it' refers to the astrological sign Gemini [1].  This passage describes Gemini as a mutable sign that precedes summer, signifying change after the spring season of Taurus [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Identifies the subject of the claim as the astrological sign Gemini.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the characteristics of a 'mutable sign' in the context of astrology or seasonal changes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In astrology, mutable signs, such as Gemini, are associated with periods of transition and change [1, 3].  Gemini, for example, precedes summer and marks the end of spring, signifying a shift in the season and in life on Earth [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Defines mutable signs as representing transition and change, aligning with the claim's description.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How does the identified 'it' signify the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the zodiac, Gemini follows Taurus, which represents spring [1, 3].  Gemini is described as a mutable sign that \"announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change\" [1, 3]. Therefore, Gemini signifies the transition from spring to summer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by showing Gemini's position signifies the transition from spring to summer.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text identifies \"it\" as the astrological sign Gemini.  The question-answer pairs establish that Gemini is a mutable sign, meaning it represents a period of transition and change.  Specifically, Gemini's position in the zodiac follows Taurus (spring), marking the end of spring and the beginning of summer.  Therefore, the claim that Gemini is a mutable sign indicating the end of spring and the coming change of seasons is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the provided text, 'it' refers to the astrological sign Gemini [1].  This passage describes Gemini as a mutable sign that precedes summer, signifying change after the spring season of Taurus [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"it\" in several contexts.  Document [1], passage 2, refers to \"it\" as Gemini, a mutable sign preceding summer, announcing change after the spring of Taurus.  Therefore, \"it\" in this context refers to the astrological sign Gemini.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the characteristics of a 'mutable sign' in the context of astrology or seasonal changes?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn astrology, mutable signs, such as Gemini, are associated with periods of transition and change [1, 3].  Gemini, for example, precedes summer and marks the end of spring, signifying a shift in the season and in life on Earth [1, 3]. \n",
      "\n",
      "Reasoning: Document [1] and [3] mention Gemini as a mutable sign that precedes summer, indicating change and the transition from spring (Taurus) to summer.  This suggests that mutable signs, in the context of astrology and seasonal changes, are associated with periods of transition and adaptation.  The provided text focuses on Gemini as an example, but doesn't explicitly define the broader characteristics of mutable signs beyond this transitional nature.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow does the identified 'it' signify the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the zodiac, Gemini follows Taurus, which represents spring [1, 3].  Gemini is described as a mutable sign that \"announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change\" [1, 3]. Therefore, Gemini signifies the transition from spring to summer. \n",
      "\n",
      "Reasoning: The provided text mentions Gemini as a mutable sign in the zodiac that follows Taurus (which represents spring).  It states that Gemini \"announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\"  This indicates that Gemini's position signifies the end of spring and the beginning of a new season (summer).\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided text identifies \"it\" as the astrological sign Gemini.  The question-answer pairs establish that Gemini is a mutable sign, meaning it represents a period of transition and change.  Specifically, Gemini's position in the zodiac follows Taurus (spring), marking the end of spring and the beginning of summer.  Therefore, the claim that Gemini is a mutable sign indicating the end of spring and the coming change of seasons is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided text identifies \"it\" as the astrological sign Gemini.  The question-answer pairs establish that Gemini is a mutable sign, meaning it represents a period of transition and change.  Specifically, Gemini's position in the zodiac follows Taurus (spring), marking the end of spring and the beginning of summer.  Therefore, the claim that Gemini is a mutable sign indicating the end of spring and the coming change of seasons is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mIs the subject mutable (changeable) in nature?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[subject identified in previous question] mutability', '[subject identified in previous question] change over time']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mDoes the subject reliably indicate the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[subject identified in previous question] and seasonal change', '[subject identified in previous question] spring equinox', '[subject identified in previous question] end of spring indicators']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcontextual meaning of 'it' in relation to seasonal change\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"contextual meaning of 'it' in relation to seasonal change\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the provided text, 'it' refers to the astrological sign Gemini [1].  Specifically, passage 2 describes Gemini as a mutable sign preceding summer, signifying the change from spring to summer [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"it\" in multiple contexts.  Document [1], passage 2, refers to \"it\" as Gemini, a mutable sign preceding summer, announcing change at the end of spring.  Therefore, \"it\" refers to the astrological sign Gemini and its association with seasonal change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the subject mutable (changeable) in nature?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[subject identified in previous question] mutability', '[subject identified in previous question] change over time']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[subject identified in previous question] mutability\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[subject identified in previous question] change over time\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the subject mutable (changeable) in nature?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[subject identified in previous question] mutability', '[subject identified in previous question] change over time']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, in astrology, Gemini is described as a mutable sign [2], meaning it is changeable in nature. \n",
      "\n",
      "Reasoning: The provided text mentions that Gemini is a \"mutable sign\" [2].  This indicates that, in the context of astrology, Gemini is considered changeable or mutable in nature.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the subject reliably indicate the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[subject identified in previous question] and seasonal change', '[subject identified in previous question] spring equinox', '[subject identified in previous question] end of spring indicators']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[subject identified in previous question] and seasonal change\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[subject identified in previous question] spring equinox\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[subject identified in previous question] end of spring indicators\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the subject reliably indicate the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[subject identified in previous question] and seasonal change', '[subject identified in previous question] spring equinox', '[subject identified in previous question] end of spring indicators']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAccording to the provided text, the Gemini zodiac sign follows Taurus and precedes summer, indicating the end of spring and the beginning of summer [1]. However, the reliability of this as a precise indicator of seasonal change is not explicitly discussed. \n",
      "\n",
      "Reasoning: The provided text mentions Gemini as a zodiac sign that follows Taurus and precedes summer.  It states that Gemini \"announces change while ruling the time of year when Taurus spring has ended\".  This suggests that Gemini's appearance in the zodiac is associated with the end of spring and the beginning of a new season (summer). However, the reliability of this as an indicator is not explicitly stated.  The text focuses on the mythological and historical aspects of Gemini, not its precise astronomical correlation with seasonal changes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs identify \"it\" as the astrological sign Gemini.  The answers confirm that Gemini is indeed a mutable sign, meaning it is changeable in nature.  Furthermore, the answers indicate that Gemini's position in the zodiac cycle is associated with the transition from spring to summer. However, the reliability of Gemini as a precise indicator of the end of spring is not explicitly stated in the provided text; the text focuses more on its symbolic association with the seasonal change.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the provided text, 'it' refers to the astrological sign Gemini [1].  Specifically, passage 2 describes Gemini as a mutable sign preceding summer, signifying the change from spring to summer [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Identifies 'it' as Gemini, establishing the subject of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is the subject mutable (changeable) in nature?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, in astrology, Gemini is described as a mutable sign [2], meaning it is changeable in nature.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the mutable nature of Gemini, supporting part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the subject reliably indicate the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: According to the provided text, the Gemini zodiac sign follows Taurus and precedes summer, indicating the end of spring and the beginning of summer [1]. However, the reliability of this as a precise indicator of seasonal change is not explicitly discussed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the association of Gemini with the end of spring and the start of summer, but notes the lack of explicit information on its reliability as a precise indicator.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs identify \"it\" as the astrological sign Gemini.  The answers confirm that Gemini is indeed a mutable sign, meaning it is changeable in nature.  Furthermore, the answers indicate that Gemini's position in the zodiac cycle is associated with the transition from spring to summer. However, the reliability of Gemini as a precise indicator of the end of spring is not explicitly stated in the provided text; the text focuses more on its symbolic association with the seasonal change.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is 'it' referring to in the statement?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the provided text, 'it' refers to the astrological sign Gemini [1].  Specifically, passage 2 describes Gemini as a mutable sign preceding summer, signifying the change from spring to summer [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"it\" in multiple contexts.  Document [1], passage 2, refers to \"it\" as Gemini, a mutable sign preceding summer, announcing change at the end of spring.  Therefore, \"it\" refers to the astrological sign Gemini and its association with seasonal change.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs the subject mutable (changeable) in nature?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, in astrology, Gemini is described as a mutable sign [2], meaning it is changeable in nature. \n",
      "\n",
      "Reasoning: The provided text mentions that Gemini is a \"mutable sign\" [2].  This indicates that, in the context of astrology, Gemini is considered changeable or mutable in nature.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the subject reliably indicate the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAccording to the provided text, the Gemini zodiac sign follows Taurus and precedes summer, indicating the end of spring and the beginning of summer [1]. However, the reliability of this as a precise indicator of seasonal change is not explicitly discussed. \n",
      "\n",
      "Reasoning: The provided text mentions Gemini as a zodiac sign that follows Taurus and precedes summer.  It states that Gemini \"announces change while ruling the time of year when Taurus spring has ended\".  This suggests that Gemini's appearance in the zodiac is associated with the end of spring and the beginning of a new season (summer). However, the reliability of this as an indicator is not explicitly stated.  The text focuses on the mythological and historical aspects of Gemini, not its precise astronomical correlation with seasonal changes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs identify \"it\" as the astrological sign Gemini.  The answers confirm that Gemini is indeed a mutable sign, meaning it is changeable in nature.  Furthermore, the answers indicate that Gemini's position in the zodiac cycle is associated with the transition from spring to summer. However, the reliability of Gemini as a precise indicator of the end of spring is not explicitly stated in the provided text; the text focuses more on its symbolic association with the seasonal change.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs identify \"it\" as the astrological sign Gemini.  The answers confirm that Gemini is indeed a mutable sign, meaning it is changeable in nature.  Furthermore, the answers indicate that Gemini's position in the zodiac cycle is associated with the transition from spring to summer. However, the reliability of Gemini as a precise indicator of the end of spring is not explicitly stated in the provided text; the text focuses more on its symbolic association with the seasonal change.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is meant by 'mutable sign' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['mutable sign astrology', 'mutable signs definition']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat natural phenomena or cultural events typically mark the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['signs of the end of spring', 'seasonal changes spring to summer']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mDoes the identified 'it' (which needs to be specified from the original statement not included here) exhibit characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'mutable sign' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['mutable sign astrology', 'mutable signs definition']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mmutable sign astrology\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mmutable signs definition\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'mutable sign' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['mutable sign astrology', 'mutable signs definition']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn astrology, a mutable sign is one that precedes a season and heralds change [1].  For example, Gemini is a mutable sign that precedes summer, signifying the end of spring and the beginning of a new season [1]. \n",
      "\n",
      "Reasoning: Based on the provided text, the term \"mutable sign\" in the context of astrology refers to a zodiac sign that precedes a season and signifies change.  The example given is Gemini, which precedes summer and marks the transition from spring.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat natural phenomena or cultural events typically mark the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['signs of the end of spring', 'seasonal changes spring to summer']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msigns of the end of spring\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mseasonal changes spring to summer\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat natural phenomena or cultural events typically mark the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['signs of the end of spring', 'seasonal changes spring to summer']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but based on the provided text, I cannot answer your question about natural phenomena or cultural events that mark the end of spring and the beginning of summer. The documents discuss the Gemini constellation and its mythology, which is not relevant to the question. \n",
      "\n",
      "Reasoning: The provided text focuses on the Gemini constellation and its mythological origins, not on natural phenomena or cultural events marking the end of spring.  Therefore, I cannot answer the question using the given source documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the identified 'it' (which needs to be specified from the original statement not included here) exhibit characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the identified 'it' (which needs to be specified from the original statement not included here) exhibit characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mInsufficient information is provided to answer the question.  The identity of 'it' is unknown, preventing an assessment of its characteristics relative to the end of spring and the beginning of a new season. \n",
      "\n",
      "Reasoning: Since no context or documents are provided regarding the identity of \"it,\"  it's impossible to determine whether it exhibits characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season.  The question is unanswerable without further information.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs offer some relevant information.  The first QA pair defines a mutable sign in astrology as one that precedes a season and heralds change, using Gemini as an example. However, the identity of \"it\" in the claim remains unspecified. The second QA pair is irrelevant as it seeks information on natural phenomena, which is not directly related to the claim. The third QA pair explicitly states that without knowing what \"it\" refers to, the claim cannot be evaluated.  Therefore, the claim is unsupported due to the lack of information about the identity of \"it\".\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is meant by 'mutable sign' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In astrology, a mutable sign is one that precedes a season and heralds change [1].  For example, Gemini is a mutable sign that precedes summer, signifying the end of spring and the beginning of a new season [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Defines 'mutable sign' and provides relevant context from astrology, but doesn't identify 'it' from the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What natural phenomena or cultural events typically mark the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but based on the provided text, I cannot answer your question about natural phenomena or cultural events that mark the end of spring and the beginning of summer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Irrelevant to evaluating the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the identified 'it' (which needs to be specified from the original statement not included here) exhibit characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Insufficient information is provided to answer the question.  The identity of 'it' is unknown, preventing an assessment of its characteristics relative to the end of spring and the beginning of a new season.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly addresses the lack of information needed to evaluate the claim, supporting the 'unsupported' verdict.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs offer some relevant information.  The first QA pair defines a mutable sign in astrology as one that precedes a season and heralds change, using Gemini as an example. However, the identity of \"it\" in the claim remains unspecified. The second QA pair is irrelevant as it seeks information on natural phenomena, which is not directly related to the claim. The third QA pair explicitly states that without knowing what \"it\" refers to, the claim cannot be evaluated.  Therefore, the claim is unsupported due to the lack of information about the identity of \"it\".\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is meant by 'mutable sign' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn astrology, a mutable sign is one that precedes a season and heralds change [1].  For example, Gemini is a mutable sign that precedes summer, signifying the end of spring and the beginning of a new season [1]. \n",
      "\n",
      "Reasoning: Based on the provided text, the term \"mutable sign\" in the context of astrology refers to a zodiac sign that precedes a season and signifies change.  The example given is Gemini, which precedes summer and marks the transition from spring.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat natural phenomena or cultural events typically mark the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but based on the provided text, I cannot answer your question about natural phenomena or cultural events that mark the end of spring and the beginning of summer. The documents discuss the Gemini constellation and its mythology, which is not relevant to the question. \n",
      "\n",
      "Reasoning: The provided text focuses on the Gemini constellation and its mythological origins, not on natural phenomena or cultural events marking the end of spring.  Therefore, I cannot answer the question using the given source documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the identified 'it' (which needs to be specified from the original statement not included here) exhibit characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mInsufficient information is provided to answer the question.  The identity of 'it' is unknown, preventing an assessment of its characteristics relative to the end of spring and the beginning of a new season. \n",
      "\n",
      "Reasoning: Since no context or documents are provided regarding the identity of \"it,\"  it's impossible to determine whether it exhibits characteristics consistent with a mutable sign indicating the end of spring and the beginning of a new season.  The question is unanswerable without further information.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs offer some relevant information.  The first QA pair defines a mutable sign in astrology as one that precedes a season and heralds change, using Gemini as an example. However, the identity of \"it\" in the claim remains unspecified. The second QA pair is irrelevant as it seeks information on natural phenomena, which is not directly related to the claim. The third QA pair explicitly states that without knowing what \"it\" refers to, the claim cannot be evaluated.  Therefore, the claim is unsupported due to the lack of information about the identity of \"it\".\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt is a mutable sign that indicates the end of spring and the coming change of seasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that \"it\" is a mutable sign indicating the end of spring and the coming change of seasons.  The provided QA pairs offer some relevant information.  The first QA pair defines a mutable sign in astrology as one that precedes a season and heralds change, using Gemini as an example. However, the identity of \"it\" in the claim remains unspecified. The second QA pair is irrelevant as it seeks information on natural phenomena, which is not directly related to the claim. The third QA pair explicitly states that without knowing what \"it\" refers to, the claim cannot be evaluated.  Therefore, the claim is unsupported due to the lack of information about the identity of \"it\".\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 210 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a54b4478694090a09a634c6e898db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreviews of [restaurant name] eating area size and ambiance\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] customer reviews dining area description\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe eating area is described as \"quaint but small\" [1, 4].  No further information about the atmosphere is available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that the eating area is described as \"quaint but small\" in the source material, citing specific references [1, 4].  There's no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The eating area is described as \"quaint but small\" [1, 4].  No further information about the atmosphere is available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim by stating that the eating area is described as 'quaint but small' in the source material.  The citation to [1, 4] further strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mtext describing cupcakes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mTurn me loose in this place! Yummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mBest Hamburger in town. Assuming you like them the way I do: chocolate cake, covered in frosting, hold the meat buns and veggies.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text describes the cupcakes using positive and endearing terms.  The phrase \"darling cupcakes\" directly supports the claim that they are adorable.  While \"yummy sugar delights\" focuses on taste, the overall tone of the descriptions leans towards positive and charming, indirectly supporting the \"adorable\" assessment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Cupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The description \"darling cupcakes\" directly supports the claim. The other descriptions, while focusing on taste, contribute to an overall positive and charming impression, indirectly supporting the claim of adorableness.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that the eating area is described as \"quaint but small\" in the source material, citing specific references [1, 4].  There's no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe eating area is described as \"quaint but small\" [1, 4].  No further information about the atmosphere is available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text describes the cupcakes using positive and endearing terms.  The phrase \"darling cupcakes\" directly supports the claim that they are adorable.  While \"yummy sugar delights\" focuses on taste, the overall tone of the descriptions leans towards positive and charming, indirectly supporting the \"adorable\" assessment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mTurn me loose in this place! Yummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mBest Hamburger in town. Assuming you like them the way I do: chocolate cake, covered in frosting, hold the meat buns and veggies.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreviews of [restaurant name] eating area size and ambiance\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] customer reviews dining area description\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [restaurant name] eating area size and ambiance', '[restaurant name] customer reviews dining area description']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe eating area is described as \"quaint but small\" [1, 4].  No further details on the atmosphere are available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that the eating area is described as \"quaint but small\" in the source material, citing specific references [1, 4].  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The eating area is described as \"quaint but small\" [1, 4].  No further details on the atmosphere are available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim by stating that the eating area is described as 'quaint but small' in the source material.  The citation to sources [1,4] further strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mtext describing cupcakes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['text describing cupcakes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mTurn me loose in this place! Yummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mBest Hamburger in town. Assuming you like them the way I do: chocolate cake, covered in frosting, hold the meat buns and veggies.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text describes the cupcakes using positive and endearing terms.  The phrase \"darling cupcakes\" directly supports the claim that they are adorable.  While \"yummy sugar delights\" focuses on taste, the overall tone of the descriptions leans towards positive and charming, indirectly supporting the \"adorable\" assessment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Cupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The description \"darling cupcakes\" directly supports the claim. The other descriptions, while focusing on taste, contribute to an overall positive and charming impression, indirectly supporting the claim of adorableness.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that the eating area is described as \"quaint but small\" in the source material, citing specific references [1, 4].  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe eating area is described as \"quaint but small\" [1, 4].  No further details on the atmosphere are available. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area as \"quaint but small\" [1, 4].  There is no further description of the atmosphere or ambiance available in the provided documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text describes the cupcakes using positive and endearing terms.  The phrase \"darling cupcakes\" directly supports the claim that they are adorable.  While \"yummy sugar delights\" focuses on taste, the overall tone of the descriptions leans towards positive and charming, indirectly supporting the \"adorable\" assessment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat was described about the cupcakes in the provided text?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCupcakes are described as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2], and as \"darling cupcakes\" [1]. One review even describes a hamburger as being like a cupcake: \"chocolate cake, covered in frosting, hold the meat buns and veggies\" [1]. \n",
      "\n",
      "Reasoning: The provided text contains several descriptions of cupcakes.  One review describes them as \"yummy sugar delights that make me wee little taste buds scream!\" [1, 2]. Another review mentions \"darling cupcakes\" [1]. A third review describes a hamburger as \"chocolate cake, covered in frosting, hold the meat buns and veggies,\" suggesting a cupcake-like structure [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mTurn me loose in this place! Yummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mBest Hamburger in town. Assuming you like them the way I do: chocolate cake, covered in frosting, hold the meat buns and veggies.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims, \"The eating area is quaint but small\" and \"The cupcakes are adorable,\" are supported by the evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['reviews of [location name] eating area size and ambiance', '[location name] restaurant photos eating area']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [location name] eating area size and ambiance', '[location name] restaurant photos eating area']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreviews of [location name] eating area size and ambiance\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[location name] restaurant photos eating area\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['reviews of [location name] eating area size and ambiance', '[location name] restaurant photos eating area']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe eating area at Crushcakes & Co. is described as \"quaint but small\" [2]. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area at Crushcakes & Co.  One review describes it as \"quaint but small\" [2].  No further details on size or atmosphere are given.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that a review describes the eating area as \"quaint but small\".  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The eating area at Crushcakes & Co. is described as \"quaint but small\" [2]. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area at Crushcakes & Co.  One review describes it as \"quaint but small\" [2].  No further details on size or atmosphere are given.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by quoting a review that describes the eating area as 'quaint but small'.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat was the description of the cupcakes mentioned in the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['review mentioning cupcakes description']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was the description of the cupcakes mentioned in the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review mentioning cupcakes description']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreview mentioning cupcakes description\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat was the description of the cupcakes mentioned in the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review mentioning cupcakes description']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mOne review describes the cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\" [2]. \n",
      "\n",
      "Reasoning: The provided text contains multiple reviews.  Review [1] mentions \"darling cupcakes\" but offers no further description. Review [2] describes cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\". This is the most descriptive option available.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mYummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim \"The cupcakes are adorable\" is not directly supported by the provided evidence. While one review calls the cupcakes \"darling,\" this is a subjective term and doesn't necessarily equate to \"adorable.\"  The other review focuses on the taste (\"Yummy sugar delights\"), not the visual appeal.  Therefore, there's insufficient evidence to support the claim of adorableness.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What was the description of the cupcakes mentioned in the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: One review describes the cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\" [2]. \n",
      "\n",
      "Reasoning: The provided text contains multiple reviews.  Review [1] mentions \"darling cupcakes\" but offers no further description. Review [2] describes cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\". This is the most descriptive option available.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: While one review uses the word 'darling,' which is somewhat close to 'adorable,' it's not a strong enough descriptor to support the claim. The other review focuses on taste, not appearance.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement asserts two claims: that the eating area is quaint but small, and that the cupcakes are adorable.  The first claim is supported by evidence from a review. The second claim, however, is unsupported. While a review mentions \"darling cupcakes,\"  the provided description doesn't directly equate to \"adorable.\"  Therefore, the overall statement is only partially supported.  Since one claim is supported and the other is unsupported, a conservative approach is taken to classify the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe eating area is quaint but small.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim \"The eating area is quaint but small\" is directly supported by the provided question-answer pair. The answer explicitly states that a review describes the eating area as \"quaint but small\".  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the size and atmosphere of the eating area?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe eating area at Crushcakes & Co. is described as \"quaint but small\" [2]. \n",
      "\n",
      "Reasoning: The provided text mentions the eating area at Crushcakes & Co.  One review describes it as \"quaint but small\" [2].  No further details on size or atmosphere are given.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cupcakes are adorable.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim \"The cupcakes are adorable\" is not directly supported by the provided evidence. While one review calls the cupcakes \"darling,\" this is a subjective term and doesn't necessarily equate to \"adorable.\"  The other review focuses on the taste (\"Yummy sugar delights\"), not the visual appeal.  Therefore, there's insufficient evidence to support the claim of adorableness.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat was the description of the cupcakes mentioned in the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mOne review describes the cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\" [2]. \n",
      "\n",
      "Reasoning: The provided text contains multiple reviews.  Review [1] mentions \"darling cupcakes\" but offers no further description. Review [2] describes cupcakes as \"Yummy sugar delights that make me wee little taste buds scream!\". This is the most descriptive option available.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mYummy sugar delights that make me wee little taste buds scream!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts two claims: that the eating area is quaint but small, and that the cupcakes are adorable.  The first claim is supported by evidence from a review. The second claim, however, is unsupported. While a review mentions \"darling cupcakes,\"  the provided description doesn't directly equate to \"adorable.\"  Therefore, the overall statement is only partially supported.  Since one claim is supported and the other is unsupported, a conservative approach is taken to classify the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey also mentioned the quaint but small eating area and the adorable cupcakes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts two claims: that the eating area is quaint but small, and that the cupcakes are adorable.  The first claim is supported by evidence from a review. The second claim, however, is unsupported. While a review mentions \"darling cupcakes,\"  the provided description doesn't directly equate to \"adorable.\"  Therefore, the overall statement is only partially supported.  Since one claim is supported and the other is unsupported, a conservative approach is taken to classify the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 211 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6623b61c8446bf943ce6140f1418b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [product/service name] high prices\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[product/service name] price complaints\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 472, in forward\n",
      "    result = self.synthesize(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 472, in forward\n",
      "    result = self.synthesize(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAt least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's ability to sustain its pricing model [2, 8]. Another customer acknowledged that Choppa Poke is more expensive than competitors but felt the higher price was justified by the quality and quantity of the food [3, 6].  The provided data does not offer a precise count of customer complaints regarding high prices. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review explicitly mentions the price as a factor, stating that a medium bowl costs $20 with tax and questioning the restaurant's long-term viability due to the price [2, 8]. Another review mentions that Choppa Poke is more expensive than other poke places, but justifies the price with the fresh fish and generous portions [3, 6].  There is no specific number of complaints about high prices, only individual opinions on the pricing.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive. So far so good and I'm glad it's there. Definitely worth checking out.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text shows that at least one customer complained about the high price of a poke bowl at Choppa Poke, citing a $20 price point with tax and questioning the restaurant's pricing model's sustainability.  Another customer acknowledged the higher prices compared to competitors but justified them based on quality and quantity.  While there's no exact number of complaints, the evidence supports the claim that *some* customers have noted high prices.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: At least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's ability to sustain its pricing model [2, 8]. Another customer acknowledged that Choppa Poke is more expensive than competitors but felt the higher price was justified by the quality and quantity of the food [3, 6].  The provided data does not offer a precise count of customer complaints regarding high prices. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review explicitly mentions the price as a factor, stating that a medium bowl costs $20 with tax and questioning the restaurant's long-term viability due to the price [2, 8]. Another review mentions that Choppa Poke is more expensive than other poke places, but justifies the price with the fresh fish and generous portions [3, 6].  There is no specific number of complaints about high prices, only individual opinions on the pricing.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by showing at least one customer complaint about high prices.  While a precise number isn't available, the existence of at least one complaint is sufficient to support the claim 'Some customers have noted that the prices are high'.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msite:example.com review negative customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"negative customer service experience\" site:example.com\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mOne customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service, supporting the claim with a specific example and citation.  The reasoning within the answer further explains how the identified review supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Can we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: One customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by citing a specific customer review that describes a negative customer service experience. The detail about the overheard conversation further strengthens the negative experience.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a one-star review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text shows that at least one customer complained about the high price of a poke bowl at Choppa Poke, citing a $20 price point with tax and questioning the restaurant's pricing model's sustainability.  Another customer acknowledged the higher prices compared to competitors but justified them based on quality and quantity.  While there's no exact number of complaints, the evidence supports the claim that *some* customers have noted high prices.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAt least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's ability to sustain its pricing model [2, 8]. Another customer acknowledged that Choppa Poke is more expensive than competitors but felt the higher price was justified by the quality and quantity of the food [3, 6].  The provided data does not offer a precise count of customer complaints regarding high prices. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review explicitly mentions the price as a factor, stating that a medium bowl costs $20 with tax and questioning the restaurant's long-term viability due to the price [2, 8]. Another review mentions that Choppa Poke is more expensive than other poke places, but justifies the price with the fresh fish and generous portions [3, 6].  There is no specific number of complaints about high prices, only individual opinions on the pricing.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive. So far so good and I'm glad it's there. Definitely worth checking out.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service, supporting the claim with a specific example and citation.  The reasoning within the answer further explains how the identified review supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mOne customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a one-star review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a one-star review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [product/service name] high prices\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[product/service name] price complaints\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAt least one customer review mentions that a medium poke bowl costs $20 with tax and expresses concern about the price's long-term viability [2, 8]. Another review acknowledges that Choppa Poke is more expensive than competitors but justifies the cost due to the quality and quantity of the food [3, 6].  The exact number of customers who have complained about high prices is not specified in the provided reviews. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review mentions the price of a medium bowl as $20 with tax and expresses concern about the price's sustainability [2, 8]. Another review acknowledges that Choppa Poke is more expensive than other poke places but justifies the price due to fresh fish and generous portions [3, 6].  There is no specific number of complaints about high prices, but these reviews indicate some customers find the pricing high.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided evidence shows that at least one customer review explicitly mentions the high price of a medium poke bowl ($20 with tax) and expresses concern about its long-term affordability. Another review acknowledges that the prices are higher than competitors but justifies this based on the quality and quantity of food. While the exact number of customers complaining about high prices is unknown, the existence of at least one such review supports the claim that some customers find the prices high.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: At least one customer review mentions that a medium poke bowl costs $20 with tax and expresses concern about the price's long-term viability [2, 8]. Another review acknowledges that Choppa Poke is more expensive than competitors but justifies the cost due to the quality and quantity of the food [3, 6].  The exact number of customers who have complained about high prices is not specified in the provided reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by confirming that at least one customer found the prices to be high.  While the exact number is unknown, the existence of at least one complaint is sufficient to support the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msite:example.com review negative customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"negative customer service experience\" site:example.com\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['site:example.com review negative customer service', '\"negative customer service experience\" site:example.com']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mOne customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service, supporting the claim with a specific example and citation.  The reasoning within the answer further explains how the identified review supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Can we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: One customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by citing a specific customer review that describes a negative customer service experience. The detail about the overheard conversation further strengthens the negative experience.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a specific one-star review detailing an incident of overheard negative comments from employees.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided evidence shows that at least one customer review explicitly mentions the high price of a medium poke bowl ($20 with tax) and expresses concern about its long-term affordability. Another review acknowledges that the prices are higher than competitors but justifies this based on the quality and quantity of food. While the exact number of customers complaining about high prices is unknown, the existence of at least one such review supports the claim that some customers find the prices high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAt least one customer review mentions that a medium poke bowl costs $20 with tax and expresses concern about the price's long-term viability [2, 8]. Another review acknowledges that Choppa Poke is more expensive than competitors but justifies the cost due to the quality and quantity of the food [3, 6].  The exact number of customers who have complained about high prices is not specified in the provided reviews. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review mentions the price of a medium bowl as $20 with tax and expresses concern about the price's sustainability [2, 8]. Another review acknowledges that Choppa Poke is more expensive than other poke places but justifies the price due to fresh fish and generous portions [3, 6].  There is no specific number of complaints about high prices, but these reviews indicate some customers find the pricing high.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service, supporting the claim with a specific example and citation.  The reasoning within the answer further explains how the identified review supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mCan we find a customer review detailing a negative experience with the company's customer service?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mOne customer gave a one-star review describing a negative experience with customer service.  They reported overhearing employees speaking negatively about them in Mandarin, using the phrase \"disgusting\" to describe them as customers [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews.  One review, with a 1-star rating, describes a negative experience with customer service. The reviewer states that they overheard employees speaking negatively about them in Mandarin, even though the employees thought they couldn't understand.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a specific one-star review detailing an incident of overheard negative comments from employees.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a specific one-star review detailing an incident of overheard negative comments from employees.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [product/service name] high prices\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[product/service name] price complaints\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product/service name] high prices', '[product/service name] price complaints']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAt least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's sustainability at that price point [2, 8]. Another customer acknowledged the higher prices compared to competitors but felt they were justified by the quality and quantity of food [3, 6].  The exact number of customers who complained about high prices is not specified in the provided reviews. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review explicitly mentions the price as a factor, stating that a medium bowl costs $20 with tax and questioning the restaurant's long-term viability due to the price [2, 8]. Another review acknowledges the higher price compared to other poke places but justifies it with the generous portions and fresh fish [3, 6].  There is no specific number of complaints about high prices, only individual opinions on the pricing.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive. So far so good and I'm glad it's there. Definitely worth checking out.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided evidence shows that at least one customer complained about the high price of a poke bowl at Choppa Poke, citing the $20 price tag as a concern for the restaurant's sustainability.  Another customer acknowledged the higher prices compared to competitors but felt the quality and quantity justified the cost.  While the exact number of customers complaining is unknown, the evidence supports the claim that *some* customers have noted high prices.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: At least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's sustainability at that price point [2, 8]. Another customer acknowledged the higher prices compared to competitors but felt they were justified by the quality and quantity of food [3, 6].  The exact number of customers who complained about high prices is not specified in the provided reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms that at least one customer found the prices high, supporting the claim.  While the exact number is unknown, the existence of at least one complaint is sufficient to support the claim 'Some customers have noted that the prices are high'.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mCan we find any customer reviews mentioning negative experiences with customer service for this product/company?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [product name/company name] negative customer service', '[product name/company name] bad customer service experience review']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find any customer reviews mentioning negative experiences with customer service for this product/company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product name/company name] negative customer service', '[product name/company name] bad customer service experience review']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [product name/company name] negative customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[product name/company name] bad customer service experience review\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mCan we find any customer reviews mentioning negative experiences with customer service for this product/company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [product name/company name] negative customer service', '[product name/company name] bad customer service experience review']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mOne customer review describes a negative experience with customer service at Choppa Poke.  The reviewer reported that employees spoke negatively about them in Mandarin, believing they couldn't understand, resulting in a feeling of humiliation [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews for Choppa Poke.  One review gives a 1-star rating and describes a negative experience where employees spoke negatively about the customer in Mandarin, believing they couldn't understand.  This review explicitly mentions feeling humiliated.  Other reviews are positive or neutral, focusing on food quality and portion sizes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service at Choppa Poke, citing specific details from the review.  This aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Can we find any customer reviews mentioning negative experiences with customer service for this product/company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: One customer review describes a negative experience with customer service at Choppa Poke.  The reviewer reported that employees spoke negatively about them in Mandarin, believing they couldn't understand, resulting in a feeling of humiliation [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews for Choppa Poke.  One review gives a 1-star rating and describes a negative experience where employees spoke negatively about the customer in Mandarin, believing they couldn't understand.  This review explicitly mentions feeling humiliated.  Other reviews are positive or neutral, focusing on food quality and portion sizes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by citing a specific customer review that describes a negative experience with customer service. The detail provided strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mSome customers have noted that the prices are high.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided evidence shows that at least one customer complained about the high price of a poke bowl at Choppa Poke, citing the $20 price tag as a concern for the restaurant's sustainability.  Another customer acknowledged the higher prices compared to competitors but felt the quality and quantity justified the cost.  While the exact number of customers complaining is unknown, the evidence supports the claim that *some* customers have noted high prices.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many customers have complained about high prices for this product/service?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAt least one customer commented on the price of a medium poke bowl at Choppa Poke, noting that it cost $20 with tax and expressing concern about the restaurant's sustainability at that price point [2, 8]. Another customer acknowledged the higher prices compared to competitors but felt they were justified by the quality and quantity of food [3, 6].  The exact number of customers who complained about high prices is not specified in the provided reviews. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Choppa Poke.  One review explicitly mentions the price as a factor, stating that a medium bowl costs $20 with tax and questioning the restaurant's long-term viability due to the price [2, 8]. Another review acknowledges the higher price compared to other poke places but justifies it with the generous portions and fresh fish [3, 6].  There is no specific number of complaints about high prices, only individual opinions on the pricing.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mI do feel compelled to dock a star for the prices. A medium will run you $20 with tax. Considering it's delicious and healthy, I'll splurge every now and again, but I'm just not sure how long it can survive. So far so good and I'm glad it's there. Definitely worth checking out.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mChoppa is more expensive when compared to other poke places; however, the price is justified by the fresh fish and generous portions!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mOne reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided QA pair directly addresses the claim.  The answer explicitly states that one customer review details a negative experience with customer service at Choppa Poke, citing specific details from the review.  This aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mCan we find any customer reviews mentioning negative experiences with customer service for this product/company?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mOne customer review describes a negative experience with customer service at Choppa Poke.  The reviewer reported that employees spoke negatively about them in Mandarin, believing they couldn't understand, resulting in a feeling of humiliation [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews for Choppa Poke.  One review gives a 1-star rating and describes a negative experience where employees spoke negatively about the customer in Mandarin, believing they couldn't understand.  This review explicitly mentions feeling humiliated.  Other reviews are positive or neutral, focusing on food quality and portion sizes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWORST experience ever. If you wish to be humiliated, do check them out.\\n \\nOrdered their poke bowl in english and dined in with my husband. Thinking we could not understand mandarin, we later heard one of the female employees speaking to her colleagues pretty loudly about us and mentioning how disgusting we were as customers. In her exact words \"Those two people who just came in, one male and female, having a bowl of poke bowl.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHowever, some customers have noted that the prices are high and one reviewer had a negative experience with customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are supported by the evidence.  The first claim, regarding high prices, is supported by customer reviews mentioning the price point and comparing it to competitors. The second claim, concerning a negative customer service experience, is supported by a review detailing an incident where employees spoke negatively about a customer.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 212 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbe439bafc64a5a99376e440187c4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['correlation wire cutting migrant crossings Texas', 'evidence linking border fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mTexas border wire cutting incidents\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreports of border fence damage Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there were reports that U.S. Customs and Border Patrol agents were cutting concertina wire along the Texas border [1, 2, 4, 5].  The Biden administration stated this was done to provide medical aid to migrants [1, 4], while Texas Attorney General Ken Paxton obtained a temporary restraining order against this action [2, 5]. \n",
      "\n",
      "Reasoning: Multiple sources report that U.S. Customs and Border Patrol agents were cutting concertina wire along the Texas border.  This action was challenged by Texas Attorney General Ken Paxton, who obtained a temporary restraining order.  The Biden administration argued the wire was cut to provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['correlation wire cutting migrant crossings Texas', 'evidence linking border fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcorrelation wire cutting migrant crossings Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mevidence linking border fence damage to illegal immigration Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['correlation wire cutting migrant crossings Texas', 'evidence linking border fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mTexas alleges that federal border agents cut concertina wire along the border, allowing migrants to freely cross [1, 2, 4, 5].  The Biden administration claims the wire was cut to provide medical aid [1, 4].  The provided text does not definitively prove a direct causal link between the wire-cutting and the facilitation of *illegal* migrant crossings, as the motivations behind the wire cutting remain disputed. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  While the documents detail the cutting of the wire and its impact on border security, they do not definitively establish a direct causal link between the wire-cutting incidents and the facilitation of *illegal* migrant crossings.  The documents highlight a disagreement on the motivations behind the wire cutting, with Texas arguing it facilitated illegal crossings and the Biden administration claiming it was for humanitarian reasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that a wire was allegedly being cut to allow migrants to cross into Texas.  The provided question-answer pairs address this claim directly.  The first QA pair confirms that wire cutting incidents occurred along the Texas border, involving U.S. Customs and Border Patrol agents.  The second QA pair addresses the crucial question of whether this wire cutting facilitated illegal crossings.  While Texas alleges this was the case, the Biden administration offers a counter-explanation (providing medical aid).  The evidence does not definitively prove a direct causal link between the wire cutting and the facilitation of *illegal* migrant crossings, as the motivations remain disputed.  The claim is partially supported by the evidence, but the lack of conclusive proof regarding the intent weakens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Were there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there were reports that U.S. Customs and Border Patrol agents were cutting concertina wire along the Texas border [1, 2, 4, 5].  The Biden administration stated this was done to provide medical aid to migrants [1, 4], while Texas Attorney General Ken Paxton obtained a temporary restraining order against this action [2, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This confirms that wire cutting incidents occurred, supporting part of the claim. However, it doesn't establish the intent was to facilitate illegal crossings.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Texas alleges that federal border agents cut concertina wire along the border, allowing migrants to freely cross [1, 2, 4, 5].  The Biden administration claims the wire was cut to provide medical aid [1, 4].  The provided text does not definitively prove a direct causal link between the wire-cutting and the facilitation of *illegal* migrant crossings, as the motivations behind the wire cutting remain disputed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This highlights the dispute over the intent behind the wire cutting.  While Texas alleges it facilitated illegal crossings, the lack of definitive proof weakens the claim's support.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that a wire was allegedly being cut to allow migrants to cross into Texas.  The provided question-answer pairs address this claim directly.  The first QA pair confirms that wire cutting incidents occurred along the Texas border, involving U.S. Customs and Border Patrol agents.  The second QA pair addresses the crucial question of whether this wire cutting facilitated illegal crossings.  While Texas alleges this was the case, the Biden administration offers a counter-explanation (providing medical aid).  The evidence does not definitively prove a direct causal link between the wire cutting and the facilitation of *illegal* migrant crossings, as the motivations remain disputed.  The claim is partially supported by the evidence, but the lack of conclusive proof regarding the intent weakens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there were reports that U.S. Customs and Border Patrol agents were cutting concertina wire along the Texas border [1, 2, 4, 5].  The Biden administration stated this was done to provide medical aid to migrants [1, 4], while Texas Attorney General Ken Paxton obtained a temporary restraining order against this action [2, 5]. \n",
      "\n",
      "Reasoning: Multiple sources report that U.S. Customs and Border Patrol agents were cutting concertina wire along the Texas border.  This action was challenged by Texas Attorney General Ken Paxton, who obtained a temporary restraining order.  The Biden administration argued the wire was cut to provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mTexas alleges that federal border agents cut concertina wire along the border, allowing migrants to freely cross [1, 2, 4, 5].  The Biden administration claims the wire was cut to provide medical aid [1, 4].  The provided text does not definitively prove a direct causal link between the wire-cutting and the facilitation of *illegal* migrant crossings, as the motivations behind the wire cutting remain disputed. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  While the documents detail the cutting of the wire and its impact on border security, they do not definitively establish a direct causal link between the wire-cutting incidents and the facilitation of *illegal* migrant crossings.  The documents highlight a disagreement on the motivations behind the wire cutting, with Texas arguing it facilitated illegal crossings and the Biden administration claiming it was for humanitarian reasons.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that a wire was allegedly being cut to allow migrants to cross into Texas.  The provided question-answer pairs address this claim directly.  The first QA pair confirms that wire cutting incidents occurred along the Texas border, involving U.S. Customs and Border Patrol agents.  The second QA pair addresses the crucial question of whether this wire cutting facilitated illegal crossings.  While Texas alleges this was the case, the Biden administration offers a counter-explanation (providing medical aid).  The evidence does not definitively prove a direct causal link between the wire cutting and the facilitation of *illegal* migrant crossings, as the motivations remain disputed.  The claim is partially supported by the evidence, but the lack of conclusive proof regarding the intent weakens the support.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that a wire was allegedly being cut to allow migrants to cross into Texas.  The provided question-answer pairs address this claim directly.  The first QA pair confirms that wire cutting incidents occurred along the Texas border, involving U.S. Customs and Border Patrol agents.  The second QA pair addresses the crucial question of whether this wire cutting facilitated illegal crossings.  While Texas alleges this was the case, the Biden administration offers a counter-explanation (providing medical aid).  The evidence does not definitively prove a direct causal link between the wire cutting and the facilitation of *illegal* migrant crossings, as the motivations remain disputed.  The claim is partially supported by the evidence, but the lack of conclusive proof regarding the intent weakens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mTexas border wire cutting incidents\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreports of border fence damage Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there were reported incidents of concertina wire being cut along the Texas border.  U.S. Customs and Border Patrol agents were reported to be cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration stated that this was done to provide medical aid to migrants [1, 4], while Texas argued it was unauthorized [1, 2, 4, 5]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. \n",
      "\n",
      "Reasoning: Documents 1, 2, 4, and 5 report on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  The Texas Attorney General claimed that U.S. Customs and Border Patrol agents were cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration countered that the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the removal of the wire, with an exception for medical emergencies [2, 5].  These documents describe incidents of wire being cut, but not necessarily by unauthorized individuals.  The focus is on the legal battle surrounding the actions of border patrol agents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [6] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mborder wire cutting migrant crossings Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mevidence linking fence damage to illegal immigration Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mTexas alleges that federal border agents were cutting concertina wire along the border, allowing illegal crossings [1, 2, 4, 5]. The Biden administration claims the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. While the documents don't definitively prove a direct link, they suggest a correlation, with differing interpretations of the intent behind the wire cutting. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  A judge temporarily halted the wire removal, with an exception for medical emergencies [2, 5].  The documents don't definitively prove a direct link between wire-cutting and facilitating illegal crossings, but they strongly suggest a correlation, with differing interpretations of the intent behind the wire cutting.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mTexas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text describes incidents where concertina wire along the Texas border was cut.  However, the cutting was allegedly performed by U.S. Customs and Border Patrol agents, not by individuals attempting to facilitate illegal crossings.  While Texas alleges this action allowed illegal crossings, the Biden administration claims it was done to provide medical aid.  A judge's intervention further complicates the narrative, suggesting the issue is a subject of legal dispute rather than a straightforward case of wire-cutting to aid illegal crossings.  The evidence points to a complex situation with conflicting accounts of intent and legality, rather than a simple act of facilitating illegal immigration.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Were there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there were reported incidents.  However, the cutting was allegedly done by U.S. Customs and Border Patrol agents, not by individuals facilitating illegal crossings.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Establishes that wire cutting occurred, but refutes the claim's implication of unauthorized individuals facilitating crossings.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Texas alleges a link, but the Biden administration offers a different explanation (medical aid).  A judge's intervention further complicates the situation, making a definitive link unproven.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Highlights the conflicting narratives and lack of definitive proof linking wire cutting to the facilitation of illegal crossings.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text describes incidents where concertina wire along the Texas border was cut.  However, the cutting was allegedly performed by U.S. Customs and Border Patrol agents, not by individuals attempting to facilitate illegal crossings.  While Texas alleges this action allowed illegal crossings, the Biden administration claims it was done to provide medical aid.  A judge's intervention further complicates the narrative, suggesting the issue is a subject of legal dispute rather than a straightforward case of wire-cutting to aid illegal crossings.  The evidence points to a complex situation with conflicting accounts of intent and legality, rather than a simple act of facilitating illegal immigration.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there were reported incidents of concertina wire being cut along the Texas border.  U.S. Customs and Border Patrol agents were reported to be cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration stated that this was done to provide medical aid to migrants [1, 4], while Texas argued it was unauthorized [1, 2, 4, 5]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. \n",
      "\n",
      "Reasoning: Documents 1, 2, 4, and 5 report on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  The Texas Attorney General claimed that U.S. Customs and Border Patrol agents were cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration countered that the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the removal of the wire, with an exception for medical emergencies [2, 5].  These documents describe incidents of wire being cut, but not necessarily by unauthorized individuals.  The focus is on the legal battle surrounding the actions of border patrol agents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [6] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mTexas alleges that federal border agents were cutting concertina wire along the border, allowing illegal crossings [1, 2, 4, 5]. The Biden administration claims the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. While the documents don't definitively prove a direct link, they suggest a correlation, with differing interpretations of the intent behind the wire cutting. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  A judge temporarily halted the wire removal, with an exception for medical emergencies [2, 5].  The documents don't definitively prove a direct link between wire-cutting and facilitating illegal crossings, but they strongly suggest a correlation, with differing interpretations of the intent behind the wire cutting.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mTexas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided text describes incidents where concertina wire along the Texas border was cut.  However, the cutting was allegedly performed by U.S. Customs and Border Patrol agents, not by individuals attempting to facilitate illegal crossings.  While Texas alleges this action allowed illegal crossings, the Biden administration claims it was done to provide medical aid.  A judge's intervention further complicates the narrative, suggesting the issue is a subject of legal dispute rather than a straightforward case of wire-cutting to aid illegal crossings.  The evidence points to a complex situation with conflicting accounts of intent and legality, rather than a simple act of facilitating illegal immigration.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided text describes incidents where concertina wire along the Texas border was cut.  However, the cutting was allegedly performed by U.S. Customs and Border Patrol agents, not by individuals attempting to facilitate illegal crossings.  While Texas alleges this action allowed illegal crossings, the Biden administration claims it was done to provide medical aid.  A judge's intervention further complicates the narrative, suggesting the issue is a subject of legal dispute rather than a straightforward case of wire-cutting to aid illegal crossings.  The evidence points to a complex situation with conflicting accounts of intent and legality, rather than a simple act of facilitating illegal immigration.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mTexas border wire cutting incidents\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreports of border fence damage Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Texas border wire cutting incidents', 'reports of border fence damage Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there were reported incidents of concertina wire being cut along the Texas border.  U.S. Customs and Border Patrol agents were reported to be cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration stated that this was done to provide medical aid to migrants [1, 4], while Texas argued it was unauthorized [1, 2, 4, 5]. A judge issued a temporary restraining order, with exceptions for medical emergencies [2, 5]. \n",
      "\n",
      "Reasoning: Documents 1, 2, 4, and 5 report on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  The Texas Attorney General claimed that U.S. Customs and Border Patrol agents were cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration countered that the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the removal of the wire, with an exception for medical emergencies [2, 5].  These documents describe incidents of wire being cut, but not necessarily by unauthorized individuals.  The focus is on the legal battle surrounding the actions of border patrol agents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [6] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mborder wire cutting migrant crossings Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mevidence linking fence damage to illegal immigration Texas\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['border wire cutting migrant crossings Texas', 'evidence linking fence damage to illegal immigration Texas']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mTexas alleges that federal border agents were cutting concertina wire along the border, allowing illegal crossings [1, 2, 4, 5]. The Biden administration claims the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. While the documents don't definitively prove a direct link, they suggest a correlation between wire cutting and increased illegal crossings, with differing interpretations of the intent behind the wire cutting. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  A judge temporarily halted the wire removal, with an exception for medical emergencies [2, 5].  The documents do not definitively prove a direct link between wire-cutting incidents and the facilitation of illegal crossings, but they strongly suggest a correlation, with differing perspectives on the intent behind the wire cutting.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mTexas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs detail a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal agents were cutting the wire, facilitating illegal crossings. The Biden administration claims the wire was cut to provide medical aid to migrants.  A judge issued a temporary restraining order, with exceptions for medical emergencies. While the sources report wire cutting incidents, they don't definitively prove that the cutting was done *to* facilitate illegal crossings, although a correlation is strongly suggested by Texas's claims.  The differing interpretations of the intent behind the wire cutting make a definitive conclusion difficult.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Were there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there were reported incidents.  U.S. Customs and Border Patrol agents were reported to be cutting the wire, allowing migrants to cross freely. The Biden administration stated this was to provide medical aid, while Texas argued it was unauthorized. A judge issued a temporary restraining order, with exceptions for medical emergencies.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Establishes that wire cutting incidents occurred, but the intent remains disputed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Texas alleges that federal border agents were cutting concertina wire along the border, allowing illegal crossings. The Biden administration claims the wire was cut to provide medical aid to migrants. A judge temporarily halted the wire removal, except in medical emergencies. While the documents don't definitively prove a direct link, they suggest a correlation between wire cutting and increased illegal crossings, with differing interpretations of the intent behind the wire cutting.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Highlights the conflicting narratives and the lack of definitive proof linking wire cutting directly to facilitating illegal crossings, despite a suggested correlation.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mA wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs detail a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal agents were cutting the wire, facilitating illegal crossings. The Biden administration claims the wire was cut to provide medical aid to migrants.  A judge issued a temporary restraining order, with exceptions for medical emergencies. While the sources report wire cutting incidents, they don't definitively prove that the cutting was done *to* facilitate illegal crossings, although a correlation is strongly suggested by Texas's claims.  The differing interpretations of the intent behind the wire cutting make a definitive conclusion difficult.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWere there any reported incidents of wire being cut along the Texas border during the period in question?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there were reported incidents of concertina wire being cut along the Texas border.  U.S. Customs and Border Patrol agents were reported to be cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration stated that this was done to provide medical aid to migrants [1, 4], while Texas argued it was unauthorized [1, 2, 4, 5]. A judge issued a temporary restraining order, with exceptions for medical emergencies [2, 5]. \n",
      "\n",
      "Reasoning: Documents 1, 2, 4, and 5 report on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  The Texas Attorney General claimed that U.S. Customs and Border Patrol agents were cutting the wire, allowing migrants to cross freely [1, 2, 4, 5]. The Biden administration countered that the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the removal of the wire, with an exception for medical emergencies [2, 5].  These documents describe incidents of wire being cut, but not necessarily by unauthorized individuals.  The focus is on the legal battle surrounding the actions of border patrol agents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mAccording to ABC News, Texas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe Biden administration said the wire was cut so they could provide medical aid to migrants.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [6] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there evidence linking these wire-cutting incidents to the facilitation of illegal migrant crossings into Texas?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mTexas alleges that federal border agents were cutting concertina wire along the border, allowing illegal crossings [1, 2, 4, 5]. The Biden administration claims the wire was cut to provide medical aid to migrants [1, 4]. A judge temporarily halted the wire removal, except in medical emergencies [2, 5]. While the documents don't definitively prove a direct link, they suggest a correlation between wire cutting and increased illegal crossings, with differing interpretations of the intent behind the wire cutting. \n",
      "\n",
      "Reasoning: The provided text focuses on a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal border agents were cutting the wire, allowing illegal crossings [1, 2, 4, 5]. The Biden administration counters that the wire was cut to provide medical aid to migrants [1, 4].  A judge temporarily halted the wire removal, with an exception for medical emergencies [2, 5].  The documents do not definitively prove a direct link between wire-cutting incidents and the facilitation of illegal crossings, but they strongly suggest a correlation, with differing perspectives on the intent behind the wire cutting.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mTexas states border agents cut through the wire allowing migrants to freely enter Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mTexas states border agents were \"cutting, destroying, or otherwise damaging Texas’s concertina wire that had been strategically positioned for the purpose of securing the border and stemming the flow of illegal migration.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mU.S. Customs and Border Patrol agents were cutting concertina wire allowing migrants to freely cross the southern border.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe Court shall grant the temporary relief requested, with one important exception for any medical emergency that mostly likely results in serious bodily injury or death to a person, absent any boats or other life-saving apparatus available to avoid such medical emergencies prior to reaching the concertina wire barrier.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs detail a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal agents were cutting the wire, facilitating illegal crossings. The Biden administration claims the wire was cut to provide medical aid to migrants.  A judge issued a temporary restraining order, with exceptions for medical emergencies. While the sources report wire cutting incidents, they don't definitively prove that the cutting was done *to* facilitate illegal crossings, although a correlation is strongly suggested by Texas's claims.  The differing interpretations of the intent behind the wire cutting make a definitive conclusion difficult.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe wire was allegedly being cut to allow migrants to cross into Texas.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs detail a legal dispute between Texas and the Biden administration regarding the cutting of concertina wire along the Texas border.  Texas alleges that federal agents were cutting the wire, facilitating illegal crossings. The Biden administration claims the wire was cut to provide medical aid to migrants.  A judge issued a temporary restraining order, with exceptions for medical emergencies. While the sources report wire cutting incidents, they don't definitively prove that the cutting was done *to* facilitate illegal crossings, although a correlation is strongly suggested by Texas's claims.  The differing interpretations of the intent behind the wire cutting make a definitive conclusion difficult.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 213 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c2e1ce8534762b51b5c4aef991361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the rationale or purpose behind burning the blue field and burying the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burial of ashes', 'environmental reasons for burning fields']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mdefinition blue field\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field symbolism\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field agriculture\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [4/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field art\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text focuses on the proper disposal of a worn American flag.  The documents repeatedly mention removing the \"blue field\" from the flag before disposal.  This implies that the \"blue field\" refers to the blue rectangular section of the US flag containing the stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or purpose behind burning the blue field and burying the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burial of ashes', 'environmental reasons for burning fields']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mritualistic burning of fields\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymbolic burial of ashes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/3] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33menvironmental reasons for burning fields\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or purpose behind burning the blue field and burying the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burial of ashes', 'environmental reasons for burning fields']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe rationale behind burning the blue field of a worn American flag and burying the ashes is a respectful way to retire a flag that has served its purpose [1, 2, 3].  This method honors the flag's representation of the nation. \n",
      "\n",
      "Reasoning: The provided text describes the proper disposal of a worn American flag.  One method involves removing the blue field, burning it peacefully, and burying the ashes.  The rationale is not explicitly stated, but it can be inferred that this is a respectful way to retire a flag that has served its purpose, honoring its representation of the nation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim advocates for a peaceful burning and burial of the blue field of a flag.  The provided question-answer pairs confirm that the \"blue field\" refers to the blue section of the American flag with the stars.  Furthermore, the answers explain that burning and burying the ashes is a respectful method of disposing of a worn flag, aligning with the claim's suggestion.  The rationale provided emphasizes the respectful nature of this disposal method, which is consistent with the claim's peaceful approach.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This clarifies the terminology used in the claim, confirming the subject of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the rationale or purpose behind burning the blue field and burying the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The rationale behind burning the blue field of a worn American flag and burying the ashes is a respectful way to retire a flag that has served its purpose [1, 2, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This directly supports the claim by providing the rationale for the suggested method of disposal, emphasizing its respectful nature which aligns with the claim's 'peaceful' description.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim advocates for a peaceful burning and burial of the blue field of a flag.  The provided question-answer pairs confirm that the \"blue field\" refers to the blue section of the American flag with the stars.  Furthermore, the answers explain that burning and burying the ashes is a respectful method of disposing of a worn flag, aligning with the claim's suggestion.  The rationale provided emphasizes the respectful nature of this disposal method, which is consistent with the claim's peaceful approach.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text focuses on the proper disposal of a worn American flag.  The documents repeatedly mention removing the \"blue field\" from the flag before disposal.  This implies that the \"blue field\" refers to the blue rectangular section of the US flag containing the stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the rationale or purpose behind burning the blue field and burying the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe rationale behind burning the blue field of a worn American flag and burying the ashes is a respectful way to retire a flag that has served its purpose [1, 2, 3].  This method honors the flag's representation of the nation. \n",
      "\n",
      "Reasoning: The provided text describes the proper disposal of a worn American flag.  One method involves removing the blue field, burning it peacefully, and burying the ashes.  The rationale is not explicitly stated, but it can be inferred that this is a respectful way to retire a flag that has served its purpose, honoring its representation of the nation.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim advocates for a peaceful burning and burial of the blue field of a flag.  The provided question-answer pairs confirm that the \"blue field\" refers to the blue section of the American flag with the stars.  Furthermore, the answers explain that burning and burying the ashes is a respectful method of disposing of a worn flag, aligning with the claim's suggestion.  The rationale provided emphasizes the respectful nature of this disposal method, which is consistent with the claim's peaceful approach.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim advocates for a peaceful burning and burial of the blue field of a flag.  The provided question-answer pairs confirm that the \"blue field\" refers to the blue section of the American flag with the stars.  Furthermore, the answers explain that burning and burying the ashes is a respectful method of disposing of a worn flag, aligning with the claim's suggestion.  The rationale provided emphasizes the respectful nature of this disposal method, which is consistent with the claim's peaceful approach.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burning of blue', 'environmental impact of burning fields', 'cultural significance of burying ashes']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mdefinition blue field\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field symbolism\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field agriculture\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [4/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field art\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field art']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text focuses on the proper disposal of a worn American flag.  The documents repeatedly mention removing the \"blue field\" from the flag before disposal.  This implies that the \"blue field\" refers to the blue rectangular section of the US flag containing the stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burning of blue', 'environmental impact of burning fields', 'cultural significance of burying ashes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mritualistic burning of fields\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymbolic burning of blue\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33menvironmental impact of burning fields\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [4/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcultural significance of burying ashes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ritualistic burning of fields', 'symbolic burning of blue', 'environmental impact of burning fields', 'cultural significance of burying ashes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe suggestion to burn the blue field of a worn American flag peacefully and bury the ashes is a respectful way to dispose of the flag after it has served its purpose [1, 2, 3, 4]. This method is presented as one option for honoring the flag's service to the nation. \n",
      "\n",
      "Reasoning: The provided text describes three ways to dispose of a worn American flag. One method involves removing the blue field, burning it peacefully, and burying the ashes [1, 2, 3, 4].  The rationale isn't explicitly stated, but it's implied to be a respectful way to retire a flag that has served its purpose, honoring its service to the nation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim suggests burning the blue field of a flag peacefully and burying the ashes.  The provided question-answer pairs confirm that this is a method of respectfully disposing of a worn American flag, presented as one option among others.  The rationale is implied to be a respectful way to retire the flag, honoring its service.  The answers directly support the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Defines the term 'blue field' within the context of flag disposal, clarifying the claim's subject.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The suggestion to burn the blue field of a worn American flag peacefully and bury the ashes is a respectful way to dispose of the flag after it has served its purpose [1, 2, 3, 4]. This method is presented as one option for honoring the flag's service to the nation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by stating that burning the blue field and burying the ashes is a respectful disposal method.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim suggests burning the blue field of a flag peacefully and burying the ashes.  The provided question-answer pairs confirm that this is a method of respectfully disposing of a worn American flag, presented as one option among others.  The rationale is implied to be a respectful way to retire the flag, honoring its service.  The answers directly support the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text focuses on the proper disposal of a worn American flag.  The documents repeatedly mention removing the \"blue field\" from the flag before disposal.  This implies that the \"blue field\" refers to the blue rectangular section of the US flag containing the stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe suggestion to burn the blue field of a worn American flag peacefully and bury the ashes is a respectful way to dispose of the flag after it has served its purpose [1, 2, 3, 4]. This method is presented as one option for honoring the flag's service to the nation. \n",
      "\n",
      "Reasoning: The provided text describes three ways to dispose of a worn American flag. One method involves removing the blue field, burning it peacefully, and burying the ashes [1, 2, 3, 4].  The rationale isn't explicitly stated, but it's implied to be a respectful way to retire a flag that has served its purpose, honoring its service to the nation.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests burning the blue field of a flag peacefully and burying the ashes.  The provided question-answer pairs confirm that this is a method of respectfully disposing of a worn American flag, presented as one option among others.  The rationale is implied to be a respectful way to retire the flag, honoring its service.  The answers directly support the claim's assertion.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests burning the blue field of a flag peacefully and burying the ashes.  The provided question-answer pairs confirm that this is a method of respectfully disposing of a worn American flag, presented as one option among others.  The rationale is implied to be a respectful way to retire the flag, honoring its service.  The answers directly support the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field context']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['burning blue field ritual', 'symbolic burning of fields', 'ash burial significance', 'environmental impact burning field']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field context']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mdefinition blue field\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field symbolism\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field agriculture\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [4/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mblue field context\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['definition blue field', 'blue field symbolism', 'blue field agriculture', 'blue field context']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4].  This section is to be removed before disposal of the rest of the flag [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text discusses the proper disposal of a worn American flag.  The documents consistently refer to \"blue field\" as the blue rectangular section of the flag containing the stars.  The instructions for disposal specify that this section should be removed before the rest of the flag is disposed of, and then burned respectfully, with the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['burning blue field ritual', 'symbolic burning of fields', 'ash burial significance', 'environmental impact burning field']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mburning blue field ritual\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymbolic burning of fields\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [3/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mash burial significance\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [4/4] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33menvironmental impact burning field\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['burning blue field ritual', 'symbolic burning of fields', 'ash burial significance', 'environmental impact burning field']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe rationale for burning the blue field of a worn American flag peacefully and burying the ashes is not explicitly stated, but it's likely a respectful way to retire a flag that has served its purpose [1].  Burning may symbolize the flag's transition from service, and burying the ashes could represent a final resting place, honoring its past service. \n",
      "\n",
      "Reasoning: The provided text describes three ways to dispose of a worn American flag. One method involves removing the blue field, burning it peacefully, and burying the ashes.  The rationale isn't explicitly stated, but it can be inferred that this is a respectful way to retire a flag that has served its purpose, treating it with dignity even in its disposal.  The act of burning might symbolize the flag's transition from service, and burying the ashes could represent a final resting place, honoring its past service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim advocates for the peaceful burning and burial of the blue field of a flag.  The provided Q&A pairs confirm that the \"blue field\" refers to the blue section of the American flag containing the stars, and that this is a common practice in flag retirement.  While the rationale behind burning and burying isn't explicitly stated in the sources, the answers suggest it's a respectful way to dispose of a worn flag, symbolizing its transition from service and providing a final resting place.  The answers strongly support the claim's method of disposal, although they don't explicitly endorse the claim's phrasing of \"peaceful\" burning.  However, the context implies a respectful and dignified process.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4].  This section is to be removed before disposal of the rest of the flag [1, 2, 3, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer defines the 'blue field' and confirms that it's a part of the flag commonly disposed of separately.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The rationale for burning the blue field of a worn American flag peacefully and burying the ashes is not explicitly stated, but it's likely a respectful way to retire a flag that has served its purpose [1].  Burning may symbolize the flag's transition from service, and burying the ashes could represent a final resting place, honoring its past service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer explains the likely reasoning behind the practice, suggesting it's a respectful and symbolic act.  While not explicitly stated, the inference strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim advocates for the peaceful burning and burial of the blue field of a flag.  The provided Q&A pairs confirm that the \"blue field\" refers to the blue section of the American flag containing the stars, and that this is a common practice in flag retirement.  While the rationale behind burning and burying isn't explicitly stated in the sources, the answers suggest it's a respectful way to dispose of a worn flag, symbolizing its transition from service and providing a final resting place.  The answers strongly support the claim's method of disposal, although they don't explicitly endorse the claim's phrasing of \"peaceful\" burning.  However, the context implies a respectful and dignified process.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is meant by 'blue field' in this context?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn the context of disposing of a worn American flag, the 'blue field' refers to the blue rectangular section of the flag containing the stars [1, 2, 3, 4].  This section is to be removed before disposal of the rest of the flag [1, 2, 3, 4]. \n",
      "\n",
      "Reasoning: The provided text discusses the proper disposal of a worn American flag.  The documents consistently refer to \"blue field\" as the blue rectangular section of the flag containing the stars.  The instructions for disposal specify that this section should be removed before the rest of the flag is disposed of, and then burned respectfully, with the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the rationale or reason behind the suggestion to burn the blue field peacefully and bury the ashes?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe rationale for burning the blue field of a worn American flag peacefully and burying the ashes is not explicitly stated, but it's likely a respectful way to retire a flag that has served its purpose [1].  Burning may symbolize the flag's transition from service, and burying the ashes could represent a final resting place, honoring its past service. \n",
      "\n",
      "Reasoning: The provided text describes three ways to dispose of a worn American flag. One method involves removing the blue field, burning it peacefully, and burying the ashes.  The rationale isn't explicitly stated, but it can be inferred that this is a respectful way to retire a flag that has served its purpose, treating it with dignity even in its disposal.  The act of burning might symbolize the flag's transition from service, and burying the ashes could represent a final resting place, honoring its past service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDisposal of a faded or worn American flag is to be done in one of three ways. You can, if necessary, put the flag out for collection only after you have removed the blue field from the flag. The blue field is to be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim advocates for the peaceful burning and burial of the blue field of a flag.  The provided Q&A pairs confirm that the \"blue field\" refers to the blue section of the American flag containing the stars, and that this is a common practice in flag retirement.  While the rationale behind burning and burying isn't explicitly stated in the sources, the answers suggest it's a respectful way to dispose of a worn flag, symbolizing its transition from service and providing a final resting place.  The answers strongly support the claim's method of disposal, although they don't explicitly endorse the claim's phrasing of \"peaceful\" burning.  However, the context implies a respectful and dignified process.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe blue field should be burned in a peaceful manner and the ashes buried.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim advocates for the peaceful burning and burial of the blue field of a flag.  The provided Q&A pairs confirm that the \"blue field\" refers to the blue section of the American flag containing the stars, and that this is a common practice in flag retirement.  While the rationale behind burning and burying isn't explicitly stated in the sources, the answers suggest it's a respectful way to dispose of a worn flag, symbolizing its transition from service and providing a final resting place.  The answers strongly support the claim's method of disposal, although they don't explicitly endorse the claim's phrasing of \"peaceful\" burning.  However, the context implies a respectful and dignified process.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 214 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ab663ad1849c391f8c46c5f441753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msirloin steak location cow anatomy diagram\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2 mentions that Porterhouse steaks are cut from further back on the sirloin, providing additional context about the sirloin's location relative to other cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mshort loin steak cuts\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mT-bone porterhouse club steak origin\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by stating the sirloin's location and its connection to the short loin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Which other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: T-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the claim by listing the other steak cuts originating from the short loin, as stated in the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2 mentions that Porterhouse steaks are cut from further back on the sirloin, providing additional context about the sirloin's location relative to other cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mSirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msirloin steak location cow anatomy diagram\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2, while focusing on the difference between sirloin and porterhouse steaks, indirectly supports this location by mentioning that porterhouse steaks are cut from further back on the sirloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mshort loin steak cuts\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mT-bone porterhouse club steak origin\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mSirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Both answers align perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by specifying the sirloin's location and its connection to the short loin, which is also the source of other mentioned steaks.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Which other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: T-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the claim by listing the other steak cuts originating from the short loin, as stated in the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mSirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Both answers align perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2, while focusing on the difference between sirloin and porterhouse steaks, indirectly supports this location by mentioning that porterhouse steaks are cut from further back on the sirloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Both answers align perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Both answers align perfectly with the claim's assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msirloin steak location cow anatomy diagram\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['sirloin steak location cow anatomy diagram']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2 mentions that Porterhouse steaks are cut from further back on the sirloin, providing additional context about the sirloin's location relative to other cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mshort loin steak cuts\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mT-bone porterhouse club steak origin\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['short loin steak cuts', 'T-bone porterhouse club steak origin']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by stating the sirloin's location and its connection to the short loin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Which other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: T-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the claim's assertion about the short loin being the source of T-bone, porterhouse, and club steaks.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the anatomical location of a sirloin steak on a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe sirloin steak is cut from the rear back portion of the cow, extending from the short loin where T-bone, porterhouse, and club steaks are also cut [1]. \n",
      "\n",
      "Reasoning: Document 1 states that the sirloin steak is cut from the rear back portion of the animal, continuing off the short loin.  This is consistent with general knowledge of beef cuts.  Document 2 mentions that Porterhouse steaks are cut from further back on the sirloin, providing additional context about the sirloin's location relative to other cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhich other steak cuts are derived from the short loin of a cow?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mT-bone, porterhouse, and club steaks are all derived from the short loin of a cow [1, 3].  A porterhouse steak is simply a larger version of a T-bone steak [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, T-bone, porterhouse, and club steaks are all cut from the short loin of a cow [1, 3].  The documents also mention that a porterhouse steak is a larger version of a T-bone steak [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe sirloin steak is a steak cut from the back of the animal. In a common U.S. butchery, the steak is cut from the rear back portion of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mT-Bone steaks and Porterhouse steaks are the same.  The Porterhouse is just a larger version of the T-Bone because it is carved from the larger portion of the tenderloin.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOn the other hand, the sirloin steak is a steak cut from the back of the animal, continuing off the short loin from which T-bone, porterhouse, and club steaks are cut.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair explicitly states that the sirloin steak is cut from the rear back portion of the cow, extending from the short loin, which is also the source of T-bone, porterhouse, and club steaks. The second QA pair confirms that T-bone, porterhouse, and club steaks are indeed derived from the short loin.  Therefore, the claim accurately reflects the anatomical location and relationship between the sirloin and other specified steak cuts.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 215 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c58d244e77d487da875ada036cb196b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant name customer reviews rating', '[restaurant name] reviews stars']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', '[restaurant name] reviews stars']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant name customer reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] reviews stars\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', '[restaurant name] reviews stars']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Ming Dynasty Restaurant has an average star rating of 3.0 [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains review data for a restaurant, including star ratings.  Document [2] and [4] show that the Ming Dynasty Restaurant has a business star rating of 3.0.  Document [1] and [3] show individual customer reviews with ratings of 5.0, but don't provide an average.  Therefore, the only available average star rating is the overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'business_stars': 3.0\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'business_stars': 3.0\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states the restaurant has a 3.0-star rating. The provided QA pair directly addresses this, stating that the Ming Dynasty Restaurant has an average star rating of 3.0 based on the information in documents [2] and [4].  The reasoning in the QA pair explains how this rating was determined, considering both individual reviews and the overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Ming Dynasty Restaurant has an average star rating of 3.0 [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains review data for a restaurant, including star ratings.  Document [2] and [4] show that the Ming Dynasty Restaurant has a business star rating of 3.0.  Document [1] and [3] show individual customer reviews with ratings of 5.0, but don't provide an average.  Therefore, the only available average star rating is the overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly supports the claim by explicitly stating that the restaurant's average star rating is 3.0, citing supporting documents. The reasoning further clarifies how this average was obtained.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant menu [restaurant name]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mBest Chinese food and buffet in town. A wide variety of choices, all very tasty.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mJust had dinner here with some friends and family. The service was great. The food was fantastic. I had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mWatch out for old waitress trying to persuade you into ordering expensive dish. We got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] menu Mongolian\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] Mongolian dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as 'Mongolian cuisine' [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu, or at least its categorization system, explicitly includes \"Mongolian\" as a cuisine type.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.7\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The provided Q&A reveals that while the restaurant (Ming Dynasty Restaurant) serves Chinese food and has a wide variety of dishes (though specifics aren't listed), it explicitly categorizes some dishes as \"Mongolian cuisine\".  The evidence directly supports the inclusion of Mongolian cuisine, but the variety and specifics of traditional Chinese dishes remain unclear due to a lack of detailed menu information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer confirms the restaurant serves Chinese food but lacks specifics on the variety of traditional dishes.  This weakens the support for the 'variety' aspect of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as 'Mongolian cuisine' [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu, or at least its categorization system, explicitly includes \"Mongolian\" as a cuisine type.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer strongly supports the claim's assertion that the restaurant includes Mongolian cuisine on its menu.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims are supported by the evidence. The first claim is supported by the average star rating found in the provided reviews. The second claim is supported by the restaurant's categorization as offering both traditional Chinese dishes and Mongolian cuisine.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states the restaurant has a 3.0-star rating. The provided QA pair directly addresses this, stating that the Ming Dynasty Restaurant has an average star rating of 3.0 based on the information in documents [2] and [4].  The reasoning in the QA pair explains how this rating was determined, considering both individual reviews and the overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Ming Dynasty Restaurant has an average star rating of 3.0 [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains review data for a restaurant, including star ratings.  Document [2] and [4] show that the Ming Dynasty Restaurant has a business star rating of 3.0.  Document [1] and [3] show individual customer reviews with ratings of 5.0, but don't provide an average.  Therefore, the only available average star rating is the overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'business_stars': 3.0\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'business_stars': 3.0\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.7\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The provided Q&A reveals that while the restaurant (Ming Dynasty Restaurant) serves Chinese food and has a wide variety of dishes (though specifics aren't listed), it explicitly categorizes some dishes as \"Mongolian cuisine\".  The evidence directly supports the inclusion of Mongolian cuisine, but the variety and specifics of traditional Chinese dishes remain unclear due to a lack of detailed menu information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mBest Chinese food and buffet in town. A wide variety of choices, all very tasty.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mJust had dinner here with some friends and family. The service was great. The food was fantastic. I had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mWatch out for old waitress trying to persuade you into ordering expensive dish. We got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as 'Mongolian cuisine' [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu, or at least its categorization system, explicitly includes \"Mongolian\" as a cuisine type.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims are supported by the evidence. The first claim is supported by the average star rating found in the provided reviews. The second claim is supported by the restaurant's categorization as offering both traditional Chinese dishes and Mongolian cuisine.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims are supported by the evidence. The first claim is supported by the average star rating found in the provided reviews. The second claim is supported by the restaurant's categorization as offering both traditional Chinese dishes and Mongolian cuisine.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant name customer reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant name average star rating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews. \n",
      "\n",
      "Reasoning: The provided text mentions two different star ratings. One document shows a business star rating of 3.0 for Ming Dynasty Restaurant [2, 4]. Another document includes individual customer reviews with ratings of 5.0 [1, 3], but doesn't provide an average.  To answer the question accurately, I need more information to calculate the average star rating from all available customer reviews.  The provided data only gives snippets of reviews and a single overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Ming Dynasty Restaurant', 'address': '290 Storke Rd, Ste G', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Mongolian, Chinese', 'hours': {'Monday': '11:0-21:30', 'Tuesday': '11:0-21:30', 'Thursday': '11:0-21:30', 'Friday': '11:0-22:0', 'Saturday': '11:30-22:0', 'Sunday': '11:30-21:30'}, 'attributes': {'BusinessParking': {'garage': True, 'street': False, 'validated': True, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': False, 'WiFi': 'no', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'touristy': False, 'hipster': False, 'divey': False, 'classy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.0, 'review_info': [{'review_stars': 1.0, 'review_date': '2019-09-24 04:12:09', 'review_text': 'Been here a few times because there are really not that many Chinese foods in town.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text states that the Ming Dynasty Restaurant has a business star rating of 3.0. However, it also notes that there is insufficient data to calculate the average star rating based on customer reviews, as only snippets of individual reviews (with a rating of 5.0 in one instance) are available, not a complete dataset of all reviews.  Therefore, while a business rating of 3.0 exists, it's unclear if this reflects the average of all customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly states insufficient data prevents calculating the average customer review rating. While a business rating of 3.0 is mentioned, it's not confirmed to be the average of customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific traditional Chinese dishes are offered by the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes list']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] website menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes list']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant menu [restaurant name]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] dishes list\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes list']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese cuisine. One review mentions roasted duck as a dish available [1], and another mentions a buffet with a wide variety of choices [2], but a complete menu is not provided. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list the specific traditional Chinese dishes offered on their menu.  One review mentions roasted duck as a dish available, but this is not an exhaustive list.  Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWe got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] website menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] menu Mongolian\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] website menu\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] website menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available information, the restaurant's categorization includes \"Mongolian\" cuisine [1, 3]. However, this is from a restaurant listing and not a direct examination of the menu itself.  The provided text does not offer details on how the menu is explicitly organized. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's categorization includes \"Mongolian\" cuisine.  However, this information comes from a listing of the restaurant's categories and not necessarily from an explicit menu categorization.  The reviews do not provide information about the menu's organization.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The provided Q&A pairs offer some support but also highlight limitations.  The first Q&A shows that while the restaurant serves Chinese food, the specific dishes are not fully listed, only roasted duck is mentioned as an example. The second Q&A reveals that restaurant listings categorize the restaurant as offering Mongolian cuisine, but this is not confirmed by a direct examination of the menu.  Therefore, the claim is partially supported regarding Chinese cuisine but lacks sufficient evidence to definitively support the inclusion of Mongolian cuisine.  The evidence suggests the restaurant likely offers some Chinese dishes, but the variety and the inclusion of Mongolian cuisine remain uncertain due to a lack of comprehensive menu information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific traditional Chinese dishes are offered by the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available information, the Ming Dynasty Restaurant offers Chinese cuisine. One review mentions roasted duck as a dish available [1], and another mentions a buffet with a wide variety of choices [2], but a complete menu is not provided.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Partially supports the claim by confirming the restaurant serves Chinese food, but lacks detail on the variety of dishes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available information, the restaurant's categorization includes \"Mongolian\" cuisine [1, 3]. However, this is from a restaurant listing and not a direct examination of the menu itself.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Weakly supports the inclusion of Mongolian cuisine, but the evidence is indirect and not conclusive.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims are unsupported. The first claim lacks sufficient data to verify the 3.0-star rating based on customer reviews. The second claim, while mentioning Mongolian and Chinese cuisine, doesn't provide a definitive menu to confirm the variety and inclusion of specific dishes.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text states that the Ming Dynasty Restaurant has a business star rating of 3.0. However, it also notes that there is insufficient data to calculate the average star rating based on customer reviews, as only snippets of individual reviews (with a rating of 5.0 in one instance) are available, not a complete dataset of all reviews.  Therefore, while a business rating of 3.0 exists, it's unclear if this reflects the average of all customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews. \n",
      "\n",
      "Reasoning: The provided text mentions two different star ratings. One document shows a business star rating of 3.0 for Ming Dynasty Restaurant [2, 4]. Another document includes individual customer reviews with ratings of 5.0 [1, 3], but doesn't provide an average.  To answer the question accurately, I need more information to calculate the average star rating from all available customer reviews.  The provided data only gives snippets of reviews and a single overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Ming Dynasty Restaurant', 'address': '290 Storke Rd, Ste G', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Mongolian, Chinese', 'hours': {'Monday': '11:0-21:30', 'Tuesday': '11:0-21:30', 'Thursday': '11:0-21:30', 'Friday': '11:0-22:0', 'Saturday': '11:30-22:0', 'Sunday': '11:30-21:30'}, 'attributes': {'BusinessParking': {'garage': True, 'street': False, 'validated': True, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': False, 'WiFi': 'no', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'touristy': False, 'hipster': False, 'divey': False, 'classy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.0, 'review_info': [{'review_stars': 1.0, 'review_date': '2019-09-24 04:12:09', 'review_text': 'Been here a few times because there are really not that many Chinese foods in town.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The provided Q&A pairs offer some support but also highlight limitations.  The first Q&A shows that while the restaurant serves Chinese food, the specific dishes are not fully listed, only roasted duck is mentioned as an example. The second Q&A reveals that restaurant listings categorize the restaurant as offering Mongolian cuisine, but this is not confirmed by a direct examination of the menu.  Therefore, the claim is partially supported regarding Chinese cuisine but lacks sufficient evidence to definitively support the inclusion of Mongolian cuisine.  The evidence suggests the restaurant likely offers some Chinese dishes, but the variety and the inclusion of Mongolian cuisine remain uncertain due to a lack of comprehensive menu information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific traditional Chinese dishes are offered by the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese cuisine. One review mentions roasted duck as a dish available [1], and another mentions a buffet with a wide variety of choices [2], but a complete menu is not provided. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list the specific traditional Chinese dishes offered on their menu.  One review mentions roasted duck as a dish available, but this is not an exhaustive list.  Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWe got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available information, the restaurant's categorization includes \"Mongolian\" cuisine [1, 3]. However, this is from a restaurant listing and not a direct examination of the menu itself.  The provided text does not offer details on how the menu is explicitly organized. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's categorization includes \"Mongolian\" cuisine.  However, this information comes from a listing of the restaurant's categories and not necessarily from an explicit menu categorization.  The reviews do not provide information about the menu's organization.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims are unsupported. The first claim lacks sufficient data to verify the 3.0-star rating based on customer reviews. The second claim, while mentioning Mongolian and Chinese cuisine, doesn't provide a definitive menu to confirm the variety and inclusion of specific dishes.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims are unsupported. The first claim lacks sufficient data to verify the 3.0-star rating based on customer reviews. The second claim, while mentioning Mongolian and Chinese cuisine, doesn't provide a definitive menu to confirm the variety and inclusion of specific dishes.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant name customer reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant name average star rating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant name customer reviews rating', 'restaurant name average star rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews. \n",
      "\n",
      "Reasoning: The provided text mentions two different star ratings. One document shows a business star rating of 3.0 for Ming Dynasty Restaurant [2, 4]. Another document includes individual customer reviews with ratings of 5.0 [1, 3], but doesn't provide an average.  To answer the question accurately, I need more information to calculate the average star rating from all available customer reviews.  The provided data only gives snippets of reviews and a single overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Ming Dynasty Restaurant', 'address': '290 Storke Rd, Ste G', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Mongolian, Chinese', 'hours': {'Monday': '11:0-21:30', 'Tuesday': '11:0-21:30', 'Thursday': '11:0-21:30', 'Friday': '11:0-22:0', 'Saturday': '11:30-22:0', 'Sunday': '11:30-21:30'}, 'attributes': {'BusinessParking': {'garage': True, 'street': False, 'validated': True, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': False, 'WiFi': 'no', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'touristy': False, 'hipster': False, 'divey': False, 'classy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.0, 'review_info': [{'review_stars': 1.0, 'review_date': '2019-09-24 04:12:09', 'review_text': 'Been here a few times because there are really not that many Chinese foods in town.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text states that the Ming Dynasty Restaurant has a business star rating of 3.0. However, it also notes that there is insufficient data to calculate the average star rating based on customer reviews, as only snippets of individual reviews (with a rating of 5.0 in one instance) are available, not a complete dataset of all reviews.  Therefore, while a business rating of 3.0 exists, it's unclear if this reflects the average of all customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly states insufficient data prevents calculating the average customer review rating. While a business rating of 3.0 is mentioned, it's not confirmed to be the average of customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant menu [restaurant name]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant menu [restaurant name]', '[restaurant name] dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [1, 3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mBest Chinese food and buffet in town. A wide variety of choices, all very tasty.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mWe got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mI had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] menu Mongolian\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[restaurant name] Mongolian dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[restaurant name] menu Mongolian', '[restaurant name] Mongolian dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as Mongolian cuisine [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu likely includes dishes categorized as Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The first QA pair reveals that while the restaurant serves Chinese food, the specific dishes are not listed.  One review mentions roasted duck, but this isn't confirmed as a regular item. The second QA pair confirms that the restaurant's menu explicitly categorizes some dishes as Mongolian cuisine.  Therefore, the claim is partially supported. The evidence supports the inclusion of Mongolian cuisine, but the variety of traditional Chinese dishes is not fully substantiated due to a lack of specific menu details.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [1, 3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer shows the restaurant serves Chinese food but lacks specific details about the variety of dishes.  It weakens the support for the 'variety of traditional Chinese dishes' part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as Mongolian cuisine [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu likely includes dishes categorized as Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer strongly supports the claim's assertion that the restaurant includes Mongolian cuisine on its menu.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement makes two claims: one about the restaurant's star rating and another about its menu. The claim regarding the 3.0-star rating is unsupported because there's insufficient data to confirm the average star rating based on customer reviews.  The claim about the restaurant offering a variety of traditional Chinese dishes, including Mongolian cuisine, is partially supported. While the evidence confirms the presence of Mongolian dishes on the menu, the extent of the variety of traditional Chinese dishes offered remains unclear due to a lack of detailed menu information.  Since one claim is unsupported and the other is only partially supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant has a rating of 3.0 stars based on customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text states that the Ming Dynasty Restaurant has a business star rating of 3.0. However, it also notes that there is insufficient data to calculate the average star rating based on customer reviews, as only snippets of individual reviews (with a rating of 5.0 in one instance) are available, not a complete dataset of all reviews.  Therefore, while a business rating of 3.0 exists, it's unclear if this reflects the average of all customer reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average star rating of the restaurant based on customer reviews?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on the available data, Ming Dynasty Restaurant has a business star rating of 3.0 [2, 4].  However, there is insufficient data to calculate the average star rating based on customer reviews. \n",
      "\n",
      "Reasoning: The provided text mentions two different star ratings. One document shows a business star rating of 3.0 for Ming Dynasty Restaurant [2, 4]. Another document includes individual customer reviews with ratings of 5.0 [1, 3], but doesn't provide an average.  To answer the question accurately, I need more information to calculate the average star rating from all available customer reviews.  The provided data only gives snippets of reviews and a single overall business rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Ming Dynasty Restaurant', 'address': '290 Storke Rd, Ste G', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Mongolian, Chinese', 'hours': {'Monday': '11:0-21:30', 'Tuesday': '11:0-21:30', 'Thursday': '11:0-21:30', 'Friday': '11:0-22:0', 'Saturday': '11:30-22:0', 'Sunday': '11:30-21:30'}, 'attributes': {'BusinessParking': {'garage': True, 'street': False, 'validated': True, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': False, 'WiFi': 'no', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'touristy': False, 'hipster': False, 'divey': False, 'classy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.0, 'review_info': [{'review_stars': 1.0, 'review_date': '2019-09-24 04:12:09', 'review_text': 'Been here a few times because there are really not that many Chinese foods in town.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a variety of traditional Chinese dishes, including Mongolian cuisine.  The first QA pair reveals that while the restaurant serves Chinese food, the specific dishes are not listed.  One review mentions roasted duck, but this isn't confirmed as a regular item. The second QA pair confirms that the restaurant's menu explicitly categorizes some dishes as Mongolian cuisine.  Therefore, the claim is partially supported. The evidence supports the inclusion of Mongolian cuisine, but the variety of traditional Chinese dishes is not fully substantiated due to a lack of specific menu details.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific traditional Chinese dishes are offered by this restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available information, the Ming Dynasty Restaurant offers Chinese food, including a buffet with a wide variety of choices [1, 3]. One review mentions roasted duck as a dish available at the restaurant [1, 3], but this is not confirmed as a regular menu item.  The specific traditional Chinese dishes offered are not listed. \n",
      "\n",
      "Reasoning: The provided text mentions a restaurant, Ming Dynasty Restaurant, that serves Chinese food.  However, the documents don't list a specific menu of traditional Chinese dishes.  One review mentions roasted duck, but this is not confirmed as a regular menu item. Other reviews mention a buffet with a \"wide variety of choices,\" but the specific dishes are not named.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mBest Chinese food and buffet in town. A wide variety of choices, all very tasty.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mWe got tricked one time and ended up ordering roasted duck but we just wanted a bowl of noodles in the first place.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mI had the buffet and I had many choices.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the restaurant's menu explicitly categorize any dishes as 'Mongolian cuisine'?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, the Ming Dynasty Restaurant's menu explicitly categorizes some dishes as Mongolian cuisine [1]. \n",
      "\n",
      "Reasoning: The provided text mentions the restaurant \"Ming Dynasty Restaurant\" and lists its categories as \"Restaurants, Mongolian, Chinese\".  This indicates that the restaurant's menu likely includes dishes categorized as Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'categories': 'Restaurants, Mongolian, Chinese'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement makes two claims: one about the restaurant's star rating and another about its menu. The claim regarding the 3.0-star rating is unsupported because there's insufficient data to confirm the average star rating based on customer reviews.  The claim about the restaurant offering a variety of traditional Chinese dishes, including Mongolian cuisine, is partially supported. While the evidence confirms the presence of Mongolian dishes on the menu, the extent of the variety of traditional Chinese dishes offered remains unclear due to a lack of detailed menu information.  Since one claim is unsupported and the other is only partially supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIt has a rating of 3.0 stars based on customer reviews and offers a variety of traditional Chinese dishes, including Mongolian cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement makes two claims: one about the restaurant's star rating and another about its menu. The claim regarding the 3.0-star rating is unsupported because there's insufficient data to confirm the average star rating based on customer reviews.  The claim about the restaurant offering a variety of traditional Chinese dishes, including Mongolian cuisine, is partially supported. While the evidence confirms the presence of Mongolian dishes on the menu, the extent of the variety of traditional Chinese dishes offered remains unclear due to a lack of detailed menu information.  Since one claim is unsupported and the other is only partially supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 216 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a00aa854f4c5d822d0dce86ec89ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., water-based, oil-based)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['effectiveness of paper towels absorbing paint', 'absorbency of cloth vs paper towel for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., water-based, oil-based)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['effectiveness of paper towels absorbing paint', 'absorbency of cloth vs paper towel for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33meffectiveness of paper towels absorbing paint\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mabsorbency of cloth vs paper towel for paint cleanup\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., water-based, oil-based)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['effectiveness of paper towels absorbing paint', 'absorbency of cloth vs paper towel for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided text does not offer information on the effectiveness of paper towels and cloths in absorbing different types of paint (water-based vs. oil-based). \n",
      "\n",
      "Reasoning: Document [2] and [4] mention using a paper towel or cloth to dab excess paint.  However, these documents don't provide information on the effectiveness of paper towels and cloths in absorbing different types of paint (water-based vs. oil-based).  The provided text focuses on techniques for creating antique effects on wood, not on the comparative absorbency of cleaning materials.  Therefore, a definitive answer cannot be derived from the given source documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim suggests using a paper towel or cloth to remove excess paint.  The provided QA pair highlights that the source material doesn't offer information on the effectiveness of this method for different paint types (water-based vs. oil-based).  Therefore, the claim's effectiveness cannot be verified or refuted based on the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How effective are paper towels and cloths at absorbing excess paint of various types (e.g., water-based, oil-based)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided text does not offer information on the effectiveness of paper towels and cloths in absorbing different types of paint (water-based vs. oil-based).\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer indicates a lack of evidence to support the claim's effectiveness.  The absence of information prevents a conclusive assessment of the claim's validity.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim suggests using a paper towel or cloth to remove excess paint.  The provided QA pair highlights that the source material doesn't offer information on the effectiveness of this method for different paint types (water-based vs. oil-based).  Therefore, the claim's effectiveness cannot be verified or refuted based on the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., water-based, oil-based)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided text does not offer information on the effectiveness of paper towels and cloths in absorbing different types of paint (water-based vs. oil-based). \n",
      "\n",
      "Reasoning: Document [2] and [4] mention using a paper towel or cloth to dab excess paint.  However, these documents don't provide information on the effectiveness of paper towels and cloths in absorbing different types of paint (water-based vs. oil-based).  The provided text focuses on techniques for creating antique effects on wood, not on the comparative absorbency of cleaning materials.  Therefore, a definitive answer cannot be derived from the given source documents.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests using a paper towel or cloth to remove excess paint.  The provided QA pair highlights that the source material doesn't offer information on the effectiveness of this method for different paint types (water-based vs. oil-based).  Therefore, the claim's effectiveness cannot be verified or refuted based on the provided evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests using a paper towel or cloth to remove excess paint.  The provided QA pair highlights that the source material doesn't offer information on the effectiveness of this method for different paint types (water-based vs. oil-based).  Therefore, the claim's effectiveness cannot be verified or refuted based on the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mabsorbency of paper towels vs paint\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33meffectiveness of cloth for paint cleanup\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided text mentions using paper towels or cloths to remove excess paint [1, 5], but it doesn't offer information on their effectiveness for different paint types (acrylic, oil, latex) or compare their absorbency. \n",
      "\n",
      "Reasoning: The provided text mentions using paper towels or cloths to remove excess paint in a painting project [1, 5].  However, there is no information about the effectiveness of this method for different paint types (acrylic, oil, latex) or a comparison between paper towels and cloths.  The documents primarily focus on painting techniques and achieving specific effects, not on the absorbency of cleaning materials.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim suggests using paper towels or cloths to remove excess paint.  The provided QA pair indicates that while the source mentions this technique, it offers no details on its effectiveness for different paint types (acrylic, oil, latex) or a comparison of paper towels versus cloths.  Therefore, the claim lacks supporting evidence regarding the efficacy and suitability of this method for various paint types.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided text mentions using paper towels or cloths to remove excess paint [1, 5], but it doesn't offer information on their effectiveness for different paint types (acrylic, oil, latex) or compare their absorbency.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer shows the source mentions the technique but provides no evidence of its effectiveness or suitability for different paint types, thus weakening support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim suggests using paper towels or cloths to remove excess paint.  The provided QA pair indicates that while the source mentions this technique, it offers no details on its effectiveness for different paint types (acrylic, oil, latex) or a comparison of paper towels versus cloths.  Therefore, the claim lacks supporting evidence regarding the efficacy and suitability of this method for various paint types.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided text mentions using paper towels or cloths to remove excess paint [1, 5], but it doesn't offer information on their effectiveness for different paint types (acrylic, oil, latex) or compare their absorbency. \n",
      "\n",
      "Reasoning: The provided text mentions using paper towels or cloths to remove excess paint in a painting project [1, 5].  However, there is no information about the effectiveness of this method for different paint types (acrylic, oil, latex) or a comparison between paper towels and cloths.  The documents primarily focus on painting techniques and achieving specific effects, not on the absorbency of cleaning materials.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests using paper towels or cloths to remove excess paint.  The provided QA pair indicates that while the source mentions this technique, it offers no details on its effectiveness for different paint types (acrylic, oil, latex) or a comparison of paper towels versus cloths.  Therefore, the claim lacks supporting evidence regarding the efficacy and suitability of this method for various paint types.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim suggests using paper towels or cloths to remove excess paint.  The provided QA pair indicates that while the source mentions this technique, it offers no details on its effectiveness for different paint types (acrylic, oil, latex) or a comparison of paper towels versus cloths.  Therefore, the claim lacks supporting evidence regarding the efficacy and suitability of this method for various paint types.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mabsorbency of paper towels vs paint\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33meffectiveness of cloth for paint cleanup\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['absorbency of paper towels vs paint', 'effectiveness of cloth for paint cleanup']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mPaper towels or cloths can be used to remove excess paint, as demonstrated in a painting project [1, 5]. However, the provided text does not offer information on the comparative effectiveness of these materials for different paint types (acrylic, oil, latex). \n",
      "\n",
      "Reasoning: The provided text mentions using paper towels or cloths to remove excess paint in a painting project [1, 5].  However, there is no information about the effectiveness of this method for different paint types (acrylic, oil, latex) or a comparison between paper towels and cloths.  The documents primarily focus on painting techniques and achieving specific effects, not on the absorbency of cleaning materials.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided evidence shows that paper towels or cloths can be used to remove excess paint, but it doesn't specify the effectiveness for different paint types or compare the effectiveness of paper towels versus cloths.  The claim is supported by the evidence insofar as it mentions the use of these materials for paint removal, but lacks detail on their efficacy.  Therefore, the claim is only weakly supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Paper towels or cloths can be used to remove excess paint, as demonstrated in a painting project [1, 5]. However, the provided text does not offer information on the comparative effectiveness of these materials for different paint types (acrylic, oil, latex).\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer supports the claim that paper towels and cloths can be used to remove excess paint, but lacks information on effectiveness for different paint types, thus limiting the confidence level.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mTo remove excess paint, dab the painted surface with a paper towel or cloth.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided evidence shows that paper towels or cloths can be used to remove excess paint, but it doesn't specify the effectiveness for different paint types or compare the effectiveness of paper towels versus cloths.  The claim is supported by the evidence insofar as it mentions the use of these materials for paint removal, but lacks detail on their efficacy.  Therefore, the claim is only weakly supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow effective are paper towels and cloths at absorbing excess paint of various types (e.g., acrylic, oil, latex)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mPaper towels or cloths can be used to remove excess paint, as demonstrated in a painting project [1, 5]. However, the provided text does not offer information on the comparative effectiveness of these materials for different paint types (acrylic, oil, latex). \n",
      "\n",
      "Reasoning: The provided text mentions using paper towels or cloths to remove excess paint in a painting project [1, 5].  However, there is no information about the effectiveness of this method for different paint types (acrylic, oil, latex) or a comparison between paper towels and cloths.  The documents primarily focus on painting techniques and achieving specific effects, not on the absorbency of cleaning materials.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mDab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided evidence shows that paper towels or cloths can be used to remove excess paint, but it doesn't specify the effectiveness for different paint types or compare the effectiveness of paper towels versus cloths.  The claim is supported by the evidence insofar as it mentions the use of these materials for paint removal, but lacks detail on their efficacy.  Therefore, the claim is only weakly supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mFinally, dab this with a paper towel or cloth to remove excess paint.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided evidence shows that paper towels or cloths can be used to remove excess paint, but it doesn't specify the effectiveness for different paint types or compare the effectiveness of paper towels versus cloths.  The claim is supported by the evidence insofar as it mentions the use of these materials for paint removal, but lacks detail on their efficacy.  Therefore, the claim is only weakly supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 217 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445854ea2b6c444fbde426d7aee1f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided documents, American Ale is located in Santa Barbara, CA, at 14 E Cota St [2, 4].  The documents categorize it as a pub, bar, and restaurant serving American (Traditional) cuisine, including burgers [2, 4].  Reviews mention the food quality [1, 3], but also note some negative experiences related to the business's management and cleanliness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara Yelp rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara Google reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].  Individual reviews show a range of ratings, including 4.0, 3.0, and 1.0 stars [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale in Santa Barbara and includes several reviews with star ratings.  Document [2] and [4] show a business star rating of 4.0.  Documents [1] and [3] contain individual reviews with ratings of 4.0, 3.0, and 1.0.  To determine an average, I will use the business star rating from Yelp/Google as the primary indicator, supplemented by the individual review ratings.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\', \\'hours\\': None, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': True, \\'street\\': True, \\'validated\\': True, \\'lot\\': False, \\'valet\\': False}, \\'RestaurantsReservations\\': False, \\'OutdoorSeating\\': True, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'touristy\\': False, \\'hipster\\': True, \\'romantic\\': False, \\'divey\\': False, \\'intimate\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'classy\\': False, \\'casual\\': True}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 1.0, \\'review_date\\': \\'2017-04-19 21:30:45\\', \\'review_text\\': \\'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}, {'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative individual reviews.  The reasoning in the QA pairs clearly links the information to the sources, providing a strong basis for supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This question and answer directly support the claim by confirming American Ale's location, type of establishment, and cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This question and answer directly supports the claim by confirming the 4.0-star rating.  While individual reviews varied, the overall rating supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative individual reviews.  The reasoning in the QA pairs clearly links the information to the sources, providing a strong basis for supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided documents, American Ale is located in Santa Barbara, CA, at 14 E Cota St [2, 4].  The documents categorize it as a pub, bar, and restaurant serving American (Traditional) cuisine, including burgers [2, 4].  Reviews mention the food quality [1, 3], but also note some negative experiences related to the business's management and cleanliness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].  Individual reviews show a range of ratings, including 4.0, 3.0, and 1.0 stars [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale in Santa Barbara and includes several reviews with star ratings.  Document [2] and [4] show a business star rating of 4.0.  Documents [1] and [3] contain individual reviews with ratings of 4.0, 3.0, and 1.0.  To determine an average, I will use the business star rating from Yelp/Google as the primary indicator, supplemented by the individual review ratings.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\', \\'hours\\': None, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': True, \\'street\\': True, \\'validated\\': True, \\'lot\\': False, \\'valet\\': False}, \\'RestaurantsReservations\\': False, \\'OutdoorSeating\\': True, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'touristy\\': False, \\'hipster\\': True, \\'romantic\\': False, \\'divey\\': False, \\'intimate\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'classy\\': False, \\'casual\\': True}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 1.0, \\'review_date\\': \\'2017-04-19 21:30:45\\', \\'review_text\\': \\'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}, {'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative individual reviews.  The reasoning in the QA pairs clearly links the information to the sources, providing a strong basis for supporting the claim.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative individual reviews.  The reasoning in the QA pairs clearly links the information to the sources, providing a strong basis for supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara menu', 'American Ale Santa Barbara reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided documents, American Ale is located in Santa Barbara, CA, at 14 E Cota St [2, 4].  The documents categorize it as a pub, bar, and restaurant serving American (Traditional) cuisine, including burgers [2, 4].  Reviews mention the food quality [1, 3], but also note some negative experiences related to the business's management and cleanliness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara Yelp rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara Google reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara Yelp rating', 'American Ale Santa Barbara Google reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].  Individual reviews show a range of ratings, including 4.0, 3.0, and 1.0 stars [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale in Santa Barbara and includes several reviews with star ratings.  Document [2] and [4] show a business star rating of 4.0.  Documents [1] and [3] contain individual reviews with ratings of 4.0, 3.0, and 1.0.  To determine an average, I will use the business star rating from Yelp/Google as the primary indicator, supplemented by the individual review ratings.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\', \\'hours\\': None, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': True, \\'street\\': True, \\'validated\\': True, \\'lot\\': False, \\'valet\\': False}, \\'RestaurantsReservations\\': False, \\'OutdoorSeating\\': True, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'touristy\\': False, \\'hipster\\': True, \\'romantic\\': False, \\'divey\\': False, \\'intimate\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'classy\\': False, \\'casual\\': True}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 1.0, \\'review_date\\': \\'2017-04-19 21:30:45\\', \\'review_text\\': \\'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}, {'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative reviews resulting in a range of individual star ratings.  The overall rating of 4.0 is used as the primary indicator of customer satisfaction.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer directly supports the claim by confirming the location, type of establishment, and cuisine offered by American Ale.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer directly supports the claim's assertion of a 4.0-star rating. While acknowledging a range of individual ratings, the overall average rating supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative reviews resulting in a range of individual star ratings.  The overall rating of 4.0 is used as the primary indicator of customer satisfaction.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs American Ale a pub and restaurant located in Santa Barbara, California, that serves American cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, American Ale is a pub and restaurant located in Santa Barbara, California, at 14 E Cota St [2, 4]. It serves American cuisine, including burgers [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided documents, American Ale is located in Santa Barbara, CA, at 14 E Cota St [2, 4].  The documents categorize it as a pub, bar, and restaurant serving American (Traditional) cuisine, including burgers [2, 4].  Reviews mention the food quality [1, 3], but also note some negative experiences related to the business's management and cleanliness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average customer rating for American Ale on review platforms like Yelp, Google Reviews, or TripAdvisor?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available data, American Ale in Santa Barbara has a 4.0-star rating on Yelp or Google Reviews [2, 4].  Individual reviews show a range of ratings, including 4.0, 3.0, and 1.0 stars [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale in Santa Barbara and includes several reviews with star ratings.  Document [2] and [4] show a business star rating of 4.0.  Documents [1] and [3] contain individual reviews with ratings of 4.0, 3.0, and 1.0.  To determine an average, I will use the business star rating from Yelp/Google as the primary indicator, supplemented by the individual review ratings.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\', \\'hours\\': None, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': True, \\'street\\': True, \\'validated\\': True, \\'lot\\': False, \\'valet\\': False}, \\'RestaurantsReservations\\': False, \\'OutdoorSeating\\': True, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'touristy\\': False, \\'hipster\\': True, \\'romantic\\': False, \\'divey\\': False, \\'intimate\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'classy\\': False, \\'casual\\': True}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 1.0, \\'review_date\\': \\'2017-04-19 21:30:45\\', \\'review_text\\': \\'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}, {'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative reviews resulting in a range of individual star ratings.  The overall rating of 4.0 is used as the primary indicator of customer satisfaction.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara offering American cuisine with a 4.0-star customer rating.  The provided QA pairs directly support this claim. The first QA pair confirms American Ale's location, type of establishment (pub and restaurant), and cuisine (American). The second QA pair confirms the 4.0-star rating, although it acknowledges the presence of both positive and negative reviews resulting in a range of individual star ratings.  The overall rating of 4.0 is used as the primary indicator of customer satisfaction.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a 4.0-star rating from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat type of establishment is American Ale in Santa Barbara?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara type of business', 'American Ale Santa Barbara restaurant review']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhere is American Ale located?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale location', 'American Ale Santa Barbara address']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mWhat is the average customer rating for American Ale?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['American Ale Santa Barbara rating', 'American Ale customer reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of establishment is American Ale in Santa Barbara?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara type of business', 'American Ale Santa Barbara restaurant review']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara type of business\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara restaurant review\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of establishment is American Ale in Santa Barbara?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara type of business', 'American Ale Santa Barbara restaurant review']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAmerican Ale in Santa Barbara is a multifaceted establishment categorized as a restaurant, bar, and pub, serving American (Traditional) food, beer, wine, and spirits [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided data, American Ale in Santa Barbara is categorized under several types of establishments.  Document [2] and [4] list its categories as Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, and Restaurants.  This indicates it functions as a restaurant, bar, and pub, serving American traditional food and alcoholic beverages.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhere is American Ale located?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale location', 'American Ale Santa Barbara address']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale location\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara address\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhere is American Ale located?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale location', 'American Ale Santa Barbara address']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mAmerican Ale is located at 14 E Cota St, Santa Barbara, CA [1, 2]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale multiple times, with document [2] and [4] providing the most relevant information.  These documents contain structured data indicating the location of American Ale.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara rating', 'American Ale customer reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale Santa Barbara rating\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAmerican Ale customer reviews rating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the average customer rating for American Ale?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['American Ale Santa Barbara rating', 'American Ale customer reviews rating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available customer reviews, the average rating for American Ale is 2.67.  Individual reviews show ratings of 4.0 [1, 3], 3.0 [1, 3], and 1.0 [2, 4].  Note that the business has an overall rating of 4.0 [2, 4], but this is not the average customer rating. \n",
      "\n",
      "Reasoning: The provided text contains multiple reviews and ratings for American Ale.  Document [2] and [4] show an overall business star rating of 4.0.  Documents [1] and [3] contain individual customer reviews with ratings of 4.0, 3.0, and 1.0. To determine the average customer rating, I will average the individual customer ratings from documents [1] and [3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33m{'review_stars': 1.0, 'review_date': '2017-04-19 21:30:45', 'review_text': 'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33m{'business_stars': 4.0\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a 4.0-star rating from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara with a 4.0-star customer rating.  The provided QA pairs confirm that American Ale is indeed a pub and restaurant in Santa Barbara. However, the average customer rating is calculated to be 2.67 based on the individual ratings provided, not 4.0.  The 4.0 rating appears to be an overall business rating, not the average customer rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What type of establishment is American Ale in Santa Barbara?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: American Ale in Santa Barbara is a multifaceted establishment categorized as a restaurant, bar, and pub, serving American (Traditional) food, beer, wine, and spirits [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the claim that American Ale is a pub and restaurant.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Where is American Ale located?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: American Ale is located at 14 E Cota St, Santa Barbara, CA [1, 2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the claim that American Ale is in Santa Barbara.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the average customer rating for American Ale?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available customer reviews, the average rating for American Ale is 2.67.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Contradicts the claim's assertion of a 4.0-star rating.  The 4.0 rating seems to be an overall business rating, not the average customer rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAmerican Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a 4.0-star rating from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara with a 4.0-star customer rating.  The provided QA pairs confirm that American Ale is indeed a pub and restaurant in Santa Barbara. However, the average customer rating is calculated to be 2.67 based on the individual ratings provided, not 4.0.  The 4.0 rating appears to be an overall business rating, not the average customer rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat type of establishment is American Ale in Santa Barbara?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAmerican Ale in Santa Barbara is a multifaceted establishment categorized as a restaurant, bar, and pub, serving American (Traditional) food, beer, wine, and spirits [2, 4]. \n",
      "\n",
      "Reasoning: Based on the provided data, American Ale in Santa Barbara is categorized under several types of establishments.  Document [2] and [4] list its categories as Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, and Restaurants.  This indicates it functions as a restaurant, bar, and pub, serving American traditional food and alcoholic beverages.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Nightlife, Pubs, Beer, Wine & Spirits, American (Traditional), Burgers, Bars, Food, Restaurants\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhere is American Ale located?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mAmerican Ale is located at 14 E Cota St, Santa Barbara, CA [1, 2]. \n",
      "\n",
      "Reasoning: The provided text mentions American Ale multiple times, with document [2] and [4] providing the most relevant information.  These documents contain structured data indicating the location of American Ale.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \\'American Ale\\', \\'address\\': \\'14 E Cota St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the average customer rating for American Ale?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available customer reviews, the average rating for American Ale is 2.67.  Individual reviews show ratings of 4.0 [1, 3], 3.0 [1, 3], and 1.0 [2, 4].  Note that the business has an overall rating of 4.0 [2, 4], but this is not the average customer rating. \n",
      "\n",
      "Reasoning: The provided text contains multiple reviews and ratings for American Ale.  Document [2] and [4] show an overall business star rating of 4.0.  Documents [1] and [3] contain individual customer reviews with ratings of 4.0, 3.0, and 1.0. To determine the average customer rating, I will average the individual customer ratings from documents [1] and [3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'review_stars': 4.0, 'review_date': '2017-01-29 15:34:16', 'review_text': 'I write these as I craving for one more   piece of a delicious  burger. Tears comes out of my eyes and joy comes out of every corner in the world.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'review_stars': 3.0, 'review_date': '2017-01-27 13:13:04', 'review_text': 'The food was wonderful and the service was good. But all the impression is spoiled a dirty floor in the bathroom.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33m{'review_stars': 1.0, 'review_date': '2017-04-19 21:30:45', 'review_text': 'WTF.. first the Cota street one closes or \"relocates\" to Union Ale.. opps, Yankee noodle.. almost forgot about the 3 month debacle. Then all I want is a PBnJ burger and American Ale 2.0 is done.. ugh. Disappointing.'}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33m{'business_stars': 4.0\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara with a 4.0-star customer rating.  The provided QA pairs confirm that American Ale is indeed a pub and restaurant in Santa Barbara. However, the average customer rating is calculated to be 2.67 based on the individual ratings provided, not 4.0.  The 4.0 rating appears to be an overall business rating, not the average customer rating.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mOverall, American Ale is a pub and restaurant in Santa Barbara that offers American cuisine and has received a rating of 4.0 stars from customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32mpartially supported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that American Ale is a pub and restaurant in Santa Barbara with a 4.0-star customer rating.  The provided QA pairs confirm that American Ale is indeed a pub and restaurant in Santa Barbara. However, the average customer rating is calculated to be 2.67 based on the individual ratings provided, not 4.0.  The 4.0 rating appears to be an overall business rating, not the average customer rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 218 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944dad03509047148242855137f7393d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mbusiness name parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mbusiness address parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided Q&A indicates that the business (Hollister Brewing Company) does have a parking lot, but it does *not* offer validated parking.  Therefore, the claim is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly states that while the business has a parking lot, it does not offer validated parking. This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] outdoor seating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its attributes.  The attribute \"OutdoorSeating\" is listed as \"True\".  Therefore, the business has outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing a document that supports this information.  The reasoning in the answer explains how the conclusion was reached based on the cited document.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its attributes.  The attribute \"OutdoorSeating\" is listed as \"True\".  Therefore, the business has outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim and provides a clear explanation and citation supporting the presence of outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] Wi-Fi', '[Business Address] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi', '[Business Address] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] Wi-Fi\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Address] Wi-Fi availability\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi', '[Business Address] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi to its customers [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including Wi-Fi availability.  Document [1] and [4] clearly state that the establishment offers free Wi-Fi.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] takeout menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] to-go options\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the business offers Wi-Fi and takeout options.  The provided QA pairs confirm both aspects. The first QA pair confirms the availability of Wi-Fi, citing sources [1] and [4]. The second QA pair confirms the availability of takeout options, citing sources [1] and [2].  Both aspects of the claim are supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers free Wi-Fi to its customers [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the claim by confirming the availability of Wi-Fi.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers takeout options. [1, 2]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the claim by confirming the availability of takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided Q&A indicates that the business (Hollister Brewing Company) does have a parking lot, but it does *not* offer validated parking.  Therefore, the claim is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing a document that supports this information.  The reasoning in the answer explains how the conclusion was reached based on the cited document.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its attributes.  The attribute \"OutdoorSeating\" is listed as \"True\".  Therefore, the business has outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the business offers Wi-Fi and takeout options.  The provided QA pairs confirm both aspects. The first QA pair confirms the availability of Wi-Fi, citing sources [1] and [4]. The second QA pair confirms the availability of takeout options, citing sources [1] and [2].  Both aspects of the claim are supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi to its customers [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including Wi-Fi availability.  Document [1] and [4] clearly state that the establishment offers free Wi-Fi.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mbusiness name parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mbusiness address parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['business name parking validation', 'business address parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided Q&A indicates that the business (Hollister Brewing Company) has a parking lot, but this parking is not validated.  The answer explicitly states that validated parking is not offered.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly refutes the claim by stating that the business does not offer validated parking, despite having a parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] outdoor seating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its attributes.  The attribute \"OutdoorSeating\" is listed as \"True\".  Therefore, the business has outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing a document that supports this information.  The reasoning in the answer explains how the conclusion was reached based on the cited document.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company has outdoor seating [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim, providing direct evidence from a cited source.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] Wi-Fi availability\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi to its customers [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its amenities.  The relevant field is 'WiFi', which is listed as 'free'. This indicates that the business offers free Wi-Fi access to its customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] takeout menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] to-go options\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs confirm that Hollister Brewing Company offers both Wi-Fi and takeout options.  The first QA pair directly states that free Wi-Fi is available, citing supporting documentation. The second QA pair confirms the availability of takeout options, also referencing supporting documentation.  Therefore, the claim is fully supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers free Wi-Fi to its customers [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming the availability of Wi-Fi.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers takeout options. [1, 2]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming the availability of takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided Q&A indicates that the business (Hollister Brewing Company) has a parking lot, but this parking is not validated.  The answer explicitly states that validated parking is not offered.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1, 4].  They do, however, have a parking lot [1, 4]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing a document that supports this information.  The reasoning in the answer explains how the conclusion was reached based on the cited document.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its attributes.  The attribute \"OutdoorSeating\" is listed as \"True\".  Therefore, the business has outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs confirm that Hollister Brewing Company offers both Wi-Fi and takeout options.  The first QA pair directly states that free Wi-Fi is available, citing supporting documentation. The second QA pair confirms the availability of takeout options, also referencing supporting documentation.  Therefore, the claim is fully supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi to its customers [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its amenities.  The relevant field is 'WiFi', which is listed as 'free'. This indicates that the business offers free Wi-Fi access to its customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  The claim about validated parking is unsupported by the evidence. The claims about outdoor seating, Wi-Fi, and takeout options are all supported. Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] parking validation', '[Business Address] parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] parking validation', '[Business Address] parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Address] parking validation\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] parking validation', '[Business Address] parking validation']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided question-answer pair directly addresses this. The answer explicitly states that Hollister Brewing Company does *not* offer validated parking, despite having a parking lot.  Therefore, the claim is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, Hollister Brewing Company does not offer validated parking [1]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly refutes the claim by stating that the business does not offer validated parking.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] outdoor seating\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] outdoor seating']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains the business's attributes, including whether or not it has outdoor seating.  The `OutdoorSeating` attribute is listed as `True`.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing evidence from document [1] which indicates the `OutdoorSeating` attribute is `True`.  This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains the business's attributes, including whether or not it has outdoor seating.  The `OutdoorSeating` attribute is listed as `True`.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the presence of outdoor seating, citing direct evidence from a source indicating a 'True' value for the relevant attribute. This provides strong support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] Wi-Fi availability\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] Wi-Fi availability']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its amenities.  The relevant attribute is \"WiFi: 'free'\". This indicates that the business offers free Wi-Fi access to its customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] takeout menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Business Name] to-go options\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Business Name] takeout menu', '[Business Name] to-go options']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs confirm that Hollister Brewing Company offers both Wi-Fi and takeout options.  The first QA pair directly states that free Wi-Fi is available, citing supporting documentation. The second QA pair confirms the availability of takeout options, also referencing supporting documentation.  Therefore, the claim is fully supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers free Wi-Fi [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming the availability of Wi-Fi.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Hollister Brewing Company offers takeout options. [1, 2]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming the availability of takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  One claim (validated parking) is unsupported, while the other two claims (outdoor seating and Wi-Fi/takeout options) are supported.  Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business has a validated parking lot.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the business has a validated parking lot.  The provided question-answer pair directly addresses this. The answer explicitly states that Hollister Brewing Company does *not* offer validated parking, despite having a parking lot.  Therefore, the claim is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer validated parking?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, Hollister Brewing Company does not offer validated parking [1]. \n",
      "\n",
      "Reasoning: The provided text mentions that Hollister Brewing Company has a parking lot, but it does not offer validated parking.  This information is found within the `BusinessParking` attribute of the business data.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers outdoor seating.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that Hollister Brewing Company has outdoor seating, citing evidence from document [1] which indicates the `OutdoorSeating` attribute is `True`.  This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business have outdoor seating?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company has outdoor seating [1]. \n",
      "\n",
      "Reasoning: Document [1] contains the business's attributes, including whether or not it has outdoor seating.  The `OutdoorSeating` attribute is listed as `True`.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Hollister Brewing Company', 'address': '6980 Marketplace Dr', 'city': 'Goleta', 'state': 'CA', 'categories': 'Restaurants, Food, Breweries, American (Traditional), Brewpubs', 'hours': {'Monday': '11:30-19:30', 'Tuesday': '11:30-19:30', 'Wednesday': '11:30-19:30', 'Thursday': '11:30-19:30', 'Friday': '11:30-20:0', 'Saturday': '11:30-20:0', 'Sunday': '11:30-19:30'}, 'attributes': {'BusinessParking': {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': True, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'touristy': False, 'hipster': False, 'romantic': False, 'divey': False, 'intimate': False, 'trendy': False, 'upscale': False, 'classy': True, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 1.0, 'review_date': '2022-01-06 18:56:07', 'review_text': \"I miss the old menu a lot!'\"}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe business offers Wi-Fi and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs confirm that Hollister Brewing Company offers both Wi-Fi and takeout options.  The first QA pair directly states that free Wi-Fi is available, citing supporting documentation. The second QA pair confirms the availability of takeout options, also referencing supporting documentation.  Therefore, the claim is fully supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer Wi-Fi access to its customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers free Wi-Fi [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about Hollister Brewing Company, including its amenities.  The relevant attribute is \"WiFi: 'free'\". This indicates that the business offers free Wi-Fi access to its customers.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'WiFi': 'free'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes the business offer takeout or to-go options for its food or services?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Hollister Brewing Company offers takeout options. [1, 2] \n",
      "\n",
      "Reasoning: The provided text mentions Hollister Brewing Company's attributes, including whether it offers takeout.  Document [2] and [4] explicitly state that `RestaurantsTakeOut` is True.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  One claim (validated parking) is unsupported, while the other two claims (outdoor seating and Wi-Fi/takeout options) are supported.  Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe business has a validated parking lot and offers outdoor seating, Wi-Fi, and takeout options.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims the business has a validated parking lot, outdoor seating, Wi-Fi, and takeout options.  One claim (validated parking) is unsupported, while the other two claims (outdoor seating and Wi-Fi/takeout options) are supported.  Because one of the conjunctive claims is unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "Running row 219 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2c2705a9c34272af3b734e089f1450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co Montecito CA\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co bakery Montecito\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co desserts\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this. The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly confirms the existence of the bakery and cafe in Montecito.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Crushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Provides evidence of the types of desserts and drinks offered, supporting the claim's description of the establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this. The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this. The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this. The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 346, in forward\n",
      "    result = self.extract(statement=statement)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co Montecito CA\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co bakery Montecito\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co desserts\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly confirms the existence of Crushcakes & Co as a bakery and cafe in Montecito.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Crushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Provides evidence of the types of desserts and drinks offered, supporting the claim's description of the establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest a level of success.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co Montecito CA\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co bakery Montecito\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co Montecito CA', 'Crushcakes & Co bakery Montecito']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mCrushcakes & Co desserts\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Crushcakes & Co menu', 'Crushcakes & Co desserts']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest it's at least a functioning establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the existence of Crushcakes & Co as a bakery and cafe in Montecito, providing location details.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Crushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Provides specific examples of desserts and drinks offered, supporting the claim's description of the establishment's offerings.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest it's at least a functioning establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes a bakery and cafe called Crushcakes & Co exist in Montecito, California?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, a bakery and cafe called Crushcakes & Co exists in Montecito, California.  It's located at 1150 Coast Village Rd [2, 4]. \n",
      "\n",
      "Reasoning: Document [2] and [4] contain information about a business named \"Crushcakes & Co\" located at 1150 Coast Village Rd, Montecito, CA.  The documents list its categories as Restaurants, Cafes, Bakeries, and more, confirming it's a bakery and cafe.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'name': 'Crushcakes & Co', 'address': '1150 Coast Village Rd', 'city': 'Montecito', 'state': 'CA', 'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat type of desserts and drinks does Crushcakes & Co offer?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCrushcakes & Co. offers a variety of desserts, including cupcakes and chocolate cake [1, 3], and drinks such as lattes [3], and coffee and tea [2]. \n",
      "\n",
      "Reasoning: Based on the provided text, Crushcakes & Co. is categorized as a bakery, cafe, and dessert shop.  Customer reviews mention cupcakes, chocolate cake, and lattes.  The business information confirms that they offer coffee and tea.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mdarling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m'categories': 'Restaurants, Cafes, Food, Bakeries, Cupcakes, Coffee & Tea, Desserts'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe lattes were delicious with healthy milk alternatives. quaint but small eating area and darling cupcakes too\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest it's at least a functioning establishment.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCrushcakes & Co is a popular bakery and cafe located in Montecito, California, serving up delicious desserts and drinks.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Crushcakes & Co is a popular bakery and cafe in Montecito, California, serving delicious desserts and drinks.  The provided QA pairs directly support this.  The first QA pair confirms the existence of Crushcakes & Co at a specific address in Montecito and identifies it as a bakery and cafe. The second QA pair details the types of desserts (cupcakes, chocolate cake) and drinks (lattes, coffee, tea) offered, further substantiating the claim.  While the claim uses the word \"popular,\" this is not directly addressed in the evidence, but the existence and offerings strongly suggest it's at least a functioning establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 220 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f3cd4f3fd94dada51abfdeb433259f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] TV channel\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] air times CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] schedule CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: On which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS [1, 2, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on CBS, a key part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS weekdays [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on weekdays, completing the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] Paramount Plus streaming\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim, providing direct evidence from a source.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] TV channel\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] air times CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] schedule CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: On which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS [1, 2, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on CBS, a key part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS weekdays [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on weekdays, completing the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] Paramount Plus streaming\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim, citing a source.  The reasoning further strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] TV channel\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] TV channel']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] air times CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] schedule CBS\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] air times CBS', '[Show Name] schedule CBS']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: On which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS [1, 2, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on CBS, a key part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Young and the Restless airs on CBS weekdays [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the show airs on weekdays, completing the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Show Name] Paramount Plus streaming\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Show Name] Paramount Plus streaming']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim, providing direct evidence from a source.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show airs weekdays on CBS.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly address the claim.  The first QA pair confirms the show airs on CBS. The second QA pair confirms that it airs on weekdays.  Both answers are supported by multiple cited documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mOn which television channel does the show air?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS [1, 2, 3]. \n",
      "\n",
      "Reasoning: Documents [1], [2], and [3] all mention that the show \"The Young and the Restless\" airs on CBS.  The information is consistent across all three documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe CBS weekly promo shows Nick in a fight with Adam, demanding Sally to choose between them.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe Young and the Restless spoilers say that Sally's decision will shock the viewers. She must make the decision that is best for her future. Y&R fans, which Newman brothers do you think Sally will pick?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the days of the week the show airs on CBS?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Young and the Restless airs on CBS weekdays [1, 4]. \n",
      "\n",
      "Reasoning: Documents 1 and 4 state that the show \"The Young and the Restless\" airs on CBS weekdays.  The other documents provide plot details but don't specify the days of the week the show airs.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mY&R fans, which Newman brothers do you think Sally will pick?  Keep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe show streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that \"The Young and the Restless\" (assuming this is the show referenced in the claim) is available to stream on Paramount Plus, citing a source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs the show available to stream on Paramount Plus?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Young and the Restless is available to stream on Paramount Plus. [1] \n",
      "\n",
      "Reasoning: The provided document explicitly states that \"The Young and the Restless\" is streaming on Paramount Plus.  Therefore, the answer will confirm its availability.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mKeep watching The Young and the Restless, airing weekdays on CBS and streaming on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe show airs weekdays on CBS and streams on Paramount Plus.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"The show airs weekdays on CBS\" and \"The show streams on Paramount Plus,\" are supported by evidence.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 221 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b17eda5ab484e9e29ce4c3da6f1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Establishment Name] reservation policy\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly refutes the claim by stating that The Daily Grind, a casual establishment, does not accept reservations. The reasoning provides a clear citation and explanation supporting the answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant takeout service [restaurant name or location if known]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms that The Daily Grind, a casual establishment, offers a takeout service, directly supporting the claim. The reasoning provides a clear explanation based on a reliable source.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [establishment name] ambience\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[establishment name] interior photos\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the establishment offers a pleasant ambience. The provided evidence only indicates that the establishment's ambience is classified as \"casual\".  While a casual ambience can be pleasant for some, it doesn't inherently guarantee pleasantness.  The lack of information regarding specific aspects like decor, lighting, music, or seating prevents a definitive assessment of whether the ambience is pleasant.  The evidence only supports the casual nature of the ambience, not its pleasantness.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms a casual ambience but provides no details about elements that contribute to a pleasant experience.  The lack of information makes it impossible to determine if the ambience is pleasant.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the establishment offers a pleasant ambience. The provided evidence only indicates that the establishment's ambience is classified as \"casual\".  While a casual ambience can be pleasant for some, it doesn't inherently guarantee pleasantness.  The lack of information regarding specific aspects like decor, lighting, music, or seating prevents a definitive assessment of whether the ambience is pleasant.  The evidence only supports the casual nature of the ambience, not its pleasantness.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Establishment Name] reservation policy\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly refutes the claim by stating that The Daily Grind, a casual establishment, does not accept reservations. The reasoning provides a clear citation and explanation supporting the answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant takeout service [restaurant name or location if known]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms that The Daily Grind, a casual establishment, offers a takeout service, directly supporting the claim. The reasoning provides a clear explanation based on a reliable source.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [establishment name] ambience\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[establishment name] interior photos\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] interior photos']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the establishment offers a pleasant ambience. The provided evidence only indicates that the establishment's ambience is classified as \"casual\".  While a casual ambience can be pleasant for some, it doesn't inherently guarantee pleasantness.  The lack of information regarding specific aspects like decor, lighting, music, or seating prevents a definitive assessment of whether the ambience is pleasant.  The evidence only supports the casual nature of the ambience, not its pleasantness.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms a casual ambience but provides no details about elements that contribute to a pleasant experience.  The lack of information makes it impossible to determine if the ambience is pleasant.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the establishment offers a pleasant ambience. The provided evidence only indicates that the establishment's ambience is classified as \"casual\".  While a casual ambience can be pleasant for some, it doesn't inherently guarantee pleasantness.  The lack of information regarding specific aspects like decor, lighting, music, or seating prevents a definitive assessment of whether the ambience is pleasant.  The evidence only supports the casual nature of the ambience, not its pleasantness.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, lighting, music, seating)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available data, The Daily Grind's ambience is described as casual [2, 4].  No information is provided regarding specific aspects like decor, lighting, music, or seating. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  Document [2] and [4] contain structured data about the establishment, including its ambience.  This data indicates that the ambience is categorized as \"casual,\" and other ambience descriptors (romantic, intimate, classy, etc.) are marked as false.  No information is given about decor, lighting, music, or specific seating arrangements.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported because while the ambience is described as casual, no further details are provided to support a positive assessment of the ambience.  Since two out of three claims are unsupported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Establishment Name] reservation policy\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Establishment Name] reservation policy']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: No, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly refutes the claim by stating that The Daily Grind, a casual establishment, does not accept reservations. The reasoning provides a clear citation and explanation supporting the answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrestaurant takeout service [restaurant name or location if known]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['restaurant takeout service [restaurant name or location if known]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms that The Daily Grind, a casual establishment, offers a takeout service, directly supporting the claim. The reasoning provides a clear explanation based on a reliable source.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, music, lighting, cleanliness)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] photos interior']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the general customer feedback regarding the atmosphere of the establishment?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[establishment name] yelp reviews atmosphere', '[establishment name] google reviews ambiance']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, music, lighting, cleanliness)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] photos interior']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [establishment name] ambience\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[establishment name] photos interior\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, music, lighting, cleanliness)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [establishment name] ambience', '[establishment name] photos interior']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on available information, The Daily Grind has a casual ambience [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  One document lists its ambience as \"casual,\" indicating a relaxed atmosphere.  Other documents contain customer reviews, but these focus on service and food quality, not the establishment's ambience details like decor, music, or lighting.  Therefore, the answer will primarily rely on the \"casual\" ambience description.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the general customer feedback regarding the atmosphere of the establishment?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[establishment name] yelp reviews atmosphere', '[establishment name] google reviews ambiance']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[establishment name] yelp reviews atmosphere\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[establishment name] google reviews ambiance\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the general customer feedback regarding the atmosphere of the establishment?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[establishment name] yelp reviews atmosphere', '[establishment name] google reviews ambiance']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer feedback on the atmosphere is mixed. One review mentions finding a bug in a salad [1, 4], while another praises the friendly owner and good coffee [1, 4].  A separate establishment, \"The Daily Grind,\" is described as having a casual ambiance [2, 5]. \n",
      "\n",
      "Reasoning: The provided text contains reviews from different sources.  Document [1] and [4] contain mixed reviews, with one mentioning a bug in a salad and another praising the coffee and owner's friendliness. Document [2] and [5] refer to \"The Daily Grind,\" indicating a casual ambiance.  There is no other information about the atmosphere in the other documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe one time I got a salad though there was a bug in it. A green beetle looking bug with little legs curled up, it blended in and I may have even eaten a bug or two who knows.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe coffee is strong and tasteful. The owner is really friendly and funny and it is always a pleasant surprise when I see him working the cash register.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the casual establishment offers a pleasant ambience.  The provided QA pairs offer mixed evidence.  While one answer confirms a \"casual\" ambience for an establishment called \"The Daily Grind,\" this doesn't explicitly state whether this ambience is \"pleasant.\"  The second answer reveals mixed customer feedback, with some positive comments (friendly owner, good coffee) and some negative (bug in a salad).  The lack of direct evidence regarding the pleasantness of the ambience, coupled with mixed customer reviews, weakens the support for the claim.  The \"casual\" ambience is established, but \"pleasant\" is not directly supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What aspects of the establishment contribute to its ambience (e.g., decor, music, lighting, cleanliness)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on available information, The Daily Grind has a casual ambience [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  One document lists its ambience as \"casual,\" indicating a relaxed atmosphere.  Other documents contain customer reviews, but these focus on service and food quality, not the establishment's ambience details like decor, music, or lighting.  Therefore, the answer will primarily rely on the \"casual\" ambience description.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Establishes a 'casual' ambience, but doesn't address 'pleasantness'.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the general customer feedback regarding the atmosphere of the establishment?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer feedback on the atmosphere is mixed. One review mentions finding a bug in a salad [1, 4], while another praises the friendly owner and good coffee [1, 4].  A separate establishment, \"The Daily Grind,\" is described as having a casual ambiance [2, 5]. \n",
      "\n",
      "Reasoning: The provided text contains reviews from different sources.  Document [1] and [4] contain mixed reviews, with one mentioning a bug in a salad and another praising the coffee and owner's friendliness. Document [2] and [5] refer to \"The Daily Grind,\" indicating a casual ambiance.  There is no other information about the atmosphere in the other documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Mixed customer feedback; some positive, some negative, doesn't directly support 'pleasant' ambience.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported due to a lack of sufficient positive evidence describing the atmosphere.  Since two out of three claims are unsupported or only one is supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can place reservations at the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that The Daily Grind, a casual establishment, does not accept reservations.  This directly contradicts the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment accept reservations?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mNo, The Daily Grind does not accept reservations [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind, including its attributes.  Specifically, it states that the establishment's `RestaurantsReservations` attribute is `False`. This indicates that the restaurant does not accept reservations.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsReservations': False\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers can take out their orders from the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer confirms that The Daily Grind, a casual establishment, offers a takeout service.  The reasoning explains that this information is extracted from a source indicating the \"RestaurantsTakeOut\" attribute is True. This strongly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes this casual establishment offer a takeout or takeaway service for customers?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, The Daily Grind offers a takeout service [1]. \n",
      "\n",
      "Reasoning: Document [1] contains information about The Daily Grind restaurant.  The relevant attribute \"RestaurantsTakeOut\" is listed as True. This indicates that the establishment offers a takeout service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m'RestaurantsTakeOut': True\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe casual establishment offers a pleasant ambience for customers to enjoy.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the casual establishment offers a pleasant ambience.  The provided QA pairs offer mixed evidence.  While one answer confirms a \"casual\" ambience for an establishment called \"The Daily Grind,\" this doesn't explicitly state whether this ambience is \"pleasant.\"  The second answer reveals mixed customer feedback, with some positive comments (friendly owner, good coffee) and some negative (bug in a salad).  The lack of direct evidence regarding the pleasantness of the ambience, coupled with mixed customer reviews, weakens the support for the claim.  The \"casual\" ambience is established, but \"pleasant\" is not directly supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat aspects of the establishment contribute to its ambience (e.g., decor, music, lighting, cleanliness)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on available information, The Daily Grind has a casual ambience [1]. \n",
      "\n",
      "Reasoning: The provided text mentions \"The Daily Grind\" restaurant and its attributes.  One document lists its ambience as \"casual,\" indicating a relaxed atmosphere.  Other documents contain customer reviews, but these focus on service and food quality, not the establishment's ambience details like decor, music, or lighting.  Therefore, the answer will primarily rely on the \"casual\" ambience description.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}, 'business_stars': 3.5, 'review_info': [{'review_stars': 3.0, 'review_date': '2022-01-18 17:28:38', 'review_text': 'I really love the daily grind and would give it 5 stars for their coffee and bagels and pastries...'}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the general customer feedback regarding the atmosphere of the establishment?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer feedback on the atmosphere is mixed. One review mentions finding a bug in a salad [1, 4], while another praises the friendly owner and good coffee [1, 4].  A separate establishment, \"The Daily Grind,\" is described as having a casual ambiance [2, 5]. \n",
      "\n",
      "Reasoning: The provided text contains reviews from different sources.  Document [1] and [4] contain mixed reviews, with one mentioning a bug in a salad and another praising the coffee and owner's friendliness. Document [2] and [5] refer to \"The Daily Grind,\" indicating a casual ambiance.  There is no other information about the atmosphere in the other documents.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe one time I got a salad though there was a bug in it. A green beetle looking bug with little legs curled up, it blended in and I may have even eaten a bug or two who knows.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe coffee is strong and tasteful. The owner is really friendly and funny and it is always a pleasant surprise when I see him working the cash register.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33m{'name': 'The Daily Grind', 'address': '2912 De La Vina St', 'city': 'Santa Barbara', 'state': 'CA', 'categories': 'Restaurants, Sandwiches, Breakfast & Brunch, Coffee & Tea, Food, Juice Bars & Smoothies', 'hours': {'Monday': '6:0-14:0', 'Tuesday': '6:0-14:0', 'Wednesday': '6:0-14:0', 'Thursday': '6:0-14:0', 'Friday': '6:0-14:0', 'Saturday': '6:0-14:0', 'Sunday': '6:0-14:0'}, 'attributes': {'BusinessParking': {'garage': False, 'street': True, 'validated': False, 'lot': True, 'valet': False}, 'RestaurantsReservations': False, 'OutdoorSeating': True, 'WiFi': 'free', 'RestaurantsTakeOut': True, 'RestaurantsGoodForGroups': True, 'Music': None, 'Ambience': {'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}}\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported due to a lack of sufficient positive evidence describing the atmosphere.  Since two out of three claims are unsupported or only one is supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCustomers can place reservations, take out their orders, and enjoy the ambience of the casual establishment.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.67\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement claims that customers can make reservations, take out orders, and enjoy the ambience.  The claim about reservations is unsupported because the establishment does not accept reservations. The claim about takeout is supported as the establishment offers takeout. The claim about ambience is unsupported due to a lack of sufficient positive evidence describing the atmosphere.  Since two out of three claims are unsupported or only one is supported, the overall statement is considered unsupported.\u001b[0m \u001b[0m\n",
      "Running row 222 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa121d9ec704a69bbadeb200233a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAchilles tendon injury medical definition\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymptoms of Achilles tendon rupture\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia fracture symptoms\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia injury diagnosis\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an injury to his Achilles tendon, citing sources [3] and [8]. The second QA pair confirms an injury to his left tibia, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources. Therefore, the claim is strongly supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the Achilles tendon injury.  The sources provide specific details about the injury.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the left tibia injury. The sources describe a severe injury to the left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an injury to his Achilles tendon, citing sources [3] and [8]. The second QA pair confirms an injury to his left tibia, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources. Therefore, the claim is strongly supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an injury to his Achilles tendon, citing sources [3] and [8]. The second QA pair confirms an injury to his left tibia, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources. Therefore, the claim is strongly supported by the evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an injury to his Achilles tendon, citing sources [3] and [8]. The second QA pair confirms an injury to his left tibia, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources. Therefore, the claim is strongly supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAchilles tendon injury medical definition\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymptoms of Achilles tendon rupture\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia fracture symptoms\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia injury diagnosis\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, strongly supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the Achilles tendon injury.  Multiple sources corroborate the injury.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the left tibia injury. Multiple sources corroborate the injury.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, strongly supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, strongly supporting the claim.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, strongly supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mAchilles tendon injury medical definition\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33msymptoms of Achilles tendon rupture\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Achilles tendon injury medical definition', 'symptoms of Achilles tendon rupture']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia fracture symptoms\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mleft tibia injury diagnosis\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['left tibia fracture symptoms', 'left tibia injury diagnosis']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the Achilles tendon injury.  Multiple sources corroborate the injury.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim regarding the left tibia injury. Multiple sources corroborate the injury.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe speaker noticed injuries to his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his Achilles tendon?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing. His Achilles tendon resembled \"transparent tape covered in blood\" [3, 8]. \n",
      "\n",
      "Reasoning: Based on the provided text, Steve Woolfenden sustained an injury to his Achilles tendon during the Boston Marathon bombing.  Document [3] and [8] explicitly state that he suffered an injury to his Achilles tendon, describing it as resembling \"transparent tape covered in blood\".\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the speaker sustain an injury to his left tibia?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Steve Woolfenden sustained an injury to his left tibia; his left tibia was protruding from his boot after the bombing. [1, 7] \n",
      "\n",
      "Reasoning: The provided text mentions Steve Woolfenden's injuries sustained during the Boston Marathon bombing.  Document [1] and [7] explicitly state that his left tibia was protruding from his boot, indicating a significant injury to his left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThat was before he noticed his Achilles tendon, which resembled transparent tape covered in blood, and his left tibia protruding from his boot. The boot was next to his left stump, he testified before a federal jury Thursday, the third day in which survivors and family members of those killed in the Boston Marathon bombing shared their stories -- often gruesome and heartbreaking -- in the sentencing phase for Dzhokhar Tsarnaev.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, supporting the claim.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mHe noticed his own injuries, including his Achilles tendon and left tibia.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that the speaker (Steve Woolfenden) noticed injuries to his Achilles tendon and left tibia.  The provided QA pairs confirm this. The first QA pair confirms an Achilles tendon injury, citing sources [3] and [8]. The second QA pair confirms a left tibia injury, citing sources [1] and [7].  Both injuries are explicitly mentioned in the provided sources, supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 223 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fadc1e0dbf24805a12cb877fd76f4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhere did the customer meet Katie, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['review location Katie meeting']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid the customer and Katie connect immediately upon meeting, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['review immediate connection Katie customer']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mDid the review mention friendly bartenders contributing to the customer and Katie connecting?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['review friendly bartenders role connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhere did the customer meet Katie, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review location Katie meeting']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreview location Katie meeting\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhere did the customer meet Katie, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review location Katie meeting']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe reviewer met Katie at Peabody's bar. [3] \n",
      "\n",
      "Reasoning: Document [3] describes a scenario where the reviewer met Katie at Peabody's.  The reviewer was sitting at the bar when Katie and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe last time I went to Peabody's I had such a good time I just had to write a review. Here's what happened:  I was sitting at the bar drinking a beer and eating some nachos when this gorgeous girl, Katie, and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the customer and Katie connect immediately upon meeting, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review immediate connection Katie customer']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreview immediate connection Katie customer\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the customer and Katie connect immediately upon meeting, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review immediate connection Katie customer']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on the review, the customer and Katie did not connect immediately upon meeting, but rather formed a connection quickly after an initial shared observation about their phones.  They began talking about how they lost their phones and continued conversing for an extended period [1]. \n",
      "\n",
      "Reasoning: The provided text focuses on a customer's experience meeting Katie at a bar.  Document [1] details their interaction, starting with a comment about their phones and progressing into a conversation filled with jokes and stories.  While it doesn't explicitly state \"immediate connection,\" the narrative strongly implies one, as they quickly moved from a shared observation to extended conversation and a phone number exchange.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the review mention friendly bartenders contributing to the customer and Katie connecting?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review friendly bartenders role connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mreview friendly bartenders role connection\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the review mention friendly bartenders contributing to the customer and Katie connecting?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['review friendly bartenders role connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, review [3] describes a bartender who kept the reviewer and Katie's drinks flowing, ensured they were having a good time, and even subtly encouraged the reviewer to continue interacting with Katie, ultimately leading to them exchanging numbers and beginning a relationship. \n",
      "\n",
      "Reasoning: The provided text mentions several instances of bartenders' actions impacting customer experiences.  Review [3] directly addresses the question.  A bartender's actions facilitated a connection between two customers, Katie and the reviewer. The bartender kept their drinks flowing, ensured they were having a good time, and even subtly encouraged the reviewer to continue interacting with Katie.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that a customer met Katie at a bar and they connected immediately due to friendly bartenders.  QA pair 1 confirms the meeting location at Peabody's bar. QA pair 2 indicates that while not an immediate connection, a strong connection formed quickly after an initial shared observation.  QA pair 3 supports the claim that friendly bartenders played a role in facilitating the connection by keeping their drinks flowing and subtly encouraging interaction.  While the connection wasn't instantaneous, the bartenders' actions significantly contributed to the development of the relationship.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Where did the customer meet Katie, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The reviewer met Katie at Peabody's bar. [3]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Confirms the meeting location as stated in the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the customer and Katie connect immediately upon meeting, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on the review, the customer and Katie did not connect immediately upon meeting, but rather formed a connection quickly after an initial shared observation about their phones.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: While not immediately, a strong connection developed quickly, partially supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the review mention friendly bartenders contributing to the customer and Katie connecting?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, review [3] describes a bartender who kept the reviewer and Katie's drinks flowing, ensured they were having a good time, and even subtly encouraged the reviewer to continue interacting with Katie, ultimately leading to them exchanging numbers and beginning a relationship.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Strongly supports the claim by highlighting the bartenders' significant role in facilitating the connection.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that a customer met Katie at a bar and they connected immediately due to friendly bartenders.  QA pair 1 confirms the meeting location at Peabody's bar. QA pair 2 indicates that while not an immediate connection, a strong connection formed quickly after an initial shared observation.  QA pair 3 supports the claim that friendly bartenders played a role in facilitating the connection by keeping their drinks flowing and subtly encouraging interaction.  While the connection wasn't instantaneous, the bartenders' actions significantly contributed to the development of the relationship.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhere did the customer meet Katie, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe reviewer met Katie at Peabody's bar. [3] \n",
      "\n",
      "Reasoning: Document [3] describes a scenario where the reviewer met Katie at Peabody's.  The reviewer was sitting at the bar when Katie and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe last time I went to Peabody's I had such a good time I just had to write a review. Here's what happened:  I was sitting at the bar drinking a beer and eating some nachos when this gorgeous girl, Katie, and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the customer and Katie connect immediately upon meeting, according to the review?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on the review, the customer and Katie did not connect immediately upon meeting, but rather formed a connection quickly after an initial shared observation about their phones.  They began talking about how they lost their phones and continued conversing for an extended period [1]. \n",
      "\n",
      "Reasoning: The provided text focuses on a customer's experience meeting Katie at a bar.  Document [1] details their interaction, starting with a comment about their phones and progressing into a conversation filled with jokes and stories.  While it doesn't explicitly state \"immediate connection,\" the narrative strongly implies one, as they quickly moved from a shared observation to extended conversation and a phone number exchange.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the review mention friendly bartenders contributing to the customer and Katie connecting?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, review [3] describes a bartender who kept the reviewer and Katie's drinks flowing, ensured they were having a good time, and even subtly encouraged the reviewer to continue interacting with Katie, ultimately leading to them exchanging numbers and beginning a relationship. \n",
      "\n",
      "Reasoning: The provided text mentions several instances of bartenders' actions impacting customer experiences.  Review [3] directly addresses the question.  A bartender's actions facilitated a connection between two customers, Katie and the reviewer. The bartender kept their drinks flowing, ensured they were having a good time, and even subtly encouraged the reviewer to continue interacting with Katie.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that a customer met Katie at a bar and they connected immediately due to friendly bartenders.  QA pair 1 confirms the meeting location at Peabody's bar. QA pair 2 indicates that while not an immediate connection, a strong connection formed quickly after an initial shared observation.  QA pair 3 supports the claim that friendly bartenders played a role in facilitating the connection by keeping their drinks flowing and subtly encouraging interaction.  While the connection wasn't instantaneous, the bartenders' actions significantly contributed to the development of the relationship.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that a customer met Katie at a bar and they connected immediately due to friendly bartenders.  QA pair 1 confirms the meeting location at Peabody's bar. QA pair 2 indicates that while not an immediate connection, a strong connection formed quickly after an initial shared observation.  QA pair 3 supports the claim that friendly bartenders played a role in facilitating the connection by keeping their drinks flowing and subtly encouraging interaction.  While the connection wasn't instantaneous, the bartenders' actions significantly contributed to the development of the relationship.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately, facilitated by friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid a customer meet Katie at a bar and connect immediately?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer review bar meeting Katie immediate connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid friendly bartenders facilitate the connection between the customer and Katie at the bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer review bar friendly bartenders facilitated meeting Katie']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid a customer meet Katie at a bar and connect immediately?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer review bar meeting Katie immediate connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer review bar meeting Katie immediate connection\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid a customer meet Katie at a bar and connect immediately?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer review bar meeting Katie immediate connection']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, a customer met Katie at a bar and they connected immediately after discovering they both lost their iPhones [1]. They spent time talking and laughing, and the bartender even encouraged the interaction [1].  The customer eventually got Katie's number [1]. \n",
      "\n",
      "Reasoning: Document [1] describes a scenario where a man met a woman named Katie at a bar.  They connected immediately after discovering they both lost their iPhones. They talked, laughed, and the bartender encouraged the interaction. The man eventually got Katie's number.  This aligns with the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\\n\\nThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\" I\\'m hopelessly ignorant to the subtle signals of the opposite sex, and if I hadn\\'t taken his advice I probably would have blown it. We kept talking for a while and I finally asked her for her number.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid friendly bartenders facilitate the connection between the customer and Katie at the bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer review bar friendly bartenders facilitated meeting Katie']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer review bar friendly bartenders facilitated meeting Katie\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid friendly bartenders facilitate the connection between the customer and Katie at the bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer review bar friendly bartenders facilitated meeting Katie']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, in one instance, a friendly bartender facilitated a connection between a customer and Katie. The bartender ensured the customer and Katie had a good time by keeping their drinks flowing and subtly encouraged the customer to stay when the conversation lagged, ultimately leading to them exchanging numbers [1]. \n",
      "\n",
      "Reasoning: Document [1] describes a scenario where a bartender facilitated a connection between the customer and Katie.  The bartender kept their drinks flowing, ensured they were having a good time, and even subtly encouraged the customer to stay by signaling that Katie liked him. This bartender's actions directly contributed to the customer and Katie connecting and ultimately getting each other's phone numbers.  Other documents mention bartenders, but don't describe them facilitating a meeting between specific individuals.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately, facilitated by friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair confirms a customer met Katie at a bar and connected immediately. The second QA pair confirms that friendly bartenders facilitated this connection by encouraging the interaction and ensuring a positive environment.  Both answers cite the same source ([1]), which strengthens the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did a customer meet Katie at a bar and connect immediately?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, a customer met Katie at a bar and they connected immediately after discovering they both lost their iPhones [1]. They spent time talking and laughing, and the bartender even encouraged the interaction [1].  The customer eventually got Katie's number [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming a customer met Katie at a bar and connected immediately.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did friendly bartenders facilitate the connection between the customer and Katie at the bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, in one instance, a friendly bartender facilitated a connection between a customer and Katie. The bartender ensured the customer and Katie had a good time by keeping their drinks flowing and subtly encouraged the customer to stay when the conversation lagged, ultimately leading to them exchanging numbers [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming friendly bartenders facilitated the connection between the customer and Katie.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately, facilitated by friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair confirms a customer met Katie at a bar and connected immediately. The second QA pair confirms that friendly bartenders facilitated this connection by encouraging the interaction and ensuring a positive environment.  Both answers cite the same source ([1]), which strengthens the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid a customer meet Katie at a bar and connect immediately?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, a customer met Katie at a bar and they connected immediately after discovering they both lost their iPhones [1]. They spent time talking and laughing, and the bartender even encouraged the interaction [1].  The customer eventually got Katie's number [1]. \n",
      "\n",
      "Reasoning: Document [1] describes a scenario where a man met a woman named Katie at a bar.  They connected immediately after discovering they both lost their iPhones. They talked, laughed, and the bartender encouraged the interaction. The man eventually got Katie's number.  This aligns with the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\\n\\nThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\" I\\'m hopelessly ignorant to the subtle signals of the opposite sex, and if I hadn\\'t taken his advice I probably would have blown it. We kept talking for a while and I finally asked her for her number.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid friendly bartenders facilitate the connection between the customer and Katie at the bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, in one instance, a friendly bartender facilitated a connection between a customer and Katie. The bartender ensured the customer and Katie had a good time by keeping their drinks flowing and subtly encouraged the customer to stay when the conversation lagged, ultimately leading to them exchanging numbers [1]. \n",
      "\n",
      "Reasoning: Document [1] describes a scenario where a bartender facilitated a connection between the customer and Katie.  The bartender kept their drinks flowing, ensured they were having a good time, and even subtly encouraged the customer to stay by signaling that Katie liked him. This bartender's actions directly contributed to the customer and Katie connecting and ultimately getting each other's phone numbers.  Other documents mention bartenders, but don't describe them facilitating a meeting between specific individuals.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe whole time the bartender kept our drinks flowing and made sure we were having a good time. At one point when the conversation ran dry and I was about to pay my bill and go. Luckily I got the signal from the bartender, \"stick around, this girl likes you.\"\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair confirms a customer met Katie at a bar and connected immediately. The second QA pair confirms that friendly bartenders facilitated this connection by encouraging the interaction and ensuring a positive environment.  Both answers cite the same source ([1]), which strengthens the evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs directly support the claim.  The first QA pair confirms a customer met Katie at a bar and connected immediately. The second QA pair confirms that friendly bartenders facilitated this connection by encouraging the interaction and ensuring a positive environment.  Both answers cite the same source ([1]), which strengthens the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs there a review mentioning a customer meeting someone named Katie at a bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['\"customer met Katie\" bar review']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDid the review state that the customer and Katie connected immediately, and attribute this to friendly bartenders?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['\"customer Katie\" bar review \"immediate connection\" \"friendly bartenders\"']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a review mentioning a customer meeting someone named Katie at a bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"customer met Katie\" bar review']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"customer met Katie\" bar review\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a review mentioning a customer meeting someone named Katie at a bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"customer met Katie\" bar review']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, one review mentions a customer meeting someone named Katie at a bar.  The review describes the author sitting at the bar when \"this gorgeous girl, Katie, and her friend walked in\" [5]. \n",
      "\n",
      "Reasoning: Document [1] describes a customer meeting Katie at a bar, but it doesn't explicitly state it was a customer review.  Document [5] mentions a customer encountering a woman named Katie at Peabody's bar, which is presented as part of a review.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe last time I went to Peabody's I had such a good time I just had to write a review. Here's what happened:  I was sitting at the bar drinking a beer and eating some nachos when this gorgeous girl, Katie, and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the review state that the customer and Katie connected immediately, and attribute this to friendly bartenders?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"customer Katie\" bar review \"immediate connection\" \"friendly bartenders\"']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"customer Katie\" bar review \"immediate connection\" \"friendly bartenders\"\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the review state that the customer and Katie connected immediately, and attribute this to friendly bartenders?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"customer Katie\" bar review \"immediate connection\" \"friendly bartenders\"']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mA review describes a scenario where the reviewer met Katie at a bar and they connected after discovering a shared experience (losing their iPhones) [2]. While the bartender kept their drinks flowing and subtly encouraged the interaction, the review doesn't attribute their immediate connection solely to friendly bartenders [2]. \n",
      "\n",
      "Reasoning: Document [2] describes a scenario where the reviewer met Katie at a bar.  They connected after discovering a shared experience (losing their iPhones). The bartender's actions of keeping their drinks flowing and subtly encouraging the interaction are mentioned, but not explicitly stated as the reason for their immediate connection.  The connection is attributed to shared experiences and conversation, not solely to friendly bartenders.  Other documents do not mention Katie.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\\n\\nThe whole time the bartender kept our drinks flowing and made sure we were having a good time.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe first QA pair confirms a review exists where a customer met Katie at a bar.  The second QA pair clarifies that while a review details a meeting between a customer and Katie at a bar, and mentions friendly bartenders facilitating their interaction, it doesn't explicitly state that their immediate connection was *solely* due to the friendly bartenders.  The connection is attributed more to a shared experience (losing their iPhones). Therefore, the claim is partially supported but lacks the crucial element of the connection being attributed to the bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there a review mentioning a customer meeting someone named Katie at a bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, one review mentions a customer meeting someone named Katie at a bar.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the 'meeting at a bar' part of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the review state that the customer and Katie connected immediately, and attribute this to friendly bartenders?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The review mentions a connection but attributes it to a shared experience, not solely to friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Undermines the claim's assertion about the reason for their immediate connection.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mAccording to a review, a customer met Katie at a bar and they connected immediately due to the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe first QA pair confirms a review exists where a customer met Katie at a bar.  The second QA pair clarifies that while a review details a meeting between a customer and Katie at a bar, and mentions friendly bartenders facilitating their interaction, it doesn't explicitly state that their immediate connection was *solely* due to the friendly bartenders.  The connection is attributed more to a shared experience (losing their iPhones). Therefore, the claim is partially supported but lacks the crucial element of the connection being attributed to the bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there a review mentioning a customer meeting someone named Katie at a bar?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, one review mentions a customer meeting someone named Katie at a bar.  The review describes the author sitting at the bar when \"this gorgeous girl, Katie, and her friend walked in\" [5]. \n",
      "\n",
      "Reasoning: Document [1] describes a customer meeting Katie at a bar, but it doesn't explicitly state it was a customer review.  Document [5] mentions a customer encountering a woman named Katie at Peabody's bar, which is presented as part of a review.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe last time I went to Peabody's I had such a good time I just had to write a review. Here's what happened:  I was sitting at the bar drinking a beer and eating some nachos when this gorgeous girl, Katie, and her friend walked in.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the review state that the customer and Katie connected immediately, and attribute this to friendly bartenders?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mA review describes a scenario where the reviewer met Katie at a bar and they connected after discovering a shared experience (losing their iPhones) [2]. While the bartender kept their drinks flowing and subtly encouraged the interaction, the review doesn't attribute their immediate connection solely to friendly bartenders [2]. \n",
      "\n",
      "Reasoning: Document [2] describes a scenario where the reviewer met Katie at a bar.  They connected after discovering a shared experience (losing their iPhones). The bartender's actions of keeping their drinks flowing and subtly encouraging the interaction are mentioned, but not explicitly stated as the reason for their immediate connection.  The connection is attributed to shared experiences and conversation, not solely to friendly bartenders.  Other documents do not mention Katie.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThey sat next to me and I noticed that Katie had the same phone as me, a crappy freebie phone they give you when you lose your iphone, and so I said, \"Hey I like your crappy cell-phone, did you lose your iphone like me?\"  As it turned out she had. We swapped stories about how we had drunkenly lost our phones and after that we just kept talking. She was making me laugh telling raunchy jokes and I was trying to impress her by weaving beach volleyball stories into the conversation.\\n\\nThe whole time the bartender kept our drinks flowing and made sure we were having a good time.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe first QA pair confirms a review exists where a customer met Katie at a bar.  The second QA pair clarifies that while a review details a meeting between a customer and Katie at a bar, and mentions friendly bartenders facilitating their interaction, it doesn't explicitly state that their immediate connection was *solely* due to the friendly bartenders.  The connection is attributed more to a shared experience (losing their iPhones). Therefore, the claim is partially supported but lacks the crucial element of the connection being attributed to the bartenders.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mAccording to the review, the customer met Katie at the bar and they hit it off immediately, thanks to the help of the friendly bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe first QA pair confirms a review exists where a customer met Katie at a bar.  The second QA pair clarifies that while a review details a meeting between a customer and Katie at a bar, and mentions friendly bartenders facilitating their interaction, it doesn't explicitly state that their immediate connection was *solely* due to the friendly bartenders.  The connection is attributed more to a shared experience (losing their iPhones). Therefore, the claim is partially supported but lacks the crucial element of the connection being attributed to the bartenders.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 224 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b059733f77c443dbc9a6aeea6e07416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu modern Chinese dishes', '[Restaurant Name] innovative Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu traditional Chinese dishes', '[Restaurant Name] classic Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mHow does the restaurant's menu compare to other Chinese restaurants in terms of its unique offerings?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu comparison other Chinese restaurants', '[Restaurant Name] unique dishes Chinese cuisine']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu modern Chinese dishes', '[Restaurant Name] innovative Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu modern Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] innovative Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu modern Chinese dishes', '[Restaurant Name] innovative Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on customer reviews, the mala soup is described as delicious but not traditional [2], suggesting it may represent a modern interpretation of Chinese cuisine.  The jian bing (Chinese crepe) is also mentioned [1, 5], and while a traditional food, its preparation at this restaurant could be a modern take on the dish.  However, without access to the restaurant's menu, it's impossible to definitively identify specific dishes as modern interpretations. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly label any as \"modern interpretations of Chinese cuisine\".  However, one review describes a mala soup as delicious but not \"traditional\" [2], suggesting it might be a modern adaptation.  Another review mentions jian bing (Chinese crepe) [1, 5], which while a traditional food, could be prepared in a modern style depending on the restaurant's approach.  The lack of specific menu information prevents a definitive answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mI think I'll be back; maybe I can try the jian bing (Chinese crepe) next time.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu traditional Chinese dishes', '[Restaurant Name] classic Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu traditional Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] classic Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu traditional Chinese dishes', '[Restaurant Name] classic Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on the provided reviews, the restaurant offers dishes such as mala soup, mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective. One reviewer, familiar with Hong Kong and Asian cuisine, considered a mala soup dish delicious but not necessarily \"traditional\" [2]. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly state which are considered traditional Chinese cuisine.  The reviews mention mala soup, mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles. However, whether these are considered \"traditional\" is subjective and not explicitly stated in the source. One review mentions the author's experience in Hong Kong and Asia, stating that while the mala soup was delicious, it wasn't necessarily \"traditional\" in some respects.  Therefore, a definitive answer about which dishes are specifically traditional Chinese cuisine cannot be provided based on the given text.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWhile I can\\\\'t quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the restaurant's menu compare to other Chinese restaurants in terms of its unique offerings?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu comparison other Chinese restaurants', '[Restaurant Name] unique dishes Chinese cuisine']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu comparison other Chinese restaurants\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] unique dishes Chinese cuisine\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the restaurant's menu compare to other Chinese restaurants in terms of its unique offerings?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu comparison other Chinese restaurants', '[Restaurant Name] unique dishes Chinese cuisine']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on the provided reviews, the restaurant offers dishes such as mala tang, dry pot, jian bing (a Chinese crepe), and spicy soup noodles. However, there is no information comparing these offerings to those of other Chinese restaurants.  Reviews mention the food as \"ok but not great\" [1, 5] and that the price is higher than expected [1, 5], but these are subjective opinions and don't provide a comparison to other establishments. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes, including mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles.  However, there's no direct comparison to other Chinese restaurants' menus.  The reviews offer opinions on the food quality and price but lack comparative data.  Therefore, a definitive answer comparing the restaurant's menu to others is not possible based on the given information.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided QA pairs offer some evidence, but it's largely inconclusive.  While some dishes mentioned (mala soup, jian bing) could be interpreted as modern or traditional depending on their preparation, there's no definitive information from the provided text to support this claim.  The reviews mention dishes but don't explicitly categorize them as modern or traditional, and there's no comparison to other restaurants to establish uniqueness.  The lack of a menu and detailed descriptions prevents a strong conclusion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on customer reviews, the mala soup is described as delicious but not traditional [2], suggesting it may represent a modern interpretation of Chinese cuisine.  The jian bing (Chinese crepe) is also mentioned [1, 5], and while a traditional food, its preparation at this restaurant could be a modern take on the dish.  However, without access to the restaurant's menu, it's impossible to definitively identify specific dishes as modern interpretations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Suggests some dishes *might* be modern interpretations, but lacks definitive proof. The ambiguity weakens support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on the provided reviews, the restaurant offers dishes such as mala soup, mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Identifies dishes, but doesn't confirm their traditional status.  The lack of explicit categorization weakens support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How does the restaurant's menu compare to other Chinese restaurants in terms of its unique offerings?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on the provided reviews, the restaurant offers dishes such as mala tang, dry pot, jian bing (a Chinese crepe), and spicy soup noodles. However, there is no information comparing these offerings to those of other Chinese restaurants.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: No comparison to other restaurants is made, preventing assessment of the restaurant's uniqueness. This significantly weakens support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] ingredient sourcing\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] supplier relationships\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but the provided reviews do not contain information about Mala Town's sourcing policy or its prioritization of fresh, high-quality ingredients. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific ingredients and dishes (e.g., broccoli, Chinese cabbage, tofu rolls, mung bean soup), they don't explicitly state the restaurant's sourcing policy or whether it prioritizes fresh, high-quality products.  Therefore, I cannot answer the question directly based on the given information.\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] reviews ingredients\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] Yelp reviews freshness\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient quality. One reviewer specifically noted the broccoli in their mala soup was \"fresh and perfectly cooked\" [2]. Another review mentioned the mung beans in a complimentary soup were \"perfectly cooked\" [3], suggesting fresh, high-quality ingredients. However, other reviews did not focus on the freshness or quality of ingredients. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review explicitly praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not directly commenting on freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews offer less positive feedback, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.4\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided QA pairs offer mixed evidence regarding the claim.  While one answer indicates a lack of direct information about the restaurant's sourcing policy, the second answer reveals mixed customer feedback on ingredient freshness and quality. Some reviews praise the freshness of specific ingredients (broccoli, implicitly mung beans), suggesting support for the claim. However, the absence of consistent positive feedback across all reviews weakens the support.  The claim is partially supported by some positive reviews but lacks comprehensive evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but the provided reviews do not contain information about Mala Town's sourcing policy or its prioritization of fresh, high-quality ingredients. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific ingredients and dishes (e.g., broccoli, Chinese cabbage, tofu rolls, mung bean soup), they don't explicitly state the restaurant's sourcing policy or whether it prioritizes fresh, high-quality products.  Therefore, I cannot answer the question directly based on the given information.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This question yields no direct support for the claim. The lack of information on sourcing weakens the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer reviews offer mixed feedback on ingredient quality. One reviewer specifically noted the broccoli in their mala soup was \"fresh and perfectly cooked\" [2]. Another review mentioned the mung beans in a complimentary soup were \"perfectly cooked\" [3], suggesting fresh, high-quality ingredients. However, other reviews did not focus on the freshness or quality of ingredients. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review explicitly praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not directly commenting on freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews offer less positive feedback, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Mixed evidence. Some reviews mention fresh ingredients (broccoli), while others don't comment on freshness or quality. This provides weak, partial support.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks evidence of specific dishes representing modern interpretations, and the evidence for traditional dishes is subjective and inconclusive.  The claim about fresh and high-quality ingredients has mixed reviews; while some reviews mention fresh ingredients in specific dishes, this is not consistent across all reviews, and there's no information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided QA pairs offer some evidence, but it's largely inconclusive.  While some dishes mentioned (mala soup, jian bing) could be interpreted as modern or traditional depending on their preparation, there's no definitive information from the provided text to support this claim.  The reviews mention dishes but don't explicitly categorize them as modern or traditional, and there's no comparison to other restaurants to establish uniqueness.  The lack of a menu and detailed descriptions prevents a strong conclusion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on customer reviews, the mala soup is described as delicious but not traditional [2], suggesting it may represent a modern interpretation of Chinese cuisine.  The jian bing (Chinese crepe) is also mentioned [1, 5], and while a traditional food, its preparation at this restaurant could be a modern take on the dish.  However, without access to the restaurant's menu, it's impossible to definitively identify specific dishes as modern interpretations. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly label any as \"modern interpretations of Chinese cuisine\".  However, one review describes a mala soup as delicious but not \"traditional\" [2], suggesting it might be a modern adaptation.  Another review mentions jian bing (Chinese crepe) [1, 5], which while a traditional food, could be prepared in a modern style depending on the restaurant's approach.  The lack of specific menu information prevents a definitive answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mI think I'll be back; maybe I can try the jian bing (Chinese crepe) next time.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on the provided reviews, the restaurant offers dishes such as mala soup, mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective. One reviewer, familiar with Hong Kong and Asian cuisine, considered a mala soup dish delicious but not necessarily \"traditional\" [2]. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly state which are considered traditional Chinese cuisine.  The reviews mention mala soup, mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles. However, whether these are considered \"traditional\" is subjective and not explicitly stated in the source. One review mentions the author's experience in Hong Kong and Asia, stating that while the mala soup was delicious, it wasn't necessarily \"traditional\" in some respects.  Therefore, a definitive answer about which dishes are specifically traditional Chinese cuisine cannot be provided based on the given text.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWhile I can\\\\'t quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow does the restaurant's menu compare to other Chinese restaurants in terms of its unique offerings?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on the provided reviews, the restaurant offers dishes such as mala tang, dry pot, jian bing (a Chinese crepe), and spicy soup noodles. However, there is no information comparing these offerings to those of other Chinese restaurants.  Reviews mention the food as \"ok but not great\" [1, 5] and that the price is higher than expected [1, 5], but these are subjective opinions and don't provide a comparison to other establishments. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes, including mala tang, dry pot, jian bing (Chinese crepe), and spicy soup noodles.  However, there's no direct comparison to other Chinese restaurants' menus.  The reviews offer opinions on the food quality and price but lack comparative data.  Therefore, a definitive answer comparing the restaurant's menu to others is not possible based on the given information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.4\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided QA pairs offer mixed evidence regarding the claim.  While one answer indicates a lack of direct information about the restaurant's sourcing policy, the second answer reveals mixed customer feedback on ingredient freshness and quality. Some reviews praise the freshness of specific ingredients (broccoli, implicitly mung beans), suggesting support for the claim. However, the absence of consistent positive feedback across all reviews weakens the support.  The claim is partially supported by some positive reviews but lacks comprehensive evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but the provided reviews do not contain information about Mala Town's sourcing policy or its prioritization of fresh, high-quality ingredients. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific ingredients and dishes (e.g., broccoli, Chinese cabbage, tofu rolls, mung bean soup), they don't explicitly state the restaurant's sourcing policy or whether it prioritizes fresh, high-quality products.  Therefore, I cannot answer the question directly based on the given information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient quality. One reviewer specifically noted the broccoli in their mala soup was \"fresh and perfectly cooked\" [2]. Another review mentioned the mung beans in a complimentary soup were \"perfectly cooked\" [3], suggesting fresh, high-quality ingredients. However, other reviews did not focus on the freshness or quality of ingredients. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review explicitly praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not directly commenting on freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews offer less positive feedback, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks evidence of specific dishes representing modern interpretations, and the evidence for traditional dishes is subjective and inconclusive.  The claim about fresh and high-quality ingredients has mixed reviews; while some reviews mention fresh ingredients in specific dishes, this is not consistent across all reviews, and there's no information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks evidence of specific dishes representing modern interpretations, and the evidence for traditional dishes is subjective and inconclusive.  The claim about fresh and high-quality ingredients has mixed reviews; while some reviews mention fresh ingredients in specific dishes, this is not consistent across all reviews, and there's no information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific dishes are offered on the restaurant's menu, and how are they categorized (e.g., modern, traditional)?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] online menu pdf']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat do online reviews say about the restaurant's blend of modern and traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] reviews', '[Restaurant Name] yelp reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes are offered on the restaurant's menu, and how are they categorized (e.g., modern, traditional)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] online menu pdf']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] online menu pdf\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes are offered on the restaurant's menu, and how are they categorized (e.g., modern, traditional)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] online menu pdf']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mMala Town's menu includes mala soup, mala tang, dry pot (a peanut-based sauce dish) [2], jian bing (a Chinese crepe) [2, 6], and spicy soup noodles [3].  The mala tang and dry pot allow customers to customize their dishes by selecting ingredients to add to the base [2]. While one review notes a dish wasn't strictly \"traditional\" [3], the restaurant overall offers a mix of modern and classic Chinese cuisine [3]. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes served at Mala Town.  Reviews describe mala soup, mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles.  One review mentions the option to customize the mala tang and dry pot by choosing ingredients to add to the base.  The style of food is described as Chinese, and while one review notes that a dish wasn't strictly \"traditional,\" the overall impression is that the restaurant offers a blend of modern interpretations and classic Chinese dishes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mThe base is free, and you pay for the items you chose to add in (minimum of 4).\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mI think I'll be back; maybe I can try the jian bing (Chinese crepe) next time.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [6] \u001b[33mWe arrived at noon and were the only ones there.  The large and lush patio was great since the weather was nice, and my bowl of spicy soup noodles hit the spot.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do online reviews say about the restaurant's blend of modern and traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews', '[Restaurant Name] yelp reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] yelp reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do online reviews say about the restaurant's blend of modern and traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews', '[Restaurant Name] yelp reviews']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mOne reviewer, familiar with Asian cuisine, described the food as delicious, although not entirely \"traditional\" [2].  Another review mentioned both mala tang and dry pot, dishes that represent a blend of modern and traditional Chinese cooking techniques and flavors [1]. \n",
      "\n",
      "Reasoning: The provided text contains several online reviews of a restaurant.  Some reviews mention specific dishes, like mala soup and dry pot, and comment on their taste and authenticity. One reviewer explicitly mentions their experience with the food not being entirely \"traditional\" but still delicious.  Other reviews focus on aspects like price and service, but offer less direct insight into the blend of modern and traditional Chinese cuisine. I will synthesize an answer based on the reviews that directly address the blend of modern and traditional aspects.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: Online Review (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided question-answer pairs offer evidence supporting this claim.  The first QA pair lists several dishes, some of which are clearly traditional (Jian Bing), while others (Mala Tang, dry pot) represent a fusion or modern adaptation of traditional techniques. The customization options further suggest a modern approach. The second QA pair cites reviews. While one review notes a dish wasn't strictly traditional, the overall impression from the reviews and the menu items is that the restaurant successfully blends modern and traditional elements.  The evidence strongly suggests the claim is accurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific dishes are offered on the restaurant's menu, and how are they categorized (e.g., modern, traditional)?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Mala Town's menu includes mala soup, mala tang, dry pot (a peanut-based sauce dish) [2], jian bing (a Chinese crepe) [2, 6], and spicy soup noodles [3].  The mala tang and dry pot allow customers to customize their dishes by selecting ingredients to add to the base [2]. While one review notes a dish wasn't strictly \"traditional\" [3], the restaurant overall offers a mix of modern and classic Chinese cuisine [3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Lists dishes representing both traditional and modern styles.  Customization options point to a modern approach.  While one review notes a dish wasn't strictly traditional, the overall assessment is a mix of modern and classic styles.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What do online reviews say about the restaurant's blend of modern and traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: One reviewer, familiar with Asian cuisine, described the food as delicious, although not entirely \"traditional\" [2].  Another review mentioned both mala tang and dry pot, dishes that represent a blend of modern and traditional Chinese cooking techniques and flavors [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Reviews support the blend, with one acknowledging a deviation from strict tradition but still praising the food.  Mention of mala tang and dry pot further supports the blend of modern and traditional elements.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, specifically focusing on freshness and quality?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, specifically focusing on freshness and quality?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] ingredient sourcing\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] supplier relationships\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, specifically focusing on freshness and quality?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but the provided text does not contain information about Mala Town's sourcing policy for ingredients, specifically regarding freshness and quality. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific dishes and ingredients (e.g., fresh broccoli, perfectly cooked mung beans), they do not explicitly detail the restaurant's sourcing policy regarding freshness and quality.  Therefore, a direct answer to the question cannot be synthesized from the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] reviews ingredients\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] Yelp reviews freshness\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient freshness and quality. One review specifically highlights the broccoli in a mala soup as \"fresh and perfectly cooked\" [2]. Another mentions the mung beans in a complimentary soup as \"perfectly cooked\" [3], suggesting good quality. However, other reviews focus more on the overall dining experience and price, without detailed comments on ingredient quality [1, 5]. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review specifically praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not explicitly mentioning freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews are less positive, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text offers mixed evidence regarding the claim that the restaurant uses fresh and high-quality ingredients. While some customer reviews mention specific dishes with \"fresh\" ingredients (e.g., broccoli) and \"perfectly cooked\" ingredients (e.g., mung beans), implying high quality, these are isolated instances and don't represent a comprehensive assessment of the restaurant's overall sourcing and ingredient quality.  The absence of information about the restaurant's sourcing policy further weakens the support for the claim.  The evidence is insufficient to definitively support or refute the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the restaurant's sourcing policy for its ingredients, specifically focusing on freshness and quality?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but the provided text does not contain information about Mala Town's sourcing policy for ingredients, specifically regarding freshness and quality. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific dishes and ingredients (e.g., fresh broccoli, perfectly cooked mung beans), they do not explicitly detail the restaurant's sourcing policy regarding freshness and quality.  Therefore, a direct answer to the question cannot be synthesized from the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Lack of information on sourcing policy weakens support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer reviews offer mixed feedback on ingredient freshness and quality. One review specifically highlights the broccoli in a mala soup as \"fresh and perfectly cooked\" [2]. Another mentions the mung beans in a complimentary soup as \"perfectly cooked\" [3], suggesting good quality. However, other reviews focus more on the overall dining experience and price, without detailed comments on ingredient quality [1, 5]. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review specifically praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not explicitly mentioning freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews are less positive, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Mixed reviews; some positive mentions of fresh and well-cooked ingredients, but insufficient to strongly support the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement asserts that the restaurant offers a unique blend of modern and traditional Chinese cuisine and uses fresh, high-quality ingredients.  The first claim, regarding the culinary style, is supported by evidence from online reviews detailing dishes representing both modern and traditional Chinese cooking. However, the second claim, concerning the freshness and quality of ingredients, is unsupported. While some reviews mention the quality of specific ingredients in certain dishes, there's insufficient evidence to support a general claim about the restaurant's consistent use of fresh, high-quality ingredients across its menu.  Therefore, the overall verdict is based on the weaker claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided question-answer pairs offer evidence supporting this claim.  The first QA pair lists several dishes, some of which are clearly traditional (Jian Bing), while others (Mala Tang, dry pot) represent a fusion or modern adaptation of traditional techniques. The customization options further suggest a modern approach. The second QA pair cites reviews. While one review notes a dish wasn't strictly traditional, the overall impression from the reviews and the menu items is that the restaurant successfully blends modern and traditional elements.  The evidence strongly suggests the claim is accurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific dishes are offered on the restaurant's menu, and how are they categorized (e.g., modern, traditional)?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mMala Town's menu includes mala soup, mala tang, dry pot (a peanut-based sauce dish) [2], jian bing (a Chinese crepe) [2, 6], and spicy soup noodles [3].  The mala tang and dry pot allow customers to customize their dishes by selecting ingredients to add to the base [2]. While one review notes a dish wasn't strictly \"traditional\" [3], the restaurant overall offers a mix of modern and classic Chinese cuisine [3]. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes served at Mala Town.  Reviews describe mala soup, mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles.  One review mentions the option to customize the mala tang and dry pot by choosing ingredients to add to the base.  The style of food is described as Chinese, and while one review notes that a dish wasn't strictly \"traditional,\" the overall impression is that the restaurant offers a blend of modern interpretations and classic Chinese dishes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mThe base is free, and you pay for the items you chose to add in (minimum of 4).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mI think I'll be back; maybe I can try the jian bing (Chinese crepe) next time.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [6] \u001b[33mWe arrived at noon and were the only ones there.  The large and lush patio was great since the weather was nice, and my bowl of spicy soup noodles hit the spot.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat do online reviews say about the restaurant's blend of modern and traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mOne reviewer, familiar with Asian cuisine, described the food as delicious, although not entirely \"traditional\" [2].  Another review mentioned both mala tang and dry pot, dishes that represent a blend of modern and traditional Chinese cooking techniques and flavors [1]. \n",
      "\n",
      "Reasoning: The provided text contains several online reviews of a restaurant.  Some reviews mention specific dishes, like mala soup and dry pot, and comment on their taste and authenticity. One reviewer explicitly mentions their experience with the food not being entirely \"traditional\" but still delicious.  Other reviews focus on aspects like price and service, but offer less direct insight into the blend of modern and traditional Chinese cuisine. I will synthesize an answer based on the reviews that directly address the blend of modern and traditional aspects.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: Online Review (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: Online Review (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text offers mixed evidence regarding the claim that the restaurant uses fresh and high-quality ingredients. While some customer reviews mention specific dishes with \"fresh\" ingredients (e.g., broccoli) and \"perfectly cooked\" ingredients (e.g., mung beans), implying high quality, these are isolated instances and don't represent a comprehensive assessment of the restaurant's overall sourcing and ingredient quality.  The absence of information about the restaurant's sourcing policy further weakens the support for the claim.  The evidence is insufficient to definitively support or refute the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, specifically focusing on freshness and quality?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but the provided text does not contain information about Mala Town's sourcing policy for ingredients, specifically regarding freshness and quality. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific dishes and ingredients (e.g., fresh broccoli, perfectly cooked mung beans), they do not explicitly detail the restaurant's sourcing policy regarding freshness and quality.  Therefore, a direct answer to the question cannot be synthesized from the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient freshness and quality. One review specifically highlights the broccoli in a mala soup as \"fresh and perfectly cooked\" [2]. Another mentions the mung beans in a complimentary soup as \"perfectly cooked\" [3], suggesting good quality. However, other reviews focus more on the overall dining experience and price, without detailed comments on ingredient quality [1, 5]. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review specifically praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not explicitly mentioning freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews are less positive, focusing more on the overall experience and value rather than ingredient quality [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts that the restaurant offers a unique blend of modern and traditional Chinese cuisine and uses fresh, high-quality ingredients.  The first claim, regarding the culinary style, is supported by evidence from online reviews detailing dishes representing both modern and traditional Chinese cooking. However, the second claim, concerning the freshness and quality of ingredients, is unsupported. While some reviews mention the quality of specific ingredients in certain dishes, there's insufficient evidence to support a general claim about the restaurant's consistent use of fresh, high-quality ingredients across its menu.  Therefore, the overall verdict is based on the weaker claim.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts that the restaurant offers a unique blend of modern and traditional Chinese cuisine and uses fresh, high-quality ingredients.  The first claim, regarding the culinary style, is supported by evidence from online reviews detailing dishes representing both modern and traditional Chinese cooking. However, the second claim, concerning the freshness and quality of ingredients, is unsupported. While some reviews mention the quality of specific ingredients in certain dishes, there's insufficient evidence to support a general claim about the restaurant's consistent use of fresh, high-quality ingredients across its menu.  Therefore, the overall verdict is based on the weaker claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (3): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] modern Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] traditional Chinese dishes', '[Restaurant Name] menu classic dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         3. Question: \u001b[33mHow does the restaurant's menu differ from other Chinese restaurants in the area in terms of its blend of modern and traditional dishes?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] menu comparison', '[Restaurant Name] vs [Competitor Restaurant Name] menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] modern Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] modern Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu', '[Restaurant Name] modern Chinese dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mBased on customer reviews, Mala Town offers dishes that could be considered modern interpretations of Chinese cuisine.  These include mala soup [3, 4], mala tang [5], dry pot (with a peanut-based sauce) [5], and jian bing (a Chinese crepe) [5].  While not explicitly described as 'modern' on the menu, the reviews suggest these dishes might represent contemporary adaptations of traditional Chinese culinary styles. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly label any as \"modern interpretations of Chinese cuisine\".  However, the reviews suggest some dishes might be considered modern takes on traditional Chinese food.  The reviews mention \"mala soup\", \"mala tang\", \"dry pot (peanut-based sauce)\", and \"jian bing (Chinese crepe)\". While these dishes have roots in traditional Chinese cuisine, their preparation and presentation at Mala Town might represent a modern adaptation.  The lack of explicit labeling requires inference based on the context of the reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mI got a big bowl of tasty mala soup (at just the right spice level after I asked for it \"da la\" 大麻) with four ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe soup came topped with sliced spring onions and a generous dollop of chili sauce with sesame seeds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] traditional Chinese dishes', '[Restaurant Name] menu classic dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] traditional Chinese dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu classic dishes\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] traditional Chinese dishes', '[Restaurant Name] menu classic dishes']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided reviews mention several dishes served at the restaurant, including mala soup, mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles [1, 2, 6]. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective. One reviewer, familiar with Hong Kong and Asian cuisine, described a dish as delicious but not necessarily \"traditional\" [2, 5].  More information is needed to definitively answer which dishes on the menu represent traditional Chinese cuisine. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly state which are considered \"traditional\" Chinese cuisine.  The reviews mention mala soup,  mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles. However, whether these are considered \"traditional\" is subjective and not explicitly stated in the source. One review mentions the author's experience in Hong Kong and Asia, stating that while the mala soup was delicious, it wasn't necessarily \"traditional.\"  Therefore, a definitive answer about which dishes are traditionally Chinese cannot be provided based on the given text.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI got a big bowl of tasty mala soup (at just the right spice level after I asked for it \"da la\" 大麻) with four ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the restaurant's menu differ from other Chinese restaurants in the area in terms of its blend of modern and traditional dishes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu comparison', '[Restaurant Name] vs [Competitor Restaurant Name] menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] menu comparison\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] vs [Competitor Restaurant Name] menu\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [3/3] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow does the restaurant's menu differ from other Chinese restaurants in the area in terms of its blend of modern and traditional dishes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] menu comparison', '[Restaurant Name] vs [Competitor Restaurant Name] menu']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but the provided text does not contain information comparing Mala Town's menu to other Chinese restaurants in the area regarding its blend of modern and traditional dishes.  The reviews describe specific dishes served at Mala Town but lack comparative data. \n",
      "\n",
      "Reasoning: The provided text focuses on customer reviews of Mala Town, a Chinese restaurant.  The reviews mention specific dishes like mala soup, dry pot, and jian bing, but don't offer comparisons to other Chinese restaurants in the area.  Therefore, a direct comparison of Mala Town's menu's blend of modern and traditional dishes to competitors cannot be made based on this data.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.3\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided QA pairs offer some evidence, but it's inconclusive.  The first QA pair identifies dishes that *might* be considered modern interpretations, based on customer reviews and inference.  However, the reviews don't explicitly label them as such. The second QA pair similarly identifies dishes that *might* be traditional, but again, this is based on inference and subjectivity, with one review even questioning the \"traditional\" nature of a dish.  The third QA pair highlights the lack of comparative data to determine the uniqueness of the restaurant's blend compared to others.  Therefore, while some dishes suggest a potential blend, the evidence is weak and lacks definitive proof of a unique blend or the extent of the modern vs. traditional balance.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Based on customer reviews, Mala Town offers dishes that could be considered modern interpretations of Chinese cuisine.  These include mala soup [3, 4], mala tang [5], dry pot (with a peanut-based sauce) [5], and jian bing (a Chinese crepe) [5].  While not explicitly described as 'modern' on the menu, the reviews suggest these dishes might represent contemporary adaptations of traditional Chinese culinary styles.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Suggests some dishes might be modern interpretations, but relies on inference from reviews, not explicit labeling.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided reviews mention several dishes served at the restaurant, including mala soup, mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles [1, 2, 6]. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Identifies dishes that *might* be traditional, but acknowledges the subjectivity and lack of explicit confirmation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How does the restaurant's menu differ from other Chinese restaurants in the area in terms of its blend of modern and traditional dishes?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but the provided text does not contain information comparing Mala Town's menu to other Chinese restaurants in the area regarding its blend of modern and traditional dishes.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Highlights the absence of crucial comparative data to assess the claim's uniqueness aspect.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] ingredient sourcing\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] supplier relationships\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] ingredient sourcing', '[Restaurant Name] supplier relationships']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided reviews do not contain information about the restaurant's sourcing policy or its prioritization of fresh, high-quality ingredients. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific ingredients and dishes (e.g., broccoli, Chinese cabbage, tofu rolls, mung bean soup), they don't explicitly detail the restaurant's sourcing policy or its prioritization of fresh, high-quality products.  Therefore, a direct answer to the question cannot be synthesized from the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] reviews ingredients\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[Restaurant Name] Yelp reviews freshness\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['[Restaurant Name] reviews ingredients', '[Restaurant Name] Yelp reviews freshness']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient quality. One review specifically highlights the freshness of the broccoli used in a mala soup, describing it as \"fresh and perfectly cooked\" [2]. Another review mentions \"perfectly cooked mung beans\" in a complimentary mung bean soup [3], suggesting good quality ingredients. However, other reviews express less positive opinions about the overall food quality and value, without directly commenting on ingredient freshness [1, 5]. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review specifically praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not explicitly mentioning freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews offer less positive feedback on the overall quality and value of the food, without directly addressing ingredient freshness [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs offer mixed evidence regarding the claim. While some customer reviews mention the freshness of specific ingredients like broccoli and the quality implied by the \"perfectly cooked\" mung beans, other reviews express less positive opinions about the overall food quality without directly addressing ingredient freshness.  The absence of information about the restaurant's sourcing policy further weakens the support for the claim.  The positive reviews are limited in scope and don't provide a comprehensive picture of the restaurant's ingredient quality.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided reviews do not contain information about the restaurant's sourcing policy or its prioritization of fresh, high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This question highlights the lack of direct evidence supporting the claim regarding the restaurant's sourcing practices and commitment to fresh, high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer reviews offer mixed feedback on ingredient quality. Some reviews mention freshness and good quality of specific ingredients (broccoli, mung beans), while others express less positive opinions about the overall food quality without directly commenting on ingredient freshness.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: While some positive mentions exist, the mixed and limited feedback from customer reviews does not strongly support the claim. The absence of consistent positive feedback regarding ingredient quality weakens the support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks sufficient evidence to support it, as the provided reviews don't explicitly categorize dishes as modern or traditional.  Similarly, the claim about using fresh and high-quality ingredients is unsupported because while some reviews mention the freshness of specific ingredients, this is not consistent across all reviews and lacks information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.3\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that the restaurant offers a unique blend of modern and traditional Chinese cuisine.  The provided QA pairs offer some evidence, but it's inconclusive.  The first QA pair identifies dishes that *might* be considered modern interpretations, based on customer reviews and inference.  However, the reviews don't explicitly label them as such. The second QA pair similarly identifies dishes that *might* be traditional, but again, this is based on inference and subjectivity, with one review even questioning the \"traditional\" nature of a dish.  The third QA pair highlights the lack of comparative data to determine the uniqueness of the restaurant's blend compared to others.  Therefore, while some dishes suggest a potential blend, the evidence is weak and lacks definitive proof of a unique blend or the extent of the modern vs. traditional balance.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific dishes on the restaurant's menu represent modern interpretations of Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mBased on customer reviews, Mala Town offers dishes that could be considered modern interpretations of Chinese cuisine.  These include mala soup [3, 4], mala tang [5], dry pot (with a peanut-based sauce) [5], and jian bing (a Chinese crepe) [5].  While not explicitly described as 'modern' on the menu, the reviews suggest these dishes might represent contemporary adaptations of traditional Chinese culinary styles. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly label any as \"modern interpretations of Chinese cuisine\".  However, the reviews suggest some dishes might be considered modern takes on traditional Chinese food.  The reviews mention \"mala soup\", \"mala tang\", \"dry pot (peanut-based sauce)\", and \"jian bing (Chinese crepe)\". While these dishes have roots in traditional Chinese cuisine, their preparation and presentation at Mala Town might represent a modern adaptation.  The lack of explicit labeling requires inference based on the context of the reviews.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mI got a big bowl of tasty mala soup (at just the right spice level after I asked for it \"da la\" 大麻) with four ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe soup came topped with sliced spring onions and a generous dollop of chili sauce with sesame seeds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific dishes on the restaurant's menu represent traditional Chinese cuisine?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided reviews mention several dishes served at the restaurant, including mala soup, mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles [1, 2, 6]. However, whether these dishes represent traditional Chinese cuisine is not explicitly stated and may be subjective. One reviewer, familiar with Hong Kong and Asian cuisine, described a dish as delicious but not necessarily \"traditional\" [2, 5].  More information is needed to definitively answer which dishes on the menu represent traditional Chinese cuisine. \n",
      "\n",
      "Reasoning: The provided text mentions several dishes but doesn't explicitly state which are considered \"traditional\" Chinese cuisine.  The reviews mention mala soup,  mala tang, dry pot (peanut-based sauce), jian bing (Chinese crepe), and spicy soup noodles. However, whether these are considered \"traditional\" is subjective and not explicitly stated in the source. One review mentions the author's experience in Hong Kong and Asia, stating that while the mala soup was delicious, it wasn't necessarily \"traditional.\"  Therefore, a definitive answer about which dishes are traditionally Chinese cannot be provided based on the given text.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mWe tried both the mala tang and the dry pot (peanut-based sauce).\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI got a big bowl of tasty mala soup (at just the right spice level after I asked for it \"da la\" 大麻) with four ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThey also have jian bing and some appetizers, which we didn't try--some of their other items may be better.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mWhile I can't quite call what I ate \"traditional\" in some respects, it was delicious.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 3 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow does the restaurant's menu differ from other Chinese restaurants in the area in terms of its blend of modern and traditional dishes?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but the provided text does not contain information comparing Mala Town's menu to other Chinese restaurants in the area regarding its blend of modern and traditional dishes.  The reviews describe specific dishes served at Mala Town but lack comparative data. \n",
      "\n",
      "Reasoning: The provided text focuses on customer reviews of Mala Town, a Chinese restaurant.  The reviews mention specific dishes like mala soup, dry pot, and jian bing, but don't offer comparisons to other Chinese restaurants in the area.  Therefore, a direct comparison of Mala Town's menu's blend of modern and traditional dishes to competitors cannot be made based on this data.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe restaurant uses fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs offer mixed evidence regarding the claim. While some customer reviews mention the freshness of specific ingredients like broccoli and the quality implied by the \"perfectly cooked\" mung beans, other reviews express less positive opinions about the overall food quality without directly addressing ingredient freshness.  The absence of information about the restaurant's sourcing policy further weakens the support for the claim.  The positive reviews are limited in scope and don't provide a comprehensive picture of the restaurant's ingredient quality.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the restaurant's sourcing policy for its ingredients, and does it prioritize fresh and high-quality products?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided reviews do not contain information about the restaurant's sourcing policy or its prioritization of fresh, high-quality ingredients. \n",
      "\n",
      "Reasoning: The provided text consists of customer reviews of Mala Town restaurant.  While the reviews mention specific ingredients and dishes (e.g., broccoli, Chinese cabbage, tofu rolls, mung bean soup), they don't explicitly detail the restaurant's sourcing policy or its prioritization of fresh, high-quality products.  Therefore, a direct answer to the question cannot be synthesized from the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat do customer reviews say about the freshness and quality of the ingredients used at the restaurant?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer reviews offer mixed feedback on ingredient quality. One review specifically highlights the freshness of the broccoli used in a mala soup, describing it as \"fresh and perfectly cooked\" [2]. Another review mentions \"perfectly cooked mung beans\" in a complimentary mung bean soup [3], suggesting good quality ingredients. However, other reviews express less positive opinions about the overall food quality and value, without directly commenting on ingredient freshness [1, 5]. \n",
      "\n",
      "Reasoning: Several reviews mention the freshness and quality of specific ingredients. One review specifically praises the freshness of the broccoli in a mala soup dish [2].  Another review, while not explicitly mentioning freshness, describes the mung beans in a complimentary mung bean soup as \"perfectly cooked\" [3], implying high-quality ingredients and preparation.  However, other reviews offer less positive feedback on the overall quality and value of the food, without directly addressing ingredient freshness [1, 5].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe food here is ok but not great, and kind of expensive for what it is.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mThe broccoli was fresh and perfectly cooked, and the cabbage soaked up the delicious mala oils while the vermicelli provided the \"umph\" to the dish.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mIt was a nice balance of sweet to match the spicy soup, and had a liberal serving of perfectly cooked mung beans at the bottom.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mI believe the restaurant will not last long, even their food is not bad, because the price is much higher than expected.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks sufficient evidence to support it, as the provided reviews don't explicitly categorize dishes as modern or traditional.  Similarly, the claim about using fresh and high-quality ingredients is unsupported because while some reviews mention the freshness of specific ingredients, this is not consistent across all reviews and lacks information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe restaurant offers a unique blend of modern and traditional Chinese cuisine, using fresh and high-quality ingredients.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims within the statement are unsupported. The claim about the unique blend of modern and traditional Chinese cuisine lacks sufficient evidence to support it, as the provided reviews don't explicitly categorize dishes as modern or traditional.  Similarly, the claim about using fresh and high-quality ingredients is unsupported because while some reviews mention the freshness of specific ingredients, this is not consistent across all reviews and lacks information about the restaurant's sourcing policies.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 225 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc0d087207b4972836c43bae78e674c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the online reputation and customer feedback for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara CA location\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara address\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the online reputation and customer feedback for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara Yelp\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the online reputation and customer feedback for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer reviews for Winchell's Donuts in Santa Barbara are mixed.  Some customers reported that the donuts were stale and dry [1, 3], with one even noting flies on the donuts [2].  One review mentioned the store wasn't very clean [1]. However, the customer service was generally considered friendly [1, 3]. The overall business rating is 3.0 stars. \n",
      "\n",
      "Reasoning: The provided text contains customer reviews of Winchell's Donuts in Santa Barbara, CA.  Several reviews mention issues with the freshness and dryness of the donuts [1, 3], and one review reports seeing flies on the donuts [2]. Another review notes that the store wasn't the cleanest [1]. However, there are also mentions of friendly customer service [1, 3].  The overall business star rating is 3.0 out of 5 [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!??\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe donuts were okay and the store was not the cleanest.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mI will say the girl who helped me was very friendly and was wearing a mask and washed her hands. So at least there's that!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mThe overall business rating is 3.0 stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Winchell's Donuts is a popular donut shop in Santa Barbara, CA.  The provided Q&A pairs confirm the existence of a Winchell's Donuts location in Santa Barbara at 202 N Milpas. However, the customer reviews are mixed. While the location is confirmed, the reviews indicate inconsistent quality (stale donuts, cleanliness issues), suggesting the \"popular\" aspect of the claim is unsupported.  The 3.0-star rating further supports this assessment.  The claim is partially supported by the existence of the shop but not by its popularity.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Supports the existence of the shop in Santa Barbara, but doesn't address popularity.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the online reputation and customer feedback for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer reviews are mixed, with reports of stale donuts, cleanliness issues, and a 3.0-star rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Indicates the shop is not necessarily 'popular' due to negative reviews and a low average star rating.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Winchell's Donuts is a popular donut shop in Santa Barbara, CA.  The provided Q&A pairs confirm the existence of a Winchell's Donuts location in Santa Barbara at 202 N Milpas. However, the customer reviews are mixed. While the location is confirmed, the reviews indicate inconsistent quality (stale donuts, cleanliness issues), suggesting the \"popular\" aspect of the claim is unsupported.  The 3.0-star rating further supports this assessment.  The claim is partially supported by the existence of the shop but not by its popularity.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the online reputation and customer feedback for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer reviews for Winchell's Donuts in Santa Barbara are mixed.  Some customers reported that the donuts were stale and dry [1, 3], with one even noting flies on the donuts [2].  One review mentioned the store wasn't very clean [1]. However, the customer service was generally considered friendly [1, 3]. The overall business rating is 3.0 stars. \n",
      "\n",
      "Reasoning: The provided text contains customer reviews of Winchell's Donuts in Santa Barbara, CA.  Several reviews mention issues with the freshness and dryness of the donuts [1, 3], and one review reports seeing flies on the donuts [2]. Another review notes that the store wasn't the cleanest [1]. However, there are also mentions of friendly customer service [1, 3].  The overall business star rating is 3.0 out of 5 [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!??\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe donuts were okay and the store was not the cleanest.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mI will say the girl who helped me was very friendly and was wearing a mask and washed her hands. So at least there's that!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mThe overall business rating is 3.0 stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Winchell's Donuts is a popular donut shop in Santa Barbara, CA.  The provided Q&A pairs confirm the existence of a Winchell's Donuts location in Santa Barbara at 202 N Milpas. However, the customer reviews are mixed. While the location is confirmed, the reviews indicate inconsistent quality (stale donuts, cleanliness issues), suggesting the \"popular\" aspect of the claim is unsupported.  The 3.0-star rating further supports this assessment.  The claim is partially supported by the existence of the shop but not by its popularity.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that Winchell's Donuts is a popular donut shop in Santa Barbara, CA.  The provided Q&A pairs confirm the existence of a Winchell's Donuts location in Santa Barbara at 202 N Milpas. However, the customer reviews are mixed. While the location is confirmed, the reviews indicate inconsistent quality (stale donuts, cleanliness issues), suggesting the \"popular\" aspect of the claim is unsupported.  The 3.0-star rating further supports this assessment.  The claim is partially supported by the existence of the shop but not by its popularity.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the customer rating and review sentiment for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews Yelp\", \"Winchell's Donuts Santa Barbara Google reviews\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara CA location\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara address\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer rating and review sentiment for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews Yelp\", \"Winchell's Donuts Santa Barbara Google reviews\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara reviews Yelp\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara Google reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer rating and review sentiment for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews Yelp\", \"Winchell's Donuts Santa Barbara Google reviews\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mWinchell's Donuts in Santa Barbara, CA, has an overall rating of 3.0 stars. Customer reviews are mixed. Some customers reported negative experiences, such as stale and dry donuts [1, 3], and one review mentioned seeing flies on the donuts [2].  However, other reviews praised the friendly customer service [1, 3]. \n",
      "\n",
      "Reasoning: The provided text contains customer reviews and ratings for Winchell's Donuts in Santa Barbara, CA.  The overall business rating is 3.0 stars. Individual reviews show a range of experiences. Some customers reported stale donuts [1, 3], while others noted cleanliness issues [1] and even flies on the donuts [2].  Positive aspects mentioned include friendly customer service [1, 3] and the availability of Wi-Fi [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite. ... The maple  buttermilk bar weighed a ton and again was so dry.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!???\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe donuts were okay and the store was not the cleanest. The customer service was good nothing special but it wasn't bad customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pairs confirm the existence of a Winchell's Donuts location in Santa Barbara, CA, at 202 N Milpas.  While the customer reviews are mixed, with some negative comments about the quality and cleanliness, the positive reviews and the confirmation of the location's existence support the claim that Winchell's Donuts is a donut shop located in Santa Barbara, CA.  The mixed reviews do not negate the claim's core assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim by confirming the existence of a Winchell's Donuts in Santa Barbara.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the customer rating and review sentiment for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Winchell's Donuts in Santa Barbara, CA, has an overall rating of 3.0 stars. Customer reviews are mixed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: While reviews are mixed, the existence of the shop is not disputed. The mixed reviews do not invalidate the claim's core assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pairs confirm the existence of a Winchell's Donuts location in Santa Barbara, CA, at 202 N Milpas.  While the customer reviews are mixed, with some negative comments about the quality and cleanliness, the positive reviews and the confirmation of the location's existence support the claim that Winchell's Donuts is a donut shop located in Santa Barbara, CA.  The mixed reviews do not negate the claim's core assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the customer rating and review sentiment for Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mWinchell's Donuts in Santa Barbara, CA, has an overall rating of 3.0 stars. Customer reviews are mixed. Some customers reported negative experiences, such as stale and dry donuts [1, 3], and one review mentioned seeing flies on the donuts [2].  However, other reviews praised the friendly customer service [1, 3]. \n",
      "\n",
      "Reasoning: The provided text contains customer reviews and ratings for Winchell's Donuts in Santa Barbara, CA.  The overall business rating is 3.0 stars. Individual reviews show a range of experiences. Some customers reported stale donuts [1, 3], while others noted cleanliness issues [1] and even flies on the donuts [2].  Positive aspects mentioned include friendly customer service [1, 3] and the availability of Wi-Fi [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite. ... The maple  buttermilk bar weighed a ton and again was so dry.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!???\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe donuts were okay and the store was not the cleanest. The customer service was good nothing special but it wasn't bad customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs confirm the existence of a Winchell's Donuts location in Santa Barbara, CA, at 202 N Milpas.  While the customer reviews are mixed, with some negative comments about the quality and cleanliness, the positive reviews and the confirmation of the location's existence support the claim that Winchell's Donuts is a donut shop located in Santa Barbara, CA.  The mixed reviews do not negate the claim's core assertion.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided question-answer pairs confirm the existence of a Winchell's Donuts location in Santa Barbara, CA, at 202 N Milpas.  While the customer reviews are mixed, with some negative comments about the quality and cleanliness, the positive reviews and the confirmation of the location's existence support the claim that Winchell's Donuts is a donut shop located in Santa Barbara, CA.  The mixed reviews do not negate the claim's core assertion.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mHow popular is the Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara CA location\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara address\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara CA location\", \"Winchell's Donuts Santa Barbara address\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow popular is the Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara reviews\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWinchell's Donuts Santa Barbara Yelp\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow popular is the Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m[\"Winchell's Donuts Santa Barbara reviews\", \"Winchell's Donuts Santa Barbara Yelp\"]\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer reviews of Winchell's Donuts in Santa Barbara, CA, are mixed.  The average rating is 3.0 stars. Some customers reported that the donuts were stale and dry [1, 3], and that the store was not clean [1].  One review even mentioned seeing flies on the donuts [2]. However, other reviews noted friendly service [1, 3].  There is no data available on the overall popularity of this location. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Winchell's Donuts in Santa Barbara, CA.  These reviews offer insights into customer experiences, including donut quality and cleanliness.  The overall star rating is 3.0, suggesting a mixed reception.  Some reviews praise the friendly service, while others criticize stale donuts and unsanitary conditions.  There is no information on overall popularity metrics like sales figures or customer traffic.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite. ... The maple  buttermilk bar weighed a ton and again was so dry. Usually the apple fritter can save the day... but no go, unfortunately. Of the six I bought the only one I was able to eat was the chocolate crueller.\\n\\nI will say the girl who helped me was very friendly and was wearing a mask and washed her hands. So at least there's that!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!??\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mThe donuts were okay and the store was not the cleanest. The customer service was good nothing special but it wasn't bad customer service. But overall it was a okay experience\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided QA pairs offer evidence both supporting and contradicting the claim.  The first QA pair confirms the existence of a Winchell's Donuts in Santa Barbara, CA, supporting the claim's assertion of its location. However, the second QA pair reveals mixed customer reviews, with some indicating negative experiences related to donut quality and cleanliness.  While the location is confirmed, the popularity aspect of the claim is unsupported due to the lack of quantitative data and the mixed nature of customer feedback.  The overall assessment will consider both aspects.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Is there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Directly supports the claim's assertion of the donut shop's location in Santa Barbara.  The address is provided as evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How popular is the Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer reviews of Winchell's Donuts in Santa Barbara, CA, are mixed.  The average rating is 3.0 stars.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: Partially supports the claim. While the existence is confirmed, the popularity is unclear due to mixed reviews and lack of quantitative data. The 3.0-star average suggests neither high nor low popularity.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided QA pairs offer evidence both supporting and contradicting the claim.  The first QA pair confirms the existence of a Winchell's Donuts in Santa Barbara, CA, supporting the claim's assertion of its location. However, the second QA pair reveals mixed customer reviews, with some indicating negative experiences related to donut quality and cleanliness.  While the location is confirmed, the popularity aspect of the claim is unsupported due to the lack of quantitative data and the mixed nature of customer feedback.  The overall assessment will consider both aspects.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mIs there a Winchell's Donuts location in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, there is a Winchell's Donuts in Santa Barbara, CA, located at 202 N Milpas [2, 4]. \n",
      "\n",
      "Reasoning: The provided text contains data about a Winchell's Donuts location in Santa Barbara, CA.  Document [2] and [4]  clearly state the address as 202 N Milpas, Santa Barbara, CA.  While some reviews mention issues with the quality and cleanliness of the store, the existence of the location is confirmed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33m{\\'name\\': \"Winchell's Donuts\", \\'address\\': \\'202 N Milpas\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\'\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow popular is the Winchell's Donuts in Santa Barbara, CA?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer reviews of Winchell's Donuts in Santa Barbara, CA, are mixed.  The average rating is 3.0 stars. Some customers reported that the donuts were stale and dry [1, 3], and that the store was not clean [1].  One review even mentioned seeing flies on the donuts [2]. However, other reviews noted friendly service [1, 3].  There is no data available on the overall popularity of this location. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Winchell's Donuts in Santa Barbara, CA.  These reviews offer insights into customer experiences, including donut quality and cleanliness.  The overall star rating is 3.0, suggesting a mixed reception.  Some reviews praise the friendly service, while others criticize stale donuts and unsanitary conditions.  There is no information on overall popularity metrics like sales figures or customer traffic.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mBut I just bough 6 donuts here today that were so stale and dried out. I threw.all but 1 away. The chocolate old fashioned was so dry I couldn't even get thru one bite. ... The maple  buttermilk bar weighed a ton and again was so dry. Usually the apple fritter can save the day... but no go, unfortunately. Of the six I bought the only one I was able to eat was the chocolate crueller.\\n\\nI will say the girl who helped me was very friendly and was wearing a mask and washed her hands. So at least there's that!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mI love this place I do but when I walked in and saw fly's on the donuts I was devastated. Why!!!??\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mThe donuts were okay and the store was not the cleanest. The customer service was good nothing special but it wasn't bad customer service. But overall it was a okay experience\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided QA pairs offer evidence both supporting and contradicting the claim.  The first QA pair confirms the existence of a Winchell's Donuts in Santa Barbara, CA, supporting the claim's assertion of its location. However, the second QA pair reveals mixed customer reviews, with some indicating negative experiences related to donut quality and cleanliness.  While the location is confirmed, the popularity aspect of the claim is unsupported due to the lack of quantitative data and the mixed nature of customer feedback.  The overall assessment will consider both aspects.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWinchell's Donuts is a popular donut shop located in Santa Barbara, CA.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe provided QA pairs offer evidence both supporting and contradicting the claim.  The first QA pair confirms the existence of a Winchell's Donuts in Santa Barbara, CA, supporting the claim's assertion of its location. However, the second QA pair reveals mixed customer reviews, with some indicating negative experiences related to donut quality and cleanliness.  While the location is confirmed, the popularity aspect of the claim is unsupported due to the lack of quantitative data and the mixed nature of customer feedback.  The overall assessment will consider both aspects.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 226 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9b970a319b4138ba0096a2cb148300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['seasoning blend recipes', 'Worcestershire sauce in seasoning blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcommon dry seasoning blend ingredients\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mtypes of spice blends\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCommon components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. \n",
      "\n",
      "Reasoning: Based on the provided text, a common dry seasoning blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].  Note that this is based on a single example recipe and may not represent all commercially available blends.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['seasoning blend recipes', 'Worcestershire sauce in seasoning blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mseasoning blend recipes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWorcestershire sauce in seasoning blends\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['seasoning blend recipes', 'Worcestershire sauce in seasoning blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. While it doesn't explicitly state this combination is *commonly* associated with a particular flavor profile, its use in a recipe suggests it creates a flavor profile suitable for seasoning. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops that uses a spice mixture containing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring.  While the text doesn't explicitly state that this combination creates a *commonly* associated flavor profile, the fact that it's used as a seasoning in a recipe suggests it produces a flavor profile suitable for that purpose.  The recipe's existence implies that this combination of ingredients is at least a *possible* and used flavor profile for seasoning.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs support this. The first Q&A shows that these ingredients are common components in commercially available dry seasoning blends. The second Q&A indicates that the combination of these ingredients, while not explicitly stated as a *common* flavor profile, is used in a recipe as a seasoning, suggesting it produces a suitable flavor profile.  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Common components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer directly supports the claim by showing that the listed ingredients are commonly found in commercial seasoning blends.  This lends credence to the idea that combining them would produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. While it doesn't explicitly state this combination is *commonly* associated with a particular flavor profile, its use in a recipe suggests it creates a flavor profile suitable for seasoning.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer provides further support. Although it doesn't confirm the blend's commonality, it shows the ingredient combination is used in a recipe for seasoning, implying it creates a functional seasoning blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs support this. The first Q&A shows that these ingredients are common components in commercially available dry seasoning blends. The second Q&A indicates that the combination of these ingredients, while not explicitly stated as a *common* flavor profile, is used in a recipe as a seasoning, suggesting it produces a suitable flavor profile.  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCommon components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. \n",
      "\n",
      "Reasoning: Based on the provided text, a common dry seasoning blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].  Note that this is based on a single example recipe and may not represent all commercially available blends.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. While it doesn't explicitly state this combination is *commonly* associated with a particular flavor profile, its use in a recipe suggests it creates a flavor profile suitable for seasoning. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops that uses a spice mixture containing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring.  While the text doesn't explicitly state that this combination creates a *commonly* associated flavor profile, the fact that it's used as a seasoning in a recipe suggests it produces a flavor profile suitable for that purpose.  The recipe's existence implies that this combination of ingredients is at least a *possible* and used flavor profile for seasoning.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs support this. The first Q&A shows that these ingredients are common components in commercially available dry seasoning blends. The second Q&A indicates that the combination of these ingredients, while not explicitly stated as a *common* flavor profile, is used in a recipe as a seasoning, suggesting it produces a suitable flavor profile.  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs support this. The first Q&A shows that these ingredients are common components in commercially available dry seasoning blends. The second Q&A indicates that the combination of these ingredients, while not explicitly stated as a *common* flavor profile, is used in a recipe as a seasoning, suggesting it produces a suitable flavor profile.  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['seasoning blend recipe analysis', 'flavor profile of paprika, garlic powder, onion powder, Worcestershire sauce, smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcommon dry seasoning blend ingredients\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mtypes of spice blends\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['common dry seasoning blend ingredients', 'types of spice blends']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCommon components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. \n",
      "\n",
      "Reasoning: Based on the provided text, a common dry seasoning blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].  Note that this is based on a single example recipe and may not represent all commercially available blends.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['seasoning blend recipe analysis', 'flavor profile of paprika, garlic powder, onion powder, Worcestershire sauce, smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mseasoning blend recipe analysis\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mflavor profile of paprika, garlic powder, onion powder, Worcestershire sauce, smoke flavoring\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['seasoning blend recipe analysis', 'flavor profile of paprika, garlic powder, onion powder, Worcestershire sauce, smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring for pork chops [1, 2]. While this suggests the combination creates a flavor profile suitable for seasoning, the text doesn't state whether this is a commonly associated flavor profile. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops.  The recipe uses a spice mixture containing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring. While the text doesn't explicitly state that this combination creates a *commonly* associated flavor profile, the fact that it's used as a seasoning in a recipe suggests it produces a flavor profile suitable for that purpose.  The lack of other examples prevents a definitive answer about common association.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mRinse pork chops, and sprinkle the wet chops on both sides with the spice mixture.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs offer some support. The first Q&A shows that the listed ingredients are common components in commercially available dry seasoning blends.  The second Q&A shows that the combination of ingredients is used in a recipe as a seasoning, suggesting it creates a suitable flavor profile. However, neither Q&A definitively confirms that this specific combination produces a *commonly* used or recognized seasoned blend.  The second answer highlights the lack of broader evidence to support the claim's assertion of common association.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Common components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This supports the claim by showing that the ingredients listed are commonly found in commercial blends.  However, it doesn't guarantee that this *specific* combination is a standard blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Does combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring for pork chops [1, 2]. While this suggests the combination creates a flavor profile suitable for seasoning, the text doesn't state whether this is a commonly associated flavor profile.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This partially supports the claim by showing the ingredient combination is used as a seasoning in a recipe. However, the lack of evidence regarding common usage weakens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mMixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring in a bowl until thoroughly combined will produce a seasoned blend.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs offer some support. The first Q&A shows that the listed ingredients are common components in commercially available dry seasoning blends.  The second Q&A shows that the combination of ingredients is used in a recipe as a seasoning, suggesting it creates a suitable flavor profile. However, neither Q&A definitively confirms that this specific combination produces a *commonly* used or recognized seasoned blend.  The second answer highlights the lack of broader evidence to support the claim's assertion of common association.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat are the common components of commercially available dry seasoning blends?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCommon components of commercially available dry seasoning blends can include seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2]. \n",
      "\n",
      "Reasoning: Based on the provided text, a common dry seasoning blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1, 2].  Note that this is based on a single example recipe and may not represent all commercially available blends.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDoes combining seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring create a flavor profile commonly associated with seasoning?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided text describes a recipe using a spice mixture of seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring for pork chops [1, 2]. While this suggests the combination creates a flavor profile suitable for seasoning, the text doesn't state whether this is a commonly associated flavor profile. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops.  The recipe uses a spice mixture containing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring. While the text doesn't explicitly state that this combination creates a *commonly* associated flavor profile, the fact that it's used as a seasoning in a recipe suggests it produces a flavor profile suitable for that purpose.  The lack of other examples prevents a definitive answer about common association.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mRinse pork chops, and sprinkle the wet chops on both sides with the spice mixture.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs offer some support. The first Q&A shows that the listed ingredients are common components in commercially available dry seasoning blends.  The second Q&A shows that the combination of ingredients is used in a recipe as a seasoning, suggesting it creates a suitable flavor profile. However, neither Q&A definitively confirms that this specific combination produces a *commonly* used or recognized seasoned blend.  The second answer highlights the lack of broader evidence to support the claim's assertion of common association.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim states that mixing specific ingredients will produce a seasoned blend.  The provided Q&A pairs offer some support. The first Q&A shows that the listed ingredients are common components in commercially available dry seasoning blends.  The second Q&A shows that the combination of ingredients is used in a recipe as a seasoning, suggesting it creates a suitable flavor profile. However, neither Q&A definitively confirms that this specific combination produces a *commonly* used or recognized seasoned blend.  The second answer highlights the lack of broader evidence to support the claim's assertion of common association.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mTo prepare a spice blend, seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring should be mixed together in a bowl until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat ingredients are listed in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['recipe ingredients spice blend seasoned salt black pepper garlic powder onion powder paprika Worcestershire sauce smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat is the instruction for combining the ingredients in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['recipe instructions spice blend mix thoroughly']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat ingredients are listed in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['recipe ingredients spice blend seasoned salt black pepper garlic powder onion powder paprika Worcestershire sauce smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrecipe ingredients spice blend seasoned salt black pepper garlic powder onion powder paprika Worcestershire sauce smoke flavoring\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat ingredients are listed in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['recipe ingredients spice blend seasoned salt black pepper garlic powder onion powder paprika Worcestershire sauce smoke flavoring']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe spice blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1]. \n",
      "\n",
      "Reasoning: The provided text describes a spice blend used in a pork chop recipe.  The recipe instructions detail mixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring.  I will use this information to construct the answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the instruction for combining the ingredients in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['recipe instructions spice blend mix thoroughly']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrecipe instructions spice blend mix thoroughly\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the instruction for combining the ingredients in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['recipe instructions spice blend mix thoroughly']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mMix the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring together until thoroughly combined [1]. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops.  A spice blend is made by mixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.  The instruction for combining the ingredients is to mix them together until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mTo prepare a spice blend, seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring should be mixed together in a bowl until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim directly matches the instructions provided in the question-answer pairs.  Both QA pairs explicitly state the ingredients (seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring) and the instruction to mix them until thoroughly combined.  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What ingredients are listed in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The spice blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer lists all the ingredients mentioned in the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the instruction for combining the ingredients in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Mix the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring together until thoroughly combined [1].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer confirms the instruction to mix the ingredients until thoroughly combined, as stated in the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mTo prepare a spice blend, seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring should be mixed together in a bowl until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim directly matches the instructions provided in the question-answer pairs.  Both QA pairs explicitly state the ingredients (seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring) and the instruction to mix them until thoroughly combined.  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat ingredients are listed in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe spice blend includes seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring [1]. \n",
      "\n",
      "Reasoning: The provided text describes a spice blend used in a pork chop recipe.  The recipe instructions detail mixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring.  I will use this information to construct the answer.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the instruction for combining the ingredients in the recipe for the spice blend?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mMix the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring together until thoroughly combined [1]. \n",
      "\n",
      "Reasoning: The provided text describes a recipe for grilling pork chops.  A spice blend is made by mixing seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.  The instruction for combining the ingredients is to mix them together until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mIn a bowl, mix together the seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim directly matches the instructions provided in the question-answer pairs.  Both QA pairs explicitly state the ingredients (seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring) and the instruction to mix them until thoroughly combined.  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe claim directly matches the instructions provided in the question-answer pairs.  Both QA pairs explicitly state the ingredients (seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring) and the instruction to mix them until thoroughly combined.  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 227 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e44a688bf5a419f9401eedce9138f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin basketball game results 71-64\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Wisconsin won a basketball game 71-64.  The provided QA pair confirms this, specifying that the opponent was Kentucky and the game was a Final Four match in the NCAA men's basketball tournament.  The evidence directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, providing the opponent (Kentucky) and context (NCAA Final Four).\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"Frank Kaminsky\" Wisconsin basketball box score\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] contains the answer.  It states that Frank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly states that Frank Kaminsky scored 20 points in a Wisconsin Badgers basketball game against the Kentucky Wildcats in the NCAA Final Four.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] contains the answer.  It states that Frank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms that Frank Kaminsky scored 20 points in a Wisconsin basketball game.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin basketball box score [Date of game]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the NCAA men's basketball Final Four.  No other document provides rebound statistics for Kaminsky in a specific game against a named opponent.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's Final Four game against Kentucky, citing a source document ([1]).  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the NCAA men's basketball Final Four.  No other document provides rebound statistics for Kaminsky in a specific game against a named opponent.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, stating that Kaminsky had 11 rebounds in a Wisconsin game (specifically against Kentucky).\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds, all of which are corroborated by the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Wisconsin won a basketball game 71-64.  The provided QA pair confirms this, specifying that the opponent was Kentucky and the game was a Final Four match in the NCAA men's basketball tournament.  The evidence directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly states that Frank Kaminsky scored 20 points in a Wisconsin Badgers basketball game against the Kentucky Wildcats in the NCAA Final Four.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] contains the answer.  It states that Frank Kaminsky scored 20 points in the Wisconsin Badgers' victory over the Kentucky Wildcats in the NCAA men's basketball Final Four.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's Final Four game against Kentucky, citing a source document ([1]).  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the NCAA men's basketball Final Four.  No other document provides rebound statistics for Kaminsky in a specific game against a named opponent.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds, all of which are corroborated by the provided evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds, all of which are corroborated by the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin basketball game results 71-64\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Wisconsin won a basketball game 71-64.  The provided QA pair confirms this, specifying that the opponent was Kentucky and the game was a Final Four match in the NCAA men's basketball tournament.  The evidence directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, providing the opponent (Kentucky) and context (NCAA Final Four).\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball points']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball points']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m\"Frank Kaminsky\" Wisconsin basketball points\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['\"Frank Kaminsky\" Wisconsin basketball points']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky scored 20 points in the Wisconsin basketball game mentioned [2]. \n",
      "\n",
      "Reasoning: Document [2] contains the answer.  It describes a Wisconsin basketball game where Frank Kaminsky scored 20 points.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 555, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky scored 20 points in a Wisconsin basketball game, citing document [2] as the source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky scored 20 points in the Wisconsin basketball game mentioned [2]. \n",
      "\n",
      "Reasoning: Document [2] contains the answer.  It describes a Wisconsin basketball game where Frank Kaminsky scored 20 points.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms the claim, stating that Frank Kaminsky scored 20 points in a Wisconsin basketball game. The citation to document [2] further strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent's Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent's Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin basketball box score [Date of game]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent's Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin basketball box score [Date of game]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the Final Four.  No other document provides rebound statistics for Kaminsky in a specific game.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's Final Four game against Kentucky, citing a source document ([1]).  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent's Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the Final Four.  No other document provides rebound statistics for Kaminsky in a specific game.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, stating that Kaminsky had 11 rebounds in a Wisconsin game (specifically against Kentucky in the Final Four).\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds.  Since all supporting claims are individually supported, the overall statement is also supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Wisconsin won a basketball game 71-64.  The provided QA pair confirms this, specifying that the opponent was Kentucky and the game was a Final Four match in the NCAA men's basketball tournament.  The evidence directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid Wisconsin win a basketball game with a final score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, Wisconsin won a basketball game with a final score of 71-64 against Kentucky in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Wisconsin won a basketball game against Kentucky with a score of 71-64.  This game was a Final Four match in the NCAA men's basketball tournament.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky scored 20 points in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky scored 20 points in a Wisconsin basketball game, citing document [2] as the source.  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin basketball game mentioned?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky scored 20 points in the Wisconsin basketball game mentioned [2]. \n",
      "\n",
      "Reasoning: Document [2] contains the answer.  It describes a Wisconsin basketball game where Frank Kaminsky scored 20 points.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky had 11 rebounds in the Wisconsin basketball game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly supports the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's Final Four game against Kentucky, citing a source document ([1]).  There is no contradictory information.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many rebounds did Frank Kaminsky have in the Wisconsin basketball game against [Opponent's Name, if known]?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's Final Four victory against Kentucky. [1] \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's victory against Kentucky in the Final Four.  No other document provides rebound statistics for Kaminsky in a specific game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds.  Since all supporting claims are individually supported, the overall statement is also supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The claims cover the team's victory, Kaminsky's points, and Kaminsky's rebounds.  Since all supporting claims are individually supported, the overall statement is also supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (3):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mFrank Kaminsky scored 20 points in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     3. \u001b[97mFrank Kaminsky had 11 rebounds in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mDid the Wisconsin Badgers men's basketball team win a game with a score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin Badgers basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the Wisconsin Badgers men's basketball team win a game with a score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin Badgers basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin Badgers basketball game results 71-64\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mDid the Wisconsin Badgers men's basketball team win a game with a score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin Badgers basketball game results 71-64']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mYes, the Wisconsin Badgers men's basketball team won a game with a score of 71-64 against the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that the Wisconsin Badgers won a game against the Kentucky Wildcats with a score of 71-64 in the NCAA men's basketball Final Four.  This directly answers the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly confirms that Wisconsin (specifically, the Wisconsin Badgers men's basketball team) won a game with a score of 71-64.  The answer explicitly states this and cites a source ([1]) supporting the claim.  The claim doesn't specify the opponent, and the answer provides that information (Kentucky Wildcats).  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: Did the Wisconsin Badgers men's basketball team win a game with a score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Yes, the Wisconsin Badgers men's basketball team won a game with a score of 71-64 against the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that the Wisconsin Badgers won a game against the Kentucky Wildcats with a score of 71-64 in the NCAA men's basketball Final Four.  This directly answers the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, specifying the winning team (Wisconsin) and the score (71-64).  The opponent is also identified, which is not required by the claim but adds context.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin game?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Frank Kaminsky Wisconsin game stats', 'Wisconsin game box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin game?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Frank Kaminsky Wisconsin game stats', 'Wisconsin game box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mFrank Kaminsky Wisconsin game stats\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin game box score\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin game?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Frank Kaminsky Wisconsin game stats', 'Wisconsin game box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky scored 20 points in the game against Wisconsin [1, 6]. \n",
      "\n",
      "Reasoning: The provided text mentions Frank Kaminsky's performance in a game against Kentucky.  Document [1] and [6] state that he scored 20 points in that game.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky scored 20 points in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe claim states that Frank Kaminsky scored 20 points in the game.  The provided QA pair directly supports this claim. The answer explicitly states that Kaminsky scored 20 points in a game against Wisconsin, citing two sources.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many points did Frank Kaminsky score in the Wisconsin game?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky scored 20 points in the game against Wisconsin [1, 6]. \n",
      "\n",
      "Reasoning: The provided text mentions Frank Kaminsky's performance in a game against Kentucky.  Document [1] and [6] state that he scored 20 points in that game.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, stating that Kaminsky scored 20 points in a game.  The citation to two sources strengthens the support.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [3/3] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mHow many rebounds did Frank Kaminsky have in Wisconsin's 71-64 victory?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Wisconsin vs [Opponent Name] basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in Wisconsin's 71-64 victory?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin vs [Opponent Name] basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/1] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mWisconsin vs [Opponent Name] basketball box score\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mHow many rebounds did Frank Kaminsky have in Wisconsin's 71-64 victory?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Wisconsin vs [Opponent Name] basketball box score']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory [1]. \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory.  This is the only document that provides the specific rebound statistic for Kaminsky in the game described in the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mFrank Kaminsky had 11 rebounds in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory, citing a source ([1]).  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: How many rebounds did Frank Kaminsky have in Wisconsin's 71-64 victory?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory [1]. \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory.  This is the only document that provides the specific rebound statistic for Kaminsky in the game described in the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly confirms the claim, stating that Frank Kaminsky had 11 rebounds in the specified game. The citation supports the accuracy of the information.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The statement asserts a Wisconsin victory with a specific score, and details Frank Kaminsky's point and rebound contributions.  Each of these aspects is confirmed by the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mWisconsin won a basketball game against an unspecified opponent with a score of 71-64.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly confirms that Wisconsin (specifically, the Wisconsin Badgers men's basketball team) won a game with a score of 71-64.  The answer explicitly states this and cites a source ([1]) supporting the claim.  The claim doesn't specify the opponent, and the answer provides that information (Kentucky Wildcats).  Therefore, the claim is supported by the evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mDid the Wisconsin Badgers men's basketball team win a game with a score of 71-64?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mYes, the Wisconsin Badgers men's basketball team won a game with a score of 71-64 against the Kentucky Wildcats in the NCAA men's basketball Final Four. [1] \n",
      "\n",
      "Reasoning: Document [1] states that the Wisconsin Badgers won a game against the Kentucky Wildcats with a score of 71-64 in the NCAA men's basketball Final Four.  This directly answers the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe Kentucky Wildcats' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men's basketball Final Four at Indianapolis.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky scored 20 points in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe claim states that Frank Kaminsky scored 20 points in the game.  The provided QA pair directly supports this claim. The answer explicitly states that Kaminsky scored 20 points in a game against Wisconsin, citing two sources.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many points did Frank Kaminsky score in the Wisconsin game?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky scored 20 points in the game against Wisconsin [1, 6]. \n",
      "\n",
      "Reasoning: The provided text mentions Frank Kaminsky's performance in a game against Kentucky.  Document [1] and [6] state that he scored 20 points in that game.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 3 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mFrank Kaminsky had 11 rebounds in the game.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory, citing a source ([1]).  This directly supports the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mHow many rebounds did Frank Kaminsky have in Wisconsin's 71-64 victory?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mFrank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory [1]. \n",
      "\n",
      "Reasoning: Document [1] states that Frank Kaminsky had 11 rebounds in Wisconsin's 71-64 victory.  This is the only document that provides the specific rebound statistic for Kaminsky in the game described in the question.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mNational player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The statement asserts a Wisconsin victory with a specific score, and details Frank Kaminsky's point and rebound contributions.  Each of these aspects is confirmed by the provided evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mAll three atomic claims within the statement are supported by evidence.  The statement asserts a Wisconsin victory with a specific score, and details Frank Kaminsky's point and rebound contributions.  Each of these aspects is confirmed by the provided evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 228 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a939fb776b248e9b94cdcace5583a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (0): \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (0): \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (1):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/1] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (0): \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThere are no question-answer pairs provided to evaluate the claim \"Cover the grill and cook for 5 minutes\".  Therefore, it's impossible to determine if the claim is supported or unsupported by evidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 229 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201533951403479a92fb52f82ccf161c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific aspects of customer service did customers praise in reviews or feedback for this company/product?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [company name/product name] customer service', '[company name/product name] customer service feedback analysis']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific aspects of customer service did customers praise in reviews or feedback for this company/product?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name/product name] customer service', '[company name/product name] customer service feedback analysis']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [company name/product name] customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33m[company name/product name] customer service feedback analysis\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific aspects of customer service did customers praise in reviews or feedback for this company/product?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name/product name] customer service', '[company name/product name] customer service feedback analysis']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomers praised Taffy's Pizza for their amazing customer service.  One customer accidentally ordered a salad not on the menu, but the restaurant was able to create a custom antipasto salad and deliver it for free [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  One review specifically praises the customer service.  The reviewer accidentally ordered a salad not on the menu, but the restaurant's customer service was able to accommodate their request, creating a custom antipasto salad and delivering it for free [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided evidence directly supports the claim.  A customer review explicitly praises Taffy's Pizza for its excellent customer service, citing an instance where a mistake was handled exceptionally well, resulting in a positive customer experience.  The positive feedback directly aligns with the claim that customers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific aspects of customer service did customers praise in reviews or feedback for this company/product?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customers praised Taffy's Pizza for their amazing customer service.  One customer accidentally ordered a salad not on the menu, but the restaurant was able to create a custom antipasto salad and deliver it for free [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  One review specifically praises the customer service.  The reviewer accidentally ordered a salad not on the menu, but the restaurant's customer service was able to accommodate their request, creating a custom antipasto salad and delivering it for free [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer provides direct evidence of positive customer feedback regarding Taffy's Pizza's customer service.  A specific example of exceptional service is given, strengthening the support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews order customization\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mfeedback on product personalization\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair reveals that the available customer reviews do not contain any information regarding customer opinions on order customization.  Therefore, there is no evidence to support the claim that customers lauded the ability to customize their orders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly states that the available customer reviews lack information about customer opinions on order customization. This absence of evidence renders the claim unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[31mAttempt 1 failed:\u001b[0m\n",
      "\u001b[31mError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1282, in completion\n",
      "    response = client.post(url=url, headers=headers, json=data)  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCROr-NR9ZpxJ18a3n-mRGFOorwMoEN7r0'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2205, in completion\n",
      "    response = vertex_chat_completion.completion(  # type: ignore\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1286, in completion\n",
      "    raise VertexAIError(\n",
      "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/utils.py\", line 82, in retry_function\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/Users/sunnyfang/Desktop/Research/LLM-FactChecker/benchmark/../pipeline_v2/main.py\", line 619, in forward\n",
      "    result = self.evaluate(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/primitives/program.py\", line 22, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py\", line 20, in forward\n",
      "    return self.predict(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 81, in __call__\n",
      "    return self.forward(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/predict/predict.py\", line 111, in forward\n",
      "    completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/adapters/base.py\", line 20, in __call__\n",
      "    outputs = lm(**inputs_, **lm_kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/utils/callback.py\", line 234, in wrapper\n",
      "    return fn(instance, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 101, in __call__\n",
      "    response = completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/dspy/clients/lm.py\", line 326, in litellm_completion\n",
      "    return litellm.completion(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1002, in wrapper\n",
      "    return litellm.completion_with_retries(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2993, in completion_with_retries\n",
      "    return retryer(original_function, *args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 1022, in wrapper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/utils.py\", line 900, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/main.py\", line 2955, in completion\n",
      "    raise exception_type(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ACL/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1231, in exception_type\n",
      "    raise RateLimitError(\n",
      "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mRate limit exceeded. Waiting for 5 seconds before retrying...\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement asserts that customers lauded both the customer service and the ability to customize orders.  While the claim regarding positive customer service feedback is supported by evidence, the claim about positive feedback on order customization is unsupported due to a lack of evidence in the provided reviews.  Therefore, the overall statement is only partially supported.  Since one part is supported and the other is unsupported, a conservative approach is to label the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided evidence directly supports the claim.  A customer review explicitly praises Taffy's Pizza for its excellent customer service, citing an instance where a mistake was handled exceptionally well, resulting in a positive customer experience.  The positive feedback directly aligns with the claim that customers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific aspects of customer service did customers praise in reviews or feedback for this company/product?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomers praised Taffy's Pizza for their amazing customer service.  One customer accidentally ordered a salad not on the menu, but the restaurant was able to create a custom antipasto salad and deliver it for free [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  One review specifically praises the customer service.  The reviewer accidentally ordered a salad not on the menu, but the restaurant's customer service was able to accommodate their request, creating a custom antipasto salad and delivering it for free [2].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair reveals that the available customer reviews do not contain any information regarding customer opinions on order customization.  Therefore, there is no evidence to support the claim that customers lauded the ability to customize their orders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts that customers lauded both the customer service and the ability to customize orders.  While the claim regarding positive customer service feedback is supported by evidence, the claim about positive feedback on order customization is unsupported due to a lack of evidence in the provided reviews.  Therefore, the overall statement is only partially supported.  Since one part is supported and the other is unsupported, a conservative approach is to label the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement asserts that customers lauded both the customer service and the ability to customize orders.  While the claim regarding positive customer service feedback is supported by evidence, the claim about positive feedback on order customization is unsupported due to a lack of evidence in the provided reviews.  Therefore, the overall statement is only partially supported.  Since one part is supported and the other is unsupported, a conservative approach is to label the overall statement as unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [company name] customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer satisfaction survey [company name] support\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer feedback on Taffy's Pizza customer service is mixed. Some customers report amazing customer service, with one reviewer specifically praising their willingness to create a custom antipasto salad and deliver it for free [2, 6].  However, another customer had a negative experience, mentioning that an access card was not accepted [3, 5].  Other reviews do not directly address customer service. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  Some reviews praise the customer service, highlighting instances of accommodating customer requests and offering free delivery [2, 6]. One review mentions exceptionally amazing customer service where they went above and beyond to create a custom salad and deliver it [2, 6]. Conversely, another review expresses negative feedback, citing an issue with an access card not being accepted [3, 5].  There is also a review that doesn't mention customer service directly but expresses overall dissatisfaction with the establishment [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided answer reveals mixed customer feedback regarding Taffy's Pizza's customer service. While some customers praised the exceptional service, citing instances of accommodating custom orders and free delivery, others reported negative experiences, such as issues with payment methods.  The claim that \"Customers lauded the customer service\" is therefore not fully supported by the evidence.  The positive feedback exists, but it's counterbalanced by negative feedback, making a blanket statement of universal praise inaccurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer feedback on Taffy's Pizza customer service is mixed. Some customers report amazing customer service, with one reviewer specifically praising their willingness to create a custom antipasto salad and deliver it for free [2, 6].  However, another customer had a negative experience, mentioning that an access card was not accepted [3, 5].  Other reviews do not directly address customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer shows mixed reviews. While some customers had positive experiences, others had negative ones, contradicting the claim of universal praise.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews order customization\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mfeedback on product personalization\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mI'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair reveals a lack of information regarding customer opinions on order customization.  The answer explicitly states that the available customer reviews do not address this specific aspect of the company's offerings.  Therefore, there is no evidence to support the claim that customers lauded the ability to customize their orders.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: I'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly states that customer reviews do not contain information about order customization, thus providing no support for the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported.  The evidence provided shows mixed reviews regarding customer service, with some positive and some negative experiences.  There is no information about customer opinions on order customization.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided answer reveals mixed customer feedback regarding Taffy's Pizza's customer service. While some customers praised the exceptional service, citing instances of accommodating custom orders and free delivery, others reported negative experiences, such as issues with payment methods.  The claim that \"Customers lauded the customer service\" is therefore not fully supported by the evidence.  The positive feedback exists, but it's counterbalanced by negative feedback, making a blanket statement of universal praise inaccurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer feedback on Taffy's Pizza customer service is mixed. Some customers report amazing customer service, with one reviewer specifically praising their willingness to create a custom antipasto salad and deliver it for free [2, 6].  However, another customer had a negative experience, mentioning that an access card was not accepted [3, 5].  Other reviews do not directly address customer service. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  Some reviews praise the customer service, highlighting instances of accommodating customer requests and offering free delivery [2, 6]. One review mentions exceptionally amazing customer service where they went above and beyond to create a custom salad and deliver it [2, 6]. Conversely, another review expresses negative feedback, citing an issue with an access card not being accepted [3, 5].  There is also a review that doesn't mention customer service directly but expresses overall dissatisfaction with the establishment [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair reveals a lack of information regarding customer opinions on order customization.  The answer explicitly states that the available customer reviews do not address this specific aspect of the company's offerings.  Therefore, there is no evidence to support the claim that customers lauded the ability to customize their orders.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mI'm sorry, but the provided customer reviews do not contain information about customer opinions on order customization options offered by the company. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  The reviews focus on overall experiences, specific menu items, service quality, and ambiance.  Therefore, I cannot answer the question about customer feedback on order customization options based on the given data.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported.  The evidence provided shows mixed reviews regarding customer service, with some positive and some negative experiences.  There is no information about customer opinions on order customization.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m1.0\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported.  The evidence provided shows mixed reviews regarding customer service, with some positive and some negative experiences.  There is no information about customer opinions on order customization.  Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews [company name] customer service\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer satisfaction survey [company name] support\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews [company name] customer service', 'customer satisfaction survey [company name] support']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mCustomer feedback on Taffy's Pizza's customer service is mixed. Some customers praise the exceptional service, citing instances where staff went above and beyond to fulfill requests, even creating a custom salad and delivering it for free [2, 6]. However, other customers report negative experiences, such as issues with an access card not being accepted [3], and general dissatisfaction with their experience [1, 4]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  Some reviews praise the customer service, highlighting instances of accommodating customer requests and offering free delivery [2, 6]. One review mentions exceptionally amazing customer service where they went above and beyond to create a custom salad and deliver it [2]. Conversely, another review expresses negative feedback, mentioning an issue with an access card not being accepted [3].  There's also a review that doesn't directly mention customer service but expresses overall dissatisfaction with the experience [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mDon't order the\\nchicken piccata, there., they declined it,,   Especially, when you go to pay, you pay full price. So why did I get the card, and pay $ 40.00\\nWill I go back,? well as a first time customer, chances are. ZERO\\nNo second chances, and I would have been a steady customer.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [5] \u001b[33mDon't order the\\nchicken piccata, there., they declined it,,   Especially, when you go to pay, you pay full price. So why did I get the card, and pay $ 40.00\\nWill I go back,? well as a first time customer, chances are. ZERO\\nNo second chances, and I would have been a steady customer.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text shows mixed customer feedback regarding customer service. While some customers praised the exceptional service and cited instances of staff going above and beyond, others reported negative experiences.  The claim that \"Customers lauded the customer service\" is therefore not fully supported by the evidence.  The evidence shows both positive and negative feedback, making a blanket statement of universal praise inaccurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: Customer feedback on Taffy's Pizza's customer service is mixed. Some customers praise the exceptional service, citing instances where staff went above and beyond to fulfill requests, even creating a custom salad and delivering it for free [2, 6]. However, other customers report negative experiences, such as issues with an access card not being accepted [3], and general dissatisfaction with their experience [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer reveals mixed feedback, with both positive and negative reviews.  The presence of negative reviews contradicts the claim that all customers lauded the service.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcustomer reviews order customization\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mfeedback on product personalization\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['customer reviews order customization', 'feedback on product personalization']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe provided reviews don't directly comment on the customer experience with order customization options. However, one review mentions a \"HUGE selection of pizza breads, veggies toppings, cheeses and meats\", suggesting a wide range of choices for customization [1]. Another review describes how the restaurant accommodated a customer's request for a salad not on the menu, highlighting their willingness to adapt [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  While some reviews mention a wide variety of toppings and the ability to create custom pizzas/salads (implied by the large selection), there's no explicit feedback on the ease of use, flexibility, or overall customer experience related to the customization process itself.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThey have a HUGE selection of pizza breads, veggies toppings, cheeses and meats.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source: None (None) \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided evidence does not directly address customer opinions on the ease or effectiveness of customizing orders. While the reviews mention a wide selection of toppings and the restaurant's willingness to accommodate special requests, this doesn't equate to explicit praise for the customization *process* itself.  The absence of direct feedback on the customization experience prevents a definitive assessment of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The provided reviews don't directly comment on the customer experience with order customization options. However, one review mentions a \"HUGE selection of pizza breads, veggies toppings, cheeses and meats\", suggesting a wide range of choices for customization [1]. Another review describes how the restaurant accommodated a customer's request for a salad not on the menu, highlighting their willingness to adapt [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  While some reviews mention a wide variety of toppings and the ability to create custom pizzas/salads (implied by the large selection), there's no explicit feedback on the ease of use, flexibility, or overall customer experience related to the customization process itself.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer indicates a lack of direct customer feedback regarding the ease and effectiveness of order customization. While the wide selection of ingredients suggests potential for customization, it doesn't confirm that customers lauded this aspect.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported based on the provided evidence.  The evidence shows mixed reviews for customer service and only implies the possibility of customization without direct customer feedback on the experience. Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the customer service.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text shows mixed customer feedback regarding customer service. While some customers praised the exceptional service and cited instances of staff going above and beyond, others reported negative experiences.  The claim that \"Customers lauded the customer service\" is therefore not fully supported by the evidence.  The evidence shows both positive and negative feedback, making a blanket statement of universal praise inaccurate.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat is the customer feedback regarding the customer service provided?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mCustomer feedback on Taffy's Pizza's customer service is mixed. Some customers praise the exceptional service, citing instances where staff went above and beyond to fulfill requests, even creating a custom salad and delivering it for free [2, 6]. However, other customers report negative experiences, such as issues with an access card not being accepted [3], and general dissatisfaction with their experience [1, 4]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza.  Some reviews praise the customer service, highlighting instances of accommodating customer requests and offering free delivery [2, 6]. One review mentions exceptionally amazing customer service where they went above and beyond to create a custom salad and deliver it [2]. Conversely, another review expresses negative feedback, mentioning an issue with an access card not being accepted [3].  There's also a review that doesn't directly mention customer service but expresses overall dissatisfaction with the experience [1, 4].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house. And they were right!! It was incredible. \n",
      "\n",
      "AND free delivery, but call early to place your order. It can take a little while depending on when you call. So worth it for free delivery!!\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mBought a axxess card, they refused to accept it.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mDon't order the\\nchicken piccata, there., they declined it,,   Especially, when you go to pay, you pay full price. So why did I get the card, and pay $ 40.00\\nWill I go back,? well as a first time customer, chances are. ZERO\\nNo second chances, and I would have been a steady customer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [5] \u001b[33mDon't order the\\nchicken piccata, there., they declined it,,   Especially, when you go to pay, you pay full price. So why did I get the card, and pay $ 40.00\\nWill I go back,? well as a first time customer, chances are. ZERO\\nNo second chances, and I would have been a steady customer.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mCustomers lauded the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.8\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided evidence does not directly address customer opinions on the ease or effectiveness of customizing orders. While the reviews mention a wide selection of toppings and the restaurant's willingness to accommodate special requests, this doesn't equate to explicit praise for the customization *process* itself.  The absence of direct feedback on the customization experience prevents a definitive assessment of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat did customers say about the order customization options offered by the company?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe provided reviews don't directly comment on the customer experience with order customization options. However, one review mentions a \"HUGE selection of pizza breads, veggies toppings, cheeses and meats\", suggesting a wide range of choices for customization [1]. Another review describes how the restaurant accommodated a customer's request for a salad not on the menu, highlighting their willingness to adapt [2]. \n",
      "\n",
      "Reasoning: The provided text contains several customer reviews of Taffy's Pizza, but none directly address order customization options.  While some reviews mention a wide variety of toppings and the ability to create custom pizzas/salads (implied by the large selection), there's no explicit feedback on the ease of use, flexibility, or overall customer experience related to the customization process itself.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThey have a HUGE selection of pizza breads, veggies toppings, cheeses and meats.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mHowever, they have such amazing customer service, that they assured me they have everything to make a great antipasto salad delivered to my house.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source: None (None) \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported based on the provided evidence.  The evidence shows mixed reviews for customer service and only implies the possibility of customization without direct customer feedback on the experience. Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.9\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth atomic claims, \"Customers lauded the customer service\" and \"Customers lauded the ability to customize their order,\" are unsupported based on the provided evidence.  The evidence shows mixed reviews for customer service and only implies the possibility of customization without direct customer feedback on the experience. Therefore, the overall statement is unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 230 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8b93840a454b1194d1b88a4cb9e359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (2): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['investigation report', 'cause of incident report']\u001b[0m \u001b[0m\n",
      "\u001b[36m         2. Question: \u001b[33mWhat specific challenges or limitations hindered the identification of the cause?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['investigation limitations', 'unidentified cause explanation']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['investigation report', 'cause of incident report']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33minvestigation report\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mcause of incident report\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['investigation report', 'cause of incident report']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity. [2, 3]  They have collected specimens for testing and will be sending staff to meet the ship to further investigate. [2, 3] The cause of the illness has not yet been determined. [2, 3] \n",
      "\n",
      "Reasoning: Documents 2 and 3 report that the Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship.  The CDC has collected specimens from those affected for testing and is sending two staffers to meet the ship in San Diego to further investigate.  The cause of the illness has not yet been determined.  Documents 1 and 4 mention previous outbreaks on the same ship in 2006 and 2013, but don't detail the investigations conducted at those times.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday. Celebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific challenges or limitations hindered the identification of the cause?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['investigation limitations', 'unidentified cause explanation']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33minvestigation limitations\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33munidentified cause explanation\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [2/2] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific challenges or limitations hindered the identification of the cause?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['investigation limitations', 'unidentified cause explanation']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship has not yet been determined by the Centers for Disease Control (CDC) [2, 4].  Although the CDC is testing samples and the cruise line is implementing preventative measures, the specific cause remains unknown.  This is noteworthy given that this is the third such outbreak on this ship [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions that the Centers for Disease Control (CDC) has yet to determine the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship [2, 4].  While the CDC is collecting specimens for testing and the cruise line is taking preventative measures, the cause remains unidentified at the time of the report.  The fact that this is the third outbreak on the same ship (in 2006, 2013, and now) [1, 3] suggests a potential recurring issue, but the specific cause is still under investigation.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mAccording to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [3] \u001b[33mAccording to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [4] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text explicitly states that the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship has not yet been determined by the CDC.  While investigations are underway, including sample collection and on-site investigation, the specific cause remains unknown.  This is supported by multiple sources and the fact that this is the third such outbreak on the same ship further highlights the ongoing uncertainty regarding the root cause.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What investigations have been conducted to determine the cause of the issue?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity. [2, 3]  They have collected specimens for testing and will be sending staff to meet the ship to further investigate. [2, 3] The cause of the illness has not yet been determined. [2, 3]\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer directly states that the cause of the illness has not yet been determined, despite ongoing investigations.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific challenges or limitations hindered the identification of the cause?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship has not yet been determined by the Centers for Disease Control (CDC) [2, 4].  Although the CDC is testing samples and the cruise line is implementing preventative measures, the specific cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: This answer reinforces that the cause remains unknown, even with ongoing testing and preventative measures.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures announcement [Ship Name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures announcement [Ship Name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mShip cleaning and disinfection protocol changes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mIncreased cleaning procedures announcement [Ship Name]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures announcement [Ship Name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided answer states that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak. However, it also explicitly notes that the specific changes are not detailed.  Therefore, while there's evidence supporting *an increase* in cleaning and disinfection, the exact nature of those increases remains unknown.  This lack of detail prevents a definitive assessment of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms an increase in cleaning and disinfection procedures but lacks specifics about the changes implemented.  This makes it impossible to fully support the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement contains two claims. The first claim, \"The cause remains unknown,\" is supported by evidence indicating an ongoing investigation and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is unsupported because while increased procedures are mentioned, the specifics of these changes are not detailed.  Since one claim is supported and the other is unsupported, the overall verdict leans towards unsupported due to the lack of concrete evidence for the second claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text explicitly states that the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship has not yet been determined by the CDC.  While investigations are underway, including sample collection and on-site investigation, the specific cause remains unknown.  This is supported by multiple sources and the fact that this is the third such outbreak on the same ship further highlights the ongoing uncertainty regarding the root cause.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity. [2, 3]  They have collected specimens for testing and will be sending staff to meet the ship to further investigate. [2, 3] The cause of the illness has not yet been determined. [2, 3] \n",
      "\n",
      "Reasoning: Documents 2 and 3 report that the Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship.  The CDC has collected specimens from those affected for testing and is sending two staffers to meet the ship in San Diego to further investigate.  The cause of the illness has not yet been determined.  Documents 1 and 4 mention previous outbreaks on the same ship in 2006 and 2013, but don't detail the investigations conducted at those times.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday. Celebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m     Component 2 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific challenges or limitations hindered the identification of the cause?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship has not yet been determined by the Centers for Disease Control (CDC) [2, 4].  Although the CDC is testing samples and the cruise line is implementing preventative measures, the specific cause remains unknown.  This is noteworthy given that this is the third such outbreak on this ship [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions that the Centers for Disease Control (CDC) has yet to determine the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship [2, 4].  While the CDC is collecting specimens for testing and the cruise line is taking preventative measures, the cause remains unidentified at the time of the report.  The fact that this is the third outbreak on the same ship (in 2006, 2013, and now) [1, 3] suggests a potential recurring issue, but the specific cause is still under investigation.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mAccording to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [3] \u001b[33mAccording to the Maritime Executive, this is the third time the Celebrity Infinity has suffered an outbreak of gastrointestinal illness, with others occurring in 2006 and 2013.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [4] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided answer states that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak. However, it also explicitly notes that the specific changes are not detailed.  Therefore, while there's evidence supporting *an increase* in cleaning and disinfection, the exact nature of those increases remains unknown.  This lack of detail prevents a definitive assessment of the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement contains two claims. The first claim, \"The cause remains unknown,\" is supported by evidence indicating an ongoing investigation and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is unsupported because while increased procedures are mentioned, the specifics of these changes are not detailed.  Since one claim is supported and the other is unsupported, the overall verdict leans towards unsupported due to the lack of concrete evidence for the second claim.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement contains two claims. The first claim, \"The cause remains unknown,\" is supported by evidence indicating an ongoing investigation and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is unsupported because while increased procedures are mentioned, the specifics of these changes are not detailed.  Since one claim is supported and the other is unsupported, the overall verdict leans towards unsupported due to the lack of concrete evidence for the second claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue on the ship?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['ship incident investigation report', 'details of investigation into [ship name] incident']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue on the ship?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ship incident investigation report', 'details of investigation into [ship name] incident']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mship incident investigation report\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mdetails of investigation into [ship name] incident\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue on the ship?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ship incident investigation report', 'details of investigation into [ship name] incident']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mThe Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship [1, 3].  They are collecting specimens from those affected for testing [1, 3], but have not yet determined the cause [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions that the Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship.  The CDC is collecting specimens from those affected for testing to determine the cause [1, 3].  While the investigation is ongoing, the CDC has not yet determined the cause of the illness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mThe CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided text explicitly states that the Centers for Disease Control (CDC) is investigating the cause of a gastrointestinal illness outbreak on a cruise ship but has not yet determined the cause.  This directly supports the claim that the cause remains unknown.  The investigation is ongoing, and the lack of a determined cause aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What investigations have been conducted to determine the cause of the issue on the ship?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: The Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship [1, 3].  They are collecting specimens from those affected for testing [1, 3], but have not yet determined the cause [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions that the Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship.  The CDC is collecting specimens from those affected for testing to determine the cause [1, 3].  While the investigation is ongoing, the CDC has not yet determined the cause of the illness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer directly states that the CDC investigation is underway but has not yet identified the cause of the illness. This strongly supports the claim that the cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Ship name cleaning protocol changes', 'Increased disinfection procedures ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship name cleaning protocol changes', 'Increased disinfection procedures ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mShip name cleaning protocol changes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mIncreased disinfection procedures ship [ship name]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship name cleaning protocol changes', 'Increased disinfection procedures ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures on the Celebrity Infinity [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided answer states that cleaning and disinfection procedures were increased on the Celebrity Infinity ship following a gastrointestinal illness outbreak.  However, the answer explicitly notes that the *specific* changes to these procedures are not detailed.  Therefore, while there's evidence supporting an increase in cleaning and disinfection, the lack of detail on the nature of those increases prevents a definitive assessment of the claim's accuracy.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures on the Celebrity Infinity [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms that cleaning and disinfection procedures were increased, but the lack of specifics lowers confidence.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mBoth claims in the statement are supported by evidence.  The first claim, \"The cause remains unknown,\" is supported by the investigation's ongoing nature and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is supported by the confirmation that increased procedures were implemented, although the specifics are not detailed.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cause remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided text explicitly states that the Centers for Disease Control (CDC) is investigating the cause of a gastrointestinal illness outbreak on a cruise ship but has not yet determined the cause.  This directly supports the claim that the cause remains unknown.  The investigation is ongoing, and the lack of a determined cause aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat investigations have been conducted to determine the cause of the issue on the ship?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mThe Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship [1, 3].  They are collecting specimens from those affected for testing [1, 3], but have not yet determined the cause [1, 3]. \n",
      "\n",
      "Reasoning: The provided text mentions that the Centers for Disease Control (CDC) is investigating the cause of the gastrointestinal illness outbreak on the Celebrity Infinity cruise ship.  The CDC is collecting specimens from those affected for testing to determine the cause [1, 3].  While the investigation is ongoing, the CDC has not yet determined the cause of the illness [1, 3].\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mThe CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments. Two staffers from the agency are scheduled to meet the West Coast-based ship in San Diego on Monday.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.5\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided answer states that cleaning and disinfection procedures were increased on the Celebrity Infinity ship following a gastrointestinal illness outbreak.  However, the answer explicitly notes that the *specific* changes to these procedures are not detailed.  Therefore, while there's evidence supporting an increase in cleaning and disinfection, the lack of detail on the nature of those increases prevents a definitive assessment of the claim's accuracy.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures on the Celebrity Infinity [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims in the statement are supported by evidence.  The first claim, \"The cause remains unknown,\" is supported by the investigation's ongoing nature and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is supported by the confirmation that increased procedures were implemented, although the specifics are not detailed.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mBoth claims in the statement are supported by evidence.  The first claim, \"The cause remains unknown,\" is supported by the investigation's ongoing nature and the lack of a determined cause. The second claim, \"The ship has increased cleaning and disinfection procedures,\" is supported by the confirmation that increased procedures were implemented, although the specifics are not detailed.  Therefore, the overall statement is supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Starting Fact Check Pipeline =====\u001b[0m\n",
      "\u001b[36m Original Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Atomic Claim Extraction =====\u001b[0m\n",
      "\u001b[36m   Extracted Claims (2):  \u001b[0m\n",
      "\u001b[36m     1. \u001b[97mThe cause of an event related to the ship remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     2. \u001b[97mThe ship has increased its cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [1/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat event occurred on the ship, and why is its cause currently unknown?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['ship incident cause unknown', 'recent ship outbreaks news']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat event occurred on the ship, and why is its cause currently unknown?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ship incident cause unknown', 'recent ship outbreaks news']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mship incident cause unknown\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mrecent ship outbreaks news\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat event occurred on the ship, and why is its cause currently unknown?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['ship incident cause unknown', 'recent ship outbreaks news']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mA gastrointestinal illness affected 100 people on the Celebrity Infinity cruise ship [1, 3].  The cause of the outbreak is currently unknown and is under investigation by the CDC [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, a gastrointestinal illness affected 100 people (95 passengers and 5 crew members) aboard the Celebrity Infinity cruise ship.  The cause of the illness is currently unknown, as the CDC is still investigating and has yet to determine the source.  The documents highlight that this is not the first time the ship has experienced such an outbreak.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe cause of an event related to the ship remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that a gastrointestinal illness affected 100 people on the Celebrity Infinity cruise ship, and the cause of this outbreak remains unknown and is under investigation by the CDC.  This aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What event occurred on the ship, and why is its cause currently unknown?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: A gastrointestinal illness affected 100 people on the Celebrity Infinity cruise ship [1, 3].  The cause of the outbreak is currently unknown and is under investigation by the CDC [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, a gastrointestinal illness affected 100 people (95 passengers and 5 crew members) aboard the Celebrity Infinity cruise ship.  The cause of the illness is currently unknown, as the CDC is still investigating and has yet to determine the source.  The documents highlight that this is not the first time the ship has experienced such an outbreak.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer explicitly confirms that an event (gastrointestinal illness) occurred on the ship, and its cause is currently unknown, directly supporting the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m    ===== Question Generation [2/2] =====\u001b[0m\n",
      "\u001b[36m       Decomposed Components (Questions + Search Queries) (1): \u001b[0m\n",
      "\u001b[36m         1. Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m            Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures cruise ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Question Answering for Component [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures cruise ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [1/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mShip cleaning and disinfection protocol changes\u001b[0m \u001b[0m\n",
      "\u001b[36m        ===== Web Search for Query [2/2] =====\u001b[0m\n",
      "\u001b[36m         Query: \u001b[33mIncreased cleaning procedures cruise ship [ship name]\u001b[0m \u001b[0m\n",
      "\u001b[36m      ===== Synthesizing Answer [1/1] =====\u001b[0m\n",
      "\u001b[36m         Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         Search Queries: \u001b[33m['Ship cleaning and disinfection protocol changes', 'Increased cleaning procedures cruise ship [ship name]']\u001b[0m \u001b[0m\n",
      "\u001b[36m         Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Citations:  \u001b[0m\n",
      "\u001b[36m           [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m           Source:  () \u001b[0m\n",
      "\u001b[36m    ===== Claim Evaluation =====\u001b[0m\n",
      "\u001b[36m       Claim: \u001b[97mThe ship has increased its cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m       Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m       Reasoning: \u001b[36mThe provided answer states that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak. However, it also explicitly notes that the specific changes made are not detailed.  Therefore, while there's evidence suggesting an increase in cleaning and disinfection, the lack of specifics prevents a definitive assessment of the claim's accuracy.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Evidence Analysis \u001b[0m\n",
      "\u001b[36m         \u001b[33mQuestion: What specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[32mAnswer: In response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m         \u001b[36mContribution: The answer confirms an increase in cleaning and disinfection procedures but lacks specifics about the nature of those increases. This makes it impossible to fully support the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m  ===== Overall Statement Evaluation =====\u001b[0m\n",
      "\u001b[36m     Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Overall Reasoning: \u001b[36mThe statement consists of two parts: one about the unknown cause of an event and another about increased cleaning and disinfection procedures.  The first part is supported by evidence showing an unknown cause for a gastrointestinal illness outbreak. The second part is unsupported because while increased procedures are mentioned, the specifics of those increases are not detailed.  Since one part is supported and the other unsupported, the overall verdict leans towards unsupported due to the lack of specific evidence for the second claim.  A more conservative approach would be to label it as partially supported, but the prompt requires a binary verdict.\u001b[0m \u001b[0m\n",
      "\u001b[36m===== Breakdown of Claims and Components =====\u001b[0m\n",
      "\u001b[36m   Claim 1 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe cause of an event related to the ship remains unknown.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.95\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided question-answer pair directly addresses the claim. The answer explicitly states that a gastrointestinal illness affected 100 people on the Celebrity Infinity cruise ship, and the cause of this outbreak remains unknown and is under investigation by the CDC.  This aligns perfectly with the claim.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat event occurred on the ship, and why is its cause currently unknown?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mA gastrointestinal illness affected 100 people on the Celebrity Infinity cruise ship [1, 3].  The cause of the outbreak is currently unknown and is under investigation by the CDC [1, 3]. \n",
      "\n",
      "Reasoning: Based on the provided text, a gastrointestinal illness affected 100 people (95 passengers and 5 crew members) aboard the Celebrity Infinity cruise ship.  The cause of the illness is currently unknown, as the CDC is still investigating and has yet to determine the source.  The documents highlight that this is not the first time the ship has experienced such an outbreak.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mGastrointestinal illness has gripped 100 people on the cruise ship Celebrity Infinity, according to a report from the Centers for Disease Control. Of the ship's 2,117 passengers, 95 have suffered from vomiting, diarrhea and other symptoms, the CDC said. The illness has also affected five members of the 964-person crew. The CDC has yet to determine what's causing the ailments.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m   Claim 2 \u001b[0m\n",
      "\u001b[36m     Text: \u001b[97mThe ship has increased its cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m     Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m     Reasoning: \u001b[36mThe provided answer states that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak. However, it also explicitly notes that the specific changes made are not detailed.  Therefore, while there's evidence suggesting an increase in cleaning and disinfection, the lack of specifics prevents a definitive assessment of the claim's accuracy.\u001b[0m \u001b[0m\n",
      "\u001b[36m     Component 1 \u001b[0m\n",
      "\u001b[36m       Question: \u001b[33mWhat specific changes were made to the ship's cleaning and disinfection procedures?\u001b[0m \u001b[0m\n",
      "\u001b[36m       Answer: \u001b[32mIn response to a gastrointestinal illness outbreak, Celebrity Cruises increased cleaning and disinfection procedures [1, 3].  However, the specific changes made to these procedures are not detailed in the provided text. \n",
      "\n",
      "Reasoning: The provided text mentions that Celebrity Cruises increased cleaning and disinfection procedures in response to a gastrointestinal illness outbreak on the Celebrity Infinity.  However, the exact changes to the procedures are not specified.  The documents only state that the procedures were *increased*, not what those increases entailed.\u001b[0m \u001b[0m\n",
      "\u001b[36m       Citations:  \u001b[0m\n",
      "\u001b[36m         [1] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\u001b[36m         [2] \u001b[33mCelebrity Cruises has been taking action since the outbreak began, including increasing cleaning and disinfection procedures, keeping passengers informed and taking specimens from the afflicted for testing by the CDC, the agency says.\u001b[0m \u001b[0m\n",
      "\u001b[36m         Source:  () \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement consists of two parts: one about the unknown cause of an event and another about increased cleaning and disinfection procedures.  The first part is supported by evidence showing an unknown cause for a gastrointestinal illness outbreak. The second part is unsupported because while increased procedures are mentioned, the specifics of those increases are not detailed.  Since one part is supported and the other unsupported, the overall verdict leans towards unsupported due to the lack of specific evidence for the second claim.  A more conservative approach would be to label it as partially supported, but the prompt requires a binary verdict.\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33m0.6\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement consists of two parts: one about the unknown cause of an event and another about increased cleaning and disinfection procedures.  The first part is supported by evidence showing an unknown cause for a gastrointestinal illness outbreak. The second part is unsupported because while increased procedures are mentioned, the specifics of those increases are not detailed.  Since one part is supported and the other unsupported, the overall verdict leans towards unsupported due to the lack of specific evidence for the second claim.  A more conservative approach would be to label it as partially supported, but the prompt requires a binary verdict.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "from utils import print_header\n",
    "\n",
    "def print_final_result(statement, verdict, confidence, reasoning, gold_verdict=None):\n",
    "    print(\"\\nFinal Fact-Check Result:\")\n",
    "    print_header(f\"Statement: {colored(statement, 'white')}\", level=1)\n",
    "    print_header(f\"Overall Verdict: {colored(verdict, 'green')}\", level=1)\n",
    "    print_header(f\"Overall Confidence: {colored(str(confidence), 'yellow')}\", level=1)\n",
    "    print_header(f\"Overall Reasoning: {colored(reasoning, 'cyan')}\", level=1)\n",
    "    if gold_verdict: print_header(f\"Gold Verdict: {colored(gold_verdict, 'green')}\", level=1)\n",
    "\n",
    "### Load data\n",
    "# output_file = args.output_file\n",
    "# if os.path.exists(output_file):\n",
    "#     df = pd.read_pickle(output_file)\n",
    "# Set custom constants for whole pipeline\n",
    "# main.VERBOSE = False # Print intermediate results\n",
    "main.VERDICTS=[\"unsupported\", \"supported\"]\n",
    "\n",
    "model = 'gemini'\n",
    "num_trials = 3\n",
    "lm = dspy.LM('gemini/gemini-1.5-flash', api_key=os.getenv('GOOGLE_GEMINI_API_KEY'), cache=False)\n",
    "output_file = f'aggrefact_{model}.pkl'\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_pickle(output_file)\n",
    "else: \n",
    "    df = pd.DataFrame(load_dataset(\"lytang/LLM-AggreFact\")['test'])\n",
    "    # Drop unneeded columns\n",
    "    df = df[df['dataset'].isin(['RAGTruth','ExpertQA','Lfqa','ClaimVerify'])]\n",
    "    df = df.groupby('dataset').apply(lambda x: x.sample(frac=0.01, random_state=42))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "dspy.settings.configure(lm=lm, temperature=0.3)\n",
    "\n",
    "# If column doesn't exist, create it\n",
    "if f'{model}_results' not in df.columns: df[f'{model}_results'] = None\n",
    "df[f'{model}_results'] = df[f'{model}_results'].astype(object)\n",
    "display(df.head())\n",
    "\n",
    "for index in tqdm(range(len(df))):\n",
    "    # If results already exist, skip if num_trials is reached\n",
    "    if df.iloc[index][f'{model}_results'] is not None: \n",
    "        if len(df.loc[index, f'{model}_results']) == num_trials:\n",
    "            print(f\"Skipping row {index} because {num_trials}/{num_trials} trials completed\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Running row {index} because {len(df.loc[index, f'{model}_results'])}/{num_trials} trials completed\")\n",
    "            results = df.loc[index, f'{model}_results']\n",
    "    else: \n",
    "        print(f\"Running row {index} because 0/{num_trials} trials completed\")\n",
    "        results = []\n",
    "\n",
    "    for trial_i in tqdm(range(num_trials-len(results)), leave=False):\n",
    "        source_doc = df.iloc[index]['doc']\n",
    "        pipeline = main.FactCheckPipeline(\n",
    "            search_provider=main.SearchProvider(provider=\"duckduckgo\"),\n",
    "            model_name=lm,\n",
    "            embedding_model=main.EMBEDDING_MODEL,\n",
    "            context=[main.Document(content=source_doc, metadata={\"source\": \"source\"})],\n",
    "            retriever_k=5\n",
    "        )\n",
    "\n",
    "        statement = df.iloc[index]['claim']\n",
    "        gold_verdict = df.iloc[index]['label']\n",
    "        try: \n",
    "            verdict, confidence, reasoning, claims = pipeline.fact_check(\n",
    "                # statement=f\"According to {statement_originator} on {statement_date}, {statement}\", \n",
    "                statement=statement, \n",
    "                web_search=False\n",
    "                # statement=statement, \n",
    "                # context=f\"Statement Originator: {statement_originator}, Date Claim Was Made: {statement_date}\"\n",
    "            )\n",
    "        except Exception as e: \n",
    "            if 'Failed to extract claim' in str(e): \n",
    "                continue\n",
    "\n",
    "        print_final_result(statement, verdict, confidence, reasoning, gold_verdict)\n",
    "        results.append({\n",
    "            'verdict': verdict,\n",
    "            'confidence': confidence,\n",
    "            'reasoning': reasoning,\n",
    "            'claims': claims\n",
    "        })\n",
    "        df.at[index, f'{model}_results'] = results\n",
    "        df.to_pickle(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9578f27546b141bda949b1a725b98ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 0 because 3/3 trials completed\n",
      "Skipping row 1 because 3/3 trials completed\n",
      "Skipping row 2 because 3/3 trials completed\n",
      "Skipping row 3 because 3/3 trials completed\n",
      "Skipping row 4 because 3/3 trials completed\n",
      "Skipping row 5 because 3/3 trials completed\n",
      "Skipping row 6 because 3/3 trials completed\n",
      "Skipping row 7 because 3/3 trials completed\n",
      "Skipping row 8 because 3/3 trials completed\n",
      "Skipping row 9 because 3/3 trials completed\n",
      "Skipping row 10 because 3/3 trials completed\n",
      "Skipping row 11 because 3/3 trials completed\n",
      "Skipping row 12 because 3/3 trials completed\n",
      "Skipping row 13 because 3/3 trials completed\n",
      "Skipping row 14 because 3/3 trials completed\n",
      "Skipping row 15 because 3/3 trials completed\n",
      "Skipping row 16 because 3/3 trials completed\n",
      "Skipping row 17 because 3/3 trials completed\n",
      "Skipping row 18 because 3/3 trials completed\n",
      "Skipping row 19 because 3/3 trials completed\n",
      "Skipping row 20 because 3/3 trials completed\n",
      "Skipping row 21 because 3/3 trials completed\n",
      "Skipping row 22 because 3/3 trials completed\n",
      "Skipping row 23 because 3/3 trials completed\n",
      "Skipping row 24 because 3/3 trials completed\n",
      "Skipping row 25 because 3/3 trials completed\n",
      "Skipping row 26 because 3/3 trials completed\n",
      "Skipping row 27 because 3/3 trials completed\n",
      "Skipping row 28 because 3/3 trials completed\n",
      "Skipping row 29 because 3/3 trials completed\n",
      "Skipping row 30 because 3/3 trials completed\n",
      "Skipping row 31 because 3/3 trials completed\n",
      "Skipping row 32 because 3/3 trials completed\n",
      "Skipping row 33 because 3/3 trials completed\n",
      "Skipping row 34 because 3/3 trials completed\n",
      "Skipping row 35 because 3/3 trials completed\n",
      "Skipping row 36 because 3/3 trials completed\n",
      "Skipping row 37 because 3/3 trials completed\n",
      "Skipping row 38 because 3/3 trials completed\n",
      "Skipping row 39 because 3/3 trials completed\n",
      "Skipping row 40 because 3/3 trials completed\n",
      "Skipping row 41 because 3/3 trials completed\n",
      "Skipping row 42 because 3/3 trials completed\n",
      "Skipping row 43 because 3/3 trials completed\n",
      "Skipping row 44 because 3/3 trials completed\n",
      "Skipping row 45 because 3/3 trials completed\n",
      "Skipping row 46 because 3/3 trials completed\n",
      "Skipping row 47 because 3/3 trials completed\n",
      "Skipping row 48 because 3/3 trials completed\n",
      "Skipping row 49 because 3/3 trials completed\n",
      "Skipping row 50 because 3/3 trials completed\n",
      "Skipping row 51 because 3/3 trials completed\n",
      "Skipping row 52 because 3/3 trials completed\n",
      "Skipping row 53 because 3/3 trials completed\n",
      "Skipping row 54 because 3/3 trials completed\n",
      "Skipping row 55 because 3/3 trials completed\n",
      "Skipping row 56 because 3/3 trials completed\n",
      "Skipping row 57 because 3/3 trials completed\n",
      "Skipping row 58 because 3/3 trials completed\n",
      "Skipping row 59 because 3/3 trials completed\n",
      "Skipping row 60 because 3/3 trials completed\n",
      "Skipping row 61 because 3/3 trials completed\n",
      "Skipping row 62 because 3/3 trials completed\n",
      "Skipping row 63 because 3/3 trials completed\n",
      "Skipping row 64 because 3/3 trials completed\n",
      "Skipping row 65 because 3/3 trials completed\n",
      "Skipping row 66 because 3/3 trials completed\n",
      "Skipping row 67 because 3/3 trials completed\n",
      "Skipping row 68 because 3/3 trials completed\n",
      "Skipping row 69 because 3/3 trials completed\n",
      "Skipping row 70 because 3/3 trials completed\n",
      "Skipping row 71 because 3/3 trials completed\n",
      "Skipping row 72 because 3/3 trials completed\n",
      "Skipping row 73 because 3/3 trials completed\n",
      "Skipping row 74 because 3/3 trials completed\n",
      "Skipping row 75 because 3/3 trials completed\n",
      "Skipping row 76 because 3/3 trials completed\n",
      "Skipping row 77 because 3/3 trials completed\n",
      "Skipping row 78 because 3/3 trials completed\n",
      "Skipping row 79 because 3/3 trials completed\n",
      "Skipping row 80 because 3/3 trials completed\n",
      "Skipping row 81 because 3/3 trials completed\n",
      "Skipping row 82 because 3/3 trials completed\n",
      "Skipping row 83 because 3/3 trials completed\n",
      "Skipping row 84 because 3/3 trials completed\n",
      "Skipping row 85 because 3/3 trials completed\n",
      "Skipping row 86 because 3/3 trials completed\n",
      "Skipping row 87 because 3/3 trials completed\n",
      "Skipping row 88 because 3/3 trials completed\n",
      "Skipping row 89 because 3/3 trials completed\n",
      "Skipping row 90 because 3/3 trials completed\n",
      "Skipping row 91 because 3/3 trials completed\n",
      "Skipping row 92 because 3/3 trials completed\n",
      "Skipping row 93 because 3/3 trials completed\n",
      "Skipping row 94 because 3/3 trials completed\n",
      "Skipping row 95 because 3/3 trials completed\n",
      "Skipping row 96 because 3/3 trials completed\n",
      "Skipping row 97 because 3/3 trials completed\n",
      "Skipping row 98 because 3/3 trials completed\n",
      "Skipping row 99 because 3/3 trials completed\n",
      "Skipping row 100 because 3/3 trials completed\n",
      "Skipping row 101 because 3/3 trials completed\n",
      "Skipping row 102 because 3/3 trials completed\n",
      "Skipping row 103 because 3/3 trials completed\n",
      "Skipping row 104 because 3/3 trials completed\n",
      "Skipping row 105 because 3/3 trials completed\n",
      "Skipping row 106 because 3/3 trials completed\n",
      "Skipping row 107 because 3/3 trials completed\n",
      "Skipping row 108 because 3/3 trials completed\n",
      "Skipping row 109 because 3/3 trials completed\n",
      "Skipping row 110 because 3/3 trials completed\n",
      "Skipping row 111 because 3/3 trials completed\n",
      "Skipping row 112 because 3/3 trials completed\n",
      "Skipping row 113 because 3/3 trials completed\n",
      "Skipping row 114 because 3/3 trials completed\n",
      "Skipping row 115 because 3/3 trials completed\n",
      "Skipping row 116 because 3/3 trials completed\n",
      "Skipping row 117 because 3/3 trials completed\n",
      "Skipping row 118 because 3/3 trials completed\n",
      "Skipping row 119 because 3/3 trials completed\n",
      "Skipping row 120 because 3/3 trials completed\n",
      "Skipping row 121 because 3/3 trials completed\n",
      "Skipping row 122 because 3/3 trials completed\n",
      "Skipping row 123 because 3/3 trials completed\n",
      "Skipping row 124 because 3/3 trials completed\n",
      "Skipping row 125 because 3/3 trials completed\n",
      "Skipping row 126 because 3/3 trials completed\n",
      "Skipping row 127 because 3/3 trials completed\n",
      "Skipping row 128 because 3/3 trials completed\n",
      "Skipping row 129 because 3/3 trials completed\n",
      "Skipping row 130 because 3/3 trials completed\n",
      "Skipping row 131 because 3/3 trials completed\n",
      "Skipping row 132 because 3/3 trials completed\n",
      "Skipping row 133 because 3/3 trials completed\n",
      "Skipping row 134 because 3/3 trials completed\n",
      "Skipping row 135 because 3/3 trials completed\n",
      "Skipping row 136 because 3/3 trials completed\n",
      "Skipping row 137 because 3/3 trials completed\n",
      "Skipping row 138 because 3/3 trials completed\n",
      "Skipping row 139 because 3/3 trials completed\n",
      "Skipping row 140 because 3/3 trials completed\n",
      "Skipping row 141 because 3/3 trials completed\n",
      "Skipping row 142 because 3/3 trials completed\n",
      "Skipping row 143 because 3/3 trials completed\n",
      "Skipping row 144 because 3/3 trials completed\n",
      "Skipping row 145 because 3/3 trials completed\n",
      "Skipping row 146 because 3/3 trials completed\n",
      "Skipping row 147 because 3/3 trials completed\n",
      "Skipping row 148 because 3/3 trials completed\n",
      "Skipping row 149 because 3/3 trials completed\n",
      "Skipping row 150 because 3/3 trials completed\n",
      "Skipping row 151 because 3/3 trials completed\n",
      "Skipping row 152 because 3/3 trials completed\n",
      "Skipping row 153 because 3/3 trials completed\n",
      "Skipping row 154 because 3/3 trials completed\n",
      "Skipping row 155 because 3/3 trials completed\n",
      "Skipping row 156 because 3/3 trials completed\n",
      "Skipping row 157 because 3/3 trials completed\n",
      "Skipping row 158 because 3/3 trials completed\n",
      "Skipping row 159 because 3/3 trials completed\n",
      "Skipping row 160 because 3/3 trials completed\n",
      "Skipping row 161 because 3/3 trials completed\n",
      "Skipping row 162 because 3/3 trials completed\n",
      "Skipping row 163 because 3/3 trials completed\n",
      "Skipping row 164 because 3/3 trials completed\n",
      "Skipping row 165 because 3/3 trials completed\n",
      "Skipping row 166 because 3/3 trials completed\n",
      "Skipping row 167 because 3/3 trials completed\n",
      "Skipping row 168 because 3/3 trials completed\n",
      "Skipping row 169 because 3/3 trials completed\n",
      "Skipping row 170 because 3/3 trials completed\n",
      "Skipping row 171 because 3/3 trials completed\n",
      "Skipping row 172 because 3/3 trials completed\n",
      "Skipping row 173 because 3/3 trials completed\n",
      "Skipping row 174 because 3/3 trials completed\n",
      "Skipping row 175 because 3/3 trials completed\n",
      "Skipping row 176 because 3/3 trials completed\n",
      "Skipping row 177 because 3/3 trials completed\n",
      "Skipping row 178 because 3/3 trials completed\n",
      "Skipping row 179 because 3/3 trials completed\n",
      "Skipping row 180 because 3/3 trials completed\n",
      "Skipping row 181 because 3/3 trials completed\n",
      "Skipping row 182 because 3/3 trials completed\n",
      "Skipping row 183 because 3/3 trials completed\n",
      "Skipping row 184 because 3/3 trials completed\n",
      "Skipping row 185 because 3/3 trials completed\n",
      "Skipping row 186 because 3/3 trials completed\n",
      "Skipping row 187 because 3/3 trials completed\n",
      "Skipping row 188 because 3/3 trials completed\n",
      "Skipping row 189 because 3/3 trials completed\n",
      "Skipping row 190 because 3/3 trials completed\n",
      "Skipping row 191 because 3/3 trials completed\n",
      "Skipping row 192 because 3/3 trials completed\n",
      "Skipping row 193 because 3/3 trials completed\n",
      "Skipping row 194 because 3/3 trials completed\n",
      "Skipping row 195 because 3/3 trials completed\n",
      "Skipping row 196 because 3/3 trials completed\n",
      "Skipping row 197 because 3/3 trials completed\n",
      "Skipping row 198 because 3/3 trials completed\n",
      "Skipping row 199 because 3/3 trials completed\n",
      "Skipping row 200 because 3/3 trials completed\n",
      "Skipping row 201 because 3/3 trials completed\n",
      "Skipping row 202 because 3/3 trials completed\n",
      "Skipping row 203 because 3/3 trials completed\n",
      "Skipping row 204 because 3/3 trials completed\n",
      "Skipping row 205 because 3/3 trials completed\n",
      "Skipping row 206 because 3/3 trials completed\n",
      "Skipping row 207 because 3/3 trials completed\n",
      "Skipping row 208 because 3/3 trials completed\n",
      "Skipping row 209 because 3/3 trials completed\n",
      "Skipping row 210 because 3/3 trials completed\n",
      "Skipping row 211 because 3/3 trials completed\n",
      "Skipping row 212 because 3/3 trials completed\n",
      "Skipping row 213 because 3/3 trials completed\n",
      "Skipping row 214 because 3/3 trials completed\n",
      "Skipping row 215 because 3/3 trials completed\n",
      "Skipping row 216 because 3/3 trials completed\n",
      "Skipping row 217 because 3/3 trials completed\n",
      "Skipping row 218 because 3/3 trials completed\n",
      "Skipping row 219 because 3/3 trials completed\n",
      "Skipping row 220 because 3/3 trials completed\n",
      "Skipping row 221 because 3/3 trials completed\n",
      "Skipping row 222 because 3/3 trials completed\n",
      "Skipping row 223 because 3/3 trials completed\n",
      "Skipping row 224 because 3/3 trials completed\n",
      "Skipping row 225 because 3/3 trials completed\n",
      "Running row 226 because 1/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa1e04af2204eb4ba340aa6eb9d7671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement describes a straightforward process of combining several ingredients in a bowl.  There's no inherent falsehood or unsupported claim within the instructions.  The process is entirely feasible and commonly done in cooking.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mIn a bowl, mix together seasoned salt, black pepper, garlic powder, onion powder, paprika, Worcestershire sauce, and smoke flavoring until thoroughly combined.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement describes a straightforward process of combining several ingredients in a bowl.  There's no inherent falsehood or unsupported claim within the instructions.  The process is entirely feasible and commonly done in cooking.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 227 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7df6bfdf824e5095ef6ac1c417110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement describes a basketball game where Wisconsin won 71-64, with Frank Kaminsky contributing 20 points and 11 rebounds.  This is a factual claim that can be verified through box scores and game reports from the specific game in question.  Without access to those sources, I cannot definitively confirm the accuracy of the statement. However, the statement is presented as a factual account of a sporting event, and such accounts are commonly reported and verifiable.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement describes a basketball game where Wisconsin won 71-64, with Frank Kaminsky contributing 20 points and 11 rebounds.  This is a factual claim that can be verified through box scores and game reports from the specific game in question.  Without access to those sources, I cannot definitively confirm the accuracy of the statement. However, the statement's structure and detail suggest it's likely based on a real game report.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mWisconsin rallied for a tough 71-64 victory, led by Frank Kaminsky who scored 20 points and 11 rebounds.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement describes a basketball game where Wisconsin won 71-64, with Frank Kaminsky contributing 20 points and 11 rebounds.  This is a factual claim that can be verified through box scores and game reports from the specific game in question.  Without access to those sources, I cannot definitively confirm the accuracy of the statement. However, the statement's structure and content suggest it's likely based on a real game report.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 228 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e78769767e944d8ada3c33e681fd1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement \"Cover the grill and cook for 5 minutes\" is a directive or instruction, not a factual statement that can be evaluated as true or false.  Therefore, it cannot be classified as supported or unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement \"Cover the grill and cook for 5 minutes\" is a directive or instruction, not a factual statement that can be evaluated as true or false.  Therefore, it cannot be classified as supported or unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mCover the grill and cook for 5 minutes.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement \"Cover the grill and cook for 5 minutes\" is a directive or instruction, not a factual statement that can be evaluated as true or false.  Therefore, it cannot be classified as supported or unsupported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 229 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe4cbd144f4486bbf0327bc282ce022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents a positive assessment of a service, highlighting aspects like customer service and customization options.  Without further context or evidence to contradict this, the statement can be considered supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents a positive assessment of a service, highlighting aspects like customer service and customization options.  Without further context or evidence to the contrary, the statement can be considered supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThey lauded the customer service and the ability to customize their order.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32msupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents a positive assessment of a service, highlighting aspects like customer service and customization options.  Without further context or evidence to contradict this, the statement can be considered supported.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "Running row 230 because 0/3 trials completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55310dd78b324c28a71b1772ac9b9da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents two separate pieces of information: the cause of something is unknown, and a ship has increased its cleaning and disinfection procedures.  While these are related in that increased cleaning might be a response to an unknown cause, the statement itself doesn't offer evidence to support or refute either claim independently.  We cannot verify the existence of an unknown cause, nor can we independently verify the increased cleaning procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents two separate pieces of information: the cause of something is unknown, and a ship has increased its cleaning and disinfection procedures.  While these are related in that increased cleaning might be a response to an unknown cause, the statement itself doesn't offer evidence to support or refute either claim independently.  We cannot verify the existence of an unknown cause, nor can we independently verify the increased cleaning procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n",
      "\n",
      "Final Fact-Check Result:\n",
      "\u001b[36m   Statement: \u001b[97mThe cause remains unknown and the ship has increased cleaning and disinfection procedures.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Verdict: \u001b[32munsupported\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Confidence: \u001b[33mN/A\u001b[0m \u001b[0m\n",
      "\u001b[36m   Overall Reasoning: \u001b[36mThe statement presents two separate pieces of information: the cause of something is unknown, and a ship has increased its cleaning and disinfection procedures.  While these pieces of information are related (the increased cleaning might be a response to the unknown cause), neither supports or refutes the other directly.  The statement doesn't make a claim that can be evaluated as true or false.\u001b[0m \u001b[0m\n",
      "\u001b[36m   Gold Verdict: \u001b[32m1\u001b[0m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize baseline\n",
    "VERDICTS = ['supported','unsupported']\n",
    "class StatementFactCheckerSignature(dspy.Signature):\n",
    "    f\"\"\"Fact check the given statement and context into one of the following verdicts: {\", \".join(VERDICTS)}\"\"\"\n",
    "    statement = dspy.InputField(desc=\"Statement to evaluate\")\n",
    "    verdict = dspy.OutputField(desc=f\"Truthful classification of the statement into one of the following verdicts: {', '.join(main.VERDICTS)}\")\n",
    "\n",
    "class StatementFactChecker(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fact_check = dspy.ChainOfThought(StatementFactCheckerSignature)\n",
    "\n",
    "    def forward(self, statement: str):\n",
    "        result = self.fact_check(statement=statement)\n",
    "        verdict = result[\"verdict\"]\n",
    "        reasoning = result[\"reasoning\"]\n",
    "        return verdict, reasoning\n",
    "    \n",
    "baseline = StatementFactChecker()\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df_1 = pd.read_pickle(output_file)\n",
    "\n",
    "# If baseline results column doesn't exist, create it\n",
    "if f'{model}_baseline_results' not in df_1.columns: df_1[f'{model}_baseline_results'] = None\n",
    "df_1[f'{model}_baseline_results'] = df_1[f'{model}_baseline_results'].astype(object)\n",
    "\n",
    "for index in tqdm(range(len(df_1))):\n",
    "    # If results already exist, skip if num_trials is reached\n",
    "    if df_1.loc[index, f'{model}_baseline_results'] is not None: \n",
    "        if len(df_1.loc[index, f'{model}_baseline_results']) == num_trials:\n",
    "            print(f\"Skipping row {index} because {num_trials}/{num_trials} trials completed\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Running row {index} because {len(df_1.loc[index, f'{model}_baseline_results'])}/{num_trials} trials completed\")\n",
    "            results = df_1.loc[index, f'{model}_baseline_results']\n",
    "    else: \n",
    "        print(f\"Running row {index} because 0/{num_trials} trials completed\")\n",
    "        results = []\n",
    "\n",
    "    for trial_i in tqdm(range(num_trials-len(results)), leave=False):\n",
    "        statement = df_1.iloc[index]['claim']\n",
    "        gold_verdict = df_1.iloc[index]['label']\n",
    "\n",
    "        verdict, reasoning = baseline(f\"{statement}\")\n",
    "        print_final_result(statement, verdict, 'N/A', reasoning, gold_verdict)\n",
    "\n",
    "        results.append({\n",
    "            'verdict': verdict,\n",
    "            'reasoning': reasoning\n",
    "        })\n",
    "\n",
    "        df_1.at[index, f'{model}_baseline_results'] = results\n",
    "        df_1.to_pickle(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'results_v2_gemini.pkl'\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_pickle(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://content.govdelivery.com/accounts/WIGOV/bulletins/3c856e9'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gemini_results'][0][0]['claims'][0].components[2].answer.citations[0].source_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../pipeline_v2/')\n",
    "import main \n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import eval data\n",
    "df_gemini = pd.read_pickle('results_v2_gemini.pkl')\n",
    "df_mistral = pd.read_pickle('results_v2_mistral.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics (e.g., F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ordinal mapping\n",
    "VERDICT_MAP = {\n",
    "    \"TRUE\": 5,\n",
    "    \"MOSTLY TRUE\": 4,\n",
    "    \"HALF TRUE\": 3,\n",
    "    \"MOSTLY FALSE\": 2,\n",
    "    \"FALSE\": 1,\n",
    "    \"UNVERIFIABLE\": 0,\n",
    "    # Weird cases\n",
    "    \"PANTS ON FIRE\": 1, # Pants on fire is the same as false\n",
    "    \"MOSTLY UNVERIFIABLE\": 0,\n",
    "    \"INDIFFERENT\": 0,\n",
    "    'MOSTLY HALF TRUE': 4,\n",
    "    'PARTIALLY TRUE': 3,\n",
    "}\n",
    "\n",
    "def get_results(df):\n",
    "    '''Convert verdicts to numeric values'''\n",
    "    true_verdicts = [VERDICT_MAP[v] for v in df['verdict']]\n",
    "    pass1_preds_baseline = [VERDICT_MAP[v] for v in df['baseline_pass1_verdict']]\n",
    "    pass1_preds_pipeline = [VERDICT_MAP[v] for v in df['pipeline_pass1_verdict']]\n",
    "    pass3_preds_baseline = [VERDICT_MAP[v] for v in df['baseline_pass3_verdict']]\n",
    "    pass3_preds_pipeline = [VERDICT_MAP[v] for v in df['pipeline_pass3_verdict']]\n",
    "\n",
    "    cohen_base_1_idx = [i for i, e in enumerate(pass1_preds_baseline) if e == 0]\n",
    "    cohen_base_1_true = [e for i, e in enumerate(true_verdicts) if i not in cohen_base_1_idx]\n",
    "    cohen_base_1_pred = [e for i, e in enumerate(pass1_preds_baseline) if i not in cohen_base_1_idx]\n",
    "\n",
    "    cohen_pipe_1_idx = [i for i, e in enumerate(pass1_preds_pipeline) if e == 0]\n",
    "    cohen_pipe_1_true = [e for i, e in enumerate(true_verdicts) if i not in cohen_pipe_1_idx]\n",
    "    cohen_pipe_1_pred = [e for i, e in enumerate(pass1_preds_pipeline) if i not in cohen_pipe_1_idx]\n",
    "\n",
    "    cohen_base_3_idx = [i for i, e in enumerate(pass3_preds_baseline) if e == 0]\n",
    "    cohen_base_3_true = [e for i, e in enumerate(true_verdicts) if i not in cohen_base_3_idx]\n",
    "    cohen_base_3_pred = [e for i, e in enumerate(pass3_preds_baseline) if i not in cohen_base_3_idx]\n",
    "\n",
    "    cohen_pipe_3_idx = [i for i, e in enumerate(pass3_preds_pipeline) if e == 0]\n",
    "    cohen_pipe_3_true = [e for i, e in enumerate(true_verdicts) if i not in cohen_pipe_3_idx]\n",
    "    cohen_pipe_3_pred = [e for i, e in enumerate(pass3_preds_pipeline) if i not in cohen_pipe_3_idx]\n",
    "    \n",
    "    all_preds = [pass1_preds_baseline, pass1_preds_pipeline, pass3_preds_baseline, pass3_preds_pipeline]\n",
    "    cohen_true = [cohen_base_1_true, cohen_pipe_1_true, cohen_base_3_true, cohen_pipe_3_true]\n",
    "    cohen_pred = [cohen_base_1_pred, cohen_pipe_1_pred, cohen_base_3_pred, cohen_pipe_3_pred]\n",
    "    return true_verdicts, all_preds, cohen_true, cohen_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_true, gemini_preds, gemini_cohen_true , gemini_cohen_pred = get_results(df_gemini)\n",
    "mistral_true, mistral_preds, mistral_cohen_true , mistral_cohen_pred = get_results(df_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate confusion matrix and other metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "def generate_metrics(y_true, y_pred):\n",
    "    # Generate the cost matrix for 6 classes\n",
    "    # cost_matrix = generate_cost_matrix(6)\n",
    "\n",
    "    # Calculate the confusion matrix with the cost matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, sample_weight=None, labels=y_true)\n",
    "    # weighted_cm = cm * cost_matrix\n",
    "\n",
    "    # Calculate the weighted F1 score\n",
    "    balanced_acc = accuracy_score(y_true, y_pred)\n",
    "    weighted_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    weighted_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return cm, balanced_acc, weighted_precision, weighted_recall, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "### generate metrics (excluding unverifiables)\n",
    "cm_base1_gemini, acc_base1_gemini, p_base1_gemini, r_base1_gemini, f1_base1_gemini = generate_metrics(gemini_cohen_true[0], gemini_cohen_pred[0])\n",
    "cm_pipe1_gemini, acc_pipe1_gemini, p_pipe1_gemini, r_pipe1_gemini, f1_pipe1_gemini = generate_metrics(gemini_cohen_true[1], gemini_cohen_pred[1])\n",
    "cm_base3_gemini, acc_base3_gemini, p_base3_gemini, r_base3_gemini, f1_base3_gemini = generate_metrics(gemini_cohen_true[2], gemini_cohen_pred[2])\n",
    "cm_pipe3_gemini, acc_pipe3_gemini, p_pipe3_gemini, r_pipe3_gemini, f1_pipe3_gemini = generate_metrics(gemini_cohen_true[3], gemini_cohen_pred[3])\n",
    "\n",
    "cm_base1_mistral, acc_base1_mistral, p_base1_mistral, r_base1_mistral, f1_base1_mistral = generate_metrics(mistral_cohen_true[0], mistral_cohen_pred[0])\n",
    "cm_pipe1_mistral, acc_pipe1_mistral, p_pipe1_mistral, r_pipe1_mistral, f1_pipe1_mistral = generate_metrics(mistral_cohen_true[1], mistral_cohen_pred[1])\n",
    "cm_base3_mistral, acc_base3_mistral, p_base3_mistral, r_base3_mistral, f1_base3_mistral = generate_metrics(mistral_cohen_true[2], mistral_cohen_pred[2])\n",
    "cm_pipe3_mistral, acc_pipe3_mistral, p_pipe3_mistral, r_pipe3_mistral, f1_pipe3_mistral = generate_metrics(mistral_cohen_true[3], mistral_cohen_pred[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1920x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot metrics (excluding unverifiables)\n",
    "# Set fall color palette\n",
    "fall_colors = ['#0a9396',\n",
    "               '#D35400',  # Pumpkin orange\n",
    "               '#005f73',   # Deep purple\n",
    "               '#F39C12',]   # Golden yellow   \n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# create data \n",
    "df = pd.DataFrame([['pass@1 baseline', acc_base1_gemini, p_base1_gemini, r_base1_gemini, f1_base1_gemini], \n",
    "                   ['pass@1 pipeline', acc_pipe1_gemini, p_pipe1_gemini, r_pipe1_gemini, f1_pipe1_gemini], \n",
    "                   ['pass@3 baseline', acc_base3_gemini, p_base3_gemini, r_base3_gemini, f1_base3_gemini], \n",
    "                   ['pass@3 pipeline', acc_pipe3_gemini, p_pipe3_gemini, r_pipe3_gemini, f1_pipe3_gemini],\n",
    "                   ['pass@1 baseline_mistral', acc_base1_mistral, p_base1_mistral, r_base1_mistral, f1_base1_mistral], \n",
    "                   ['pass@1 pipeline_mistral', acc_pipe1_mistral, p_pipe1_mistral, r_pipe1_mistral, f1_pipe1_mistral], \n",
    "                   ['pass@3 baseline_mistral', acc_base3_mistral, p_base3_mistral, r_base3_mistral, f1_base3_mistral], \n",
    "                   ['pass@3 pipeline_mistral', acc_pipe3_mistral, p_pipe3_mistral, r_pipe3_mistral, f1_pipe3_mistral]], \n",
    "                  columns=['Team', 'Balanced Acc', 'Macro Precision', 'Macro Recall', 'Macro F1 Score']) \n",
    "\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "# plot grouped bar chart with fall colors\n",
    "df.plot(kind='bar', \n",
    "        stacked=False, \n",
    "        title='',\n",
    "        ax=ax,\n",
    "        width=0.8,\n",
    "        color=fall_colors)\n",
    "\n",
    "for bar in ax.patches:\n",
    "  # The text annotation for each bar should be its height.\n",
    "  bar_value = bar.get_height()\n",
    "  text = f'{bar_value:.2f}'\n",
    "  text = text[-3:]\n",
    "  # This will give the middle of each bar on the x-axis.\n",
    "  text_x = bar.get_x() + bar.get_width() / 2\n",
    "  # get_y() is where the bar starts so we add the height to it.\n",
    "  text_y = bar.get_y() + bar_value\n",
    "  # If we want the text to be the same color as the bar, we can\n",
    "  # get the color like so:\n",
    "  bar_color = bar.get_facecolor()\n",
    "  # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "          size=6)\n",
    "ax.set_xticks(df.index, [\"pass@1\\nbaseline\", \"pass@1\\npipeline\", \"pass@3\\nbaseline\", \"pass@3\\npipeline*\",\n",
    "                         \"pass@1\\nbaseline\", \"pass@1\\npipeline\", \"pass@3\\nbaseline\", \"pass@3\\npipeline**\"], \n",
    "              fontsize=8,\n",
    "              rotation=0)\n",
    "ax.set_title('Evaluation Metrics by LMs and Methods\\n(excluding unverifables)', fontsize=12)\n",
    "ax.set_ylabel('Metrics')\n",
    "ax.set_xlabel('LMs and methods')\n",
    "ax.xaxis.set_label_coords(0.5, -0.18)\n",
    "ax.tick_params(axis='y', which='major', labelsize=8)\n",
    "ax.text(1.5, -0.08, 'Gemini-1.5-Flash', fontsize=8, ha='center')\n",
    "ax.annotate('', xy=(0.5, -0.075), xytext=(0, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.annotate('', xy=(2.5, -0.075), xytext=(3, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "\n",
    "ax.text(5.5, -0.08, 'Mistral-7B-v0.3', fontsize=8, ha='center')\n",
    "ax.annotate('', xy=(4.5, -0.075), xytext=(4, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.annotate('', xy=(6.5, -0.075), xytext=(7, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.legend(loc=\"upper left\",\n",
    "         facecolor='white',\n",
    "         fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('eval.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "### generate metrics (including unverifiables)\n",
    "cm_base1_gemini, acc_base1_gemini, p_base1_gemini, r_base1_gemini, f1_base1_gemini = generate_metrics(gemini_true, gemini_preds[0])\n",
    "cm_pipe1_gemini, acc_pipe1_gemini, p_pipe1_gemini, r_pipe1_gemini, f1_pipe1_gemini = generate_metrics(gemini_true, gemini_preds[1])\n",
    "cm_base3_gemini, acc_base3_gemini, p_base3_gemini, r_base3_gemini, f1_base3_gemini = generate_metrics(gemini_true, gemini_preds[2])\n",
    "cm_pipe3_gemini, acc_pipe3_gemini, p_pipe3_gemini, r_pipe3_gemini, f1_pipe3_gemini = generate_metrics(gemini_true, gemini_preds[3])\n",
    "\n",
    "cm_base1_mistral, acc_base1_mistral, p_base1_mistral, r_base1_mistral, f1_base1_mistral = generate_metrics(mistral_true, mistral_preds[0])\n",
    "cm_pipe1_mistral, acc_pipe1_mistral, p_pipe1_mistral, r_pipe1_mistral, f1_pipe1_mistral = generate_metrics(mistral_true, mistral_preds[1])\n",
    "cm_base3_mistral, acc_base3_mistral, p_base3_mistral, r_base3_mistral, f1_base3_mistral = generate_metrics(mistral_true, mistral_preds[2])\n",
    "cm_pipe3_mistral, acc_pipe3_mistral, p_pipe3_mistral, r_pipe3_mistral, f1_pipe3_mistral = generate_metrics(mistral_true, mistral_preds[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1920x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot metrics (including unverifiables)\n",
    "# Set fall color palette\n",
    "fall_colors = ['#0a9396',\n",
    "               '#D35400',  # Pumpkin orange\n",
    "               '#005f73',   # Deep purple\n",
    "               '#F39C12',]   # Golden yellow   \n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# create data \n",
    "df = pd.DataFrame([['pass@1 baseline', acc_base1_gemini, p_base1_gemini, r_base1_gemini, f1_base1_gemini], \n",
    "                   ['pass@1 pipeline', acc_pipe1_gemini, p_pipe1_gemini, r_pipe1_gemini, f1_pipe1_gemini], \n",
    "                   ['pass@3 baseline', acc_base3_gemini, p_base3_gemini, r_base3_gemini, f1_base3_gemini], \n",
    "                   ['pass@3 pipeline', acc_pipe3_gemini, p_pipe3_gemini, r_pipe3_gemini, f1_pipe3_gemini],\n",
    "                   ['pass@1 baseline_mistral', acc_base1_mistral, p_base1_mistral, r_base1_mistral, f1_base1_mistral], \n",
    "                   ['pass@1 pipeline_mistral', acc_pipe1_mistral, p_pipe1_mistral, r_pipe1_mistral, f1_pipe1_mistral], \n",
    "                   ['pass@3 baseline_mistral', acc_base3_mistral, p_base3_mistral, r_base3_mistral, f1_base3_mistral], \n",
    "                   ['pass@3 pipeline_mistral', acc_pipe3_mistral, p_pipe3_mistral, r_pipe3_mistral, f1_pipe3_mistral]], \n",
    "                  columns=['Team', 'Balanced Acc', 'Macro Precision', 'Macro Recall', 'Macro F1 Score']) \n",
    "\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "# plot grouped bar chart with fall colors\n",
    "df.plot(kind='bar', \n",
    "        stacked=False, \n",
    "        title='',\n",
    "        ax=ax,\n",
    "        width=0.8,\n",
    "        color=fall_colors)\n",
    "\n",
    "for bar in ax.patches:\n",
    "  # The text annotation for each bar should be its height.\n",
    "  bar_value = bar.get_height()\n",
    "  text = f'{bar_value:.2f}'\n",
    "  text = text[-3:]\n",
    "  # This will give the middle of each bar on the x-axis.\n",
    "  text_x = bar.get_x() + bar.get_width() / 2\n",
    "  # get_y() is where the bar starts so we add the height to it.\n",
    "  text_y = bar.get_y() + bar_value\n",
    "  # If we want the text to be the same color as the bar, we can\n",
    "  # get the color like so:\n",
    "  bar_color = bar.get_facecolor()\n",
    "  # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "  ax.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "          size=6)\n",
    "ax.set_xticks(df.index, [\"pass@1\\nbaseline\", \"pass@1\\npipeline\", \"pass@3\\nbaseline\", \"pass@3\\npipeline*\",\n",
    "                         \"pass@1\\nbaseline\", \"pass@1\\npipeline\", \"pass@3\\nbaseline\", \"pass@3\\npipeline**\"], \n",
    "              fontsize=8,\n",
    "              rotation=0)\n",
    "ax.set_title('Evaluation Metrics by LMs and Methods', fontsize=12)\n",
    "ax.set_ylabel('Metrics')\n",
    "ax.set_xlabel('LMs and methods')\n",
    "ax.xaxis.set_label_coords(0.5, -0.18)\n",
    "ax.tick_params(axis='y', which='major', labelsize=8)\n",
    "ax.text(1.5, -0.08, 'Gemini-1.5-Flash', fontsize=8, ha='center')\n",
    "ax.annotate('', xy=(0.5, -0.075), xytext=(0, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.annotate('', xy=(2.5, -0.075), xytext=(3, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "\n",
    "ax.text(5.5, -0.08, 'Mistral-7B-v0.3', fontsize=8, ha='center')\n",
    "ax.annotate('', xy=(4.5, -0.075), xytext=(4, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.annotate('', xy=(6.5, -0.075), xytext=(7, -0.05), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "ax.legend(loc=\"upper left\",\n",
    "         facecolor='white',\n",
    "         fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('eval.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.74      0.48      0.58        42\n",
      "           2       0.17      0.22      0.19        27\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.17      0.15      0.16        27\n",
      "           5       0.33      0.04      0.07        27\n",
      "\n",
      "    accuracy                           0.21       150\n",
      "   macro avg       0.24      0.15      0.17       150\n",
      "weighted avg       0.33      0.21      0.24       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/factchecker/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "### create classification report for each method & lm (if needed for appendix)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# example code for comparing with gemini pass@1 baseline\n",
    "print(classification_report(gemini_true, gemini_preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first (pass@1) and best of 3 (pass@3) verdicts\n",
    "# Define the ordinal mapping\n",
    "VERDICT_MAP = {\n",
    "    \"TRUE\": 5,\n",
    "    \"MOSTLY TRUE\": 4,\n",
    "    \"HALF TRUE\": 3,\n",
    "    \"MOSTLY FALSE\": 2,\n",
    "    \"FALSE\": 1,\n",
    "    \"UNVERIFIABLE\": 0,\n",
    "    # Weird cases\n",
    "    \"PANTS ON FIRE\": 1, # Pants on fire is the same as false\n",
    "    \"MOSTLY UNVERIFIABLE\": 0,\n",
    "    \"INDIFFERENT\": 0,\n",
    "    'MOSTLY HALF TRUE': 4,\n",
    "    'PARTIALLY TRUE': 3,\n",
    "}\n",
    "\n",
    "def map_df(model):\n",
    "    '''map the dataframe to extract the verdicts and calculate the errors'''\n",
    "    df = pd.read_pickle(f'results_v2_{model}.pkl')\n",
    "\n",
    "    # Extract the verdicts from the results columns\n",
    "    df['pred_verdicts_baseline'] = df[f'{model}_baseline_results'].apply(lambda x: [result['verdict'] for result in x] if x else None)\n",
    "    df['pred_verdicts_pipeline'] = df[f'{model}_pipeline_results'].apply(lambda x: [result['verdict'] for result in x] if x else None)\n",
    "\n",
    "    # Clean up verdicts with extraneous text (not the cleanest/fastest method but it works)\n",
    "    # \"UNVERIFIABLE (as of the time of writing, the statement cannot be definitively verified or refuted)\": 0,\n",
    "    # 'MOSTLY TRUE - Kelly Ayotte accurately mentioned a relevant bill regarding sanctuary states, but it is unclear if Joyce Craig opposed the bill since she was no longer in office when it was introduced.': 4,\n",
    "    for i, row in df.iterrows():\n",
    "        cols = ['pred_verdicts_baseline', 'pred_verdicts_pipeline']\n",
    "        for col in cols: \n",
    "            verdicts = row[col]\n",
    "            if verdicts:\n",
    "                verdicts = [v.split(':')[0].split('-')[0].split('(')[0].split('.')[0].strip() if len(v) > 12 else v for v in verdicts]\n",
    "            df.at[i, col] = verdicts\n",
    "\n",
    "    df.dropna(subset=['pred_verdicts_pipeline', 'pred_verdicts_baseline'], inplace=True)\n",
    "\n",
    "    df[['verdict', 'pred_verdicts_baseline', 'pred_verdicts_pipeline']]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        true_val = VERDICT_MAP[df.iloc[i]['verdict']]\n",
    "        \n",
    "        # Get pass@1 predictions\n",
    "        baseline_pred = df['pred_verdicts_baseline'][i][0]\n",
    "        pipeline_pred = df['pred_verdicts_pipeline'][i][0]\n",
    "        # Set pass@1 predictions to its own column\n",
    "        df.at[i, 'baseline_pass1_verdict'] = baseline_pred\n",
    "        df.at[i, 'pipeline_pass1_verdict'] = pipeline_pred\n",
    "\n",
    "        # Calculate pass@1 errors\n",
    "        df.at[i, 'baseline_pass1_MSE'] = (true_val - VERDICT_MAP[baseline_pred])**2 if baseline_pred != \"UNVERIFIABLE\" else None\n",
    "        df.at[i, 'pipeline_pass1_MSE'] = (true_val - VERDICT_MAP[pipeline_pred])**2 if pipeline_pred != \"UNVERIFIABLE\" else None\n",
    "        \n",
    "        # Get pass@3 predictions\n",
    "        sorted_baseline_results = sorted(df.at[i, 'pred_verdicts_baseline'], key=lambda x: (VERDICT_MAP[x] - VERDICT_MAP[df.at[i, 'verdict']])**2 if x != 'UNVERIFIABLE' else 100)\n",
    "        sorted_pipeline_results = sorted(df.at[i, 'pred_verdicts_pipeline'], key=lambda x: (VERDICT_MAP[x] - VERDICT_MAP[df.at[i, 'verdict']])**2 if x != 'UNVERIFIABLE' else 100)\n",
    "        best_baseline_pred = sorted_baseline_results[0]\n",
    "        best_pipeline_pred = sorted_pipeline_results[0]\n",
    "\n",
    "        # Set pass@3 predictions\n",
    "        df.at[i, 'baseline_pass3_verdict'] = best_baseline_pred\n",
    "        df.at[i, 'pipeline_pass3_verdict'] = best_pipeline_pred\n",
    "        \n",
    "        # Set pass@3 errors\n",
    "        df.at[i, 'baseline_pass3_MSE'] = (true_val - VERDICT_MAP[best_baseline_pred])**2 if best_baseline_pred != \"UNVERIFIABLE\" else None\n",
    "        df.at[i, 'pipeline_pass3_MSE'] = (true_val - VERDICT_MAP[best_pipeline_pred])**2 if best_pipeline_pred != \"UNVERIFIABLE\" else None\n",
    "\n",
    "    # Now you can sort by errors, for example:\n",
    "    print(\"\\nTop 5 statements with highest pipeline pass@1 errors:\")\n",
    "    display(df.nlargest(5, 'pipeline_pass1_MSE')[['statement', 'verdict', 'pred_verdicts_pipeline', 'pipeline_pass1_MSE']])\n",
    "\n",
    "    print(\"\\nTop 5 statements with highest pipeline pass@3 errors:\")\n",
    "    display(df.nlargest(5, 'pipeline_pass3_MSE')[['statement', 'verdict', 'pred_verdicts_pipeline', 'pipeline_pass3_MSE']])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 statements with highest pipeline pass@1 errors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>verdict</th>\n",
       "      <th>pred_verdicts_pipeline</th>\n",
       "      <th>pipeline_pass1_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>“Nearly 90% of all UW graduates stay in Wiscon...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[FALSE, UNVERIFIABLE, FALSE]</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tim Walz said he carried weapons in war, but “...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY TRUE, MOSTLY FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>“Remember in 2020, 55 of the biggest companies...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[MOSTLY TRUE, MOSTLY FALSE, MOSTLY TRUE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Says opponent Eric Hovde “supports a $4 trilli...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY TRUE, MOSTLY TRUE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>“There was a bill to basically create a ban to...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY TRUE, UNVERIFIABLE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             statement      verdict  \\\n",
       "147  “Nearly 90% of all UW graduates stay in Wiscon...         TRUE   \n",
       "38   Tim Walz said he carried weapons in war, but “...         TRUE   \n",
       "119  “Remember in 2020, 55 of the biggest companies...        FALSE   \n",
       "13   Says opponent Eric Hovde “supports a $4 trilli...  MOSTLY TRUE   \n",
       "18   “There was a bill to basically create a ban to...  MOSTLY TRUE   \n",
       "\n",
       "                        pred_verdicts_pipeline  pipeline_pass1_MSE  \n",
       "147               [FALSE, UNVERIFIABLE, FALSE]                16.0  \n",
       "38   [MOSTLY FALSE, MOSTLY TRUE, MOSTLY FALSE]                 9.0  \n",
       "119   [MOSTLY TRUE, MOSTLY FALSE, MOSTLY TRUE]                 9.0  \n",
       "13    [MOSTLY FALSE, MOSTLY TRUE, MOSTLY TRUE]                 4.0  \n",
       "18   [MOSTLY FALSE, MOSTLY TRUE, UNVERIFIABLE]                 4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 statements with highest pipeline pass@3 errors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>verdict</th>\n",
       "      <th>pred_verdicts_pipeline</th>\n",
       "      <th>pipeline_pass3_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>“Nearly 90% of all UW graduates stay in Wiscon...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[FALSE, UNVERIFIABLE, FALSE]</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>“Dave McCormick is fully against abortion.”</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>[MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>“400,000 workers are now in a union that were ...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, UNVERIFIABLE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>“Even before the pandemic, America went into a...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>[UNVERIFIABLE, UNVERIFIABLE, MOSTLY FALSE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>\"[The Trump Administration] added more to the ...</td>\n",
       "      <td>HALF TRUE</td>\n",
       "      <td>[UNVERIFIABLE, FALSE, UNVERIFIABLE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             statement       verdict  \\\n",
       "147  “Nearly 90% of all UW graduates stay in Wiscon...          TRUE   \n",
       "32         “Dave McCormick is fully against abortion.”  MOSTLY FALSE   \n",
       "35   “400,000 workers are now in a union that were ...   MOSTLY TRUE   \n",
       "41   “Even before the pandemic, America went into a...   MOSTLY TRUE   \n",
       "115  \"[The Trump Administration] added more to the ...     HALF TRUE   \n",
       "\n",
       "                         pred_verdicts_pipeline  pipeline_pass3_MSE  \n",
       "147                [FALSE, UNVERIFIABLE, FALSE]                16.0  \n",
       "32     [MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]                 4.0  \n",
       "35   [MOSTLY FALSE, MOSTLY FALSE, UNVERIFIABLE]                 4.0  \n",
       "41   [UNVERIFIABLE, UNVERIFIABLE, MOSTLY FALSE]                 4.0  \n",
       "115         [UNVERIFIABLE, FALSE, UNVERIFIABLE]                 4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini_df = map_df('gemini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 statements with highest pipeline pass@1 errors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>verdict</th>\n",
       "      <th>pred_verdicts_pipeline</th>\n",
       "      <th>pipeline_pass1_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>\"Pharmaceutical medicine has its place, but no...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[MOSTLY TRUE, MOSTLY TRUE, MOSTLY FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>\"We’ve had 12 elections in 24 years in Wiscons...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>\"In February 2024, Nikki Haley lost the Nevada...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>\"Former U.S. President Donald Trump's margin o...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“The Universities of Wisconsin are 43rd out of...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[HALF TRUE, MOSTLY TRUE, MOSTLY TRUE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             statement verdict  \\\n",
       "108  \"Pharmaceutical medicine has its place, but no...   FALSE   \n",
       "123  \"We’ve had 12 elections in 24 years in Wiscons...    TRUE   \n",
       "132  \"In February 2024, Nikki Haley lost the Nevada...    TRUE   \n",
       "138  \"Former U.S. President Donald Trump's margin o...   FALSE   \n",
       "4    “The Universities of Wisconsin are 43rd out of...    TRUE   \n",
       "\n",
       "                         pred_verdicts_pipeline  pipeline_pass1_MSE  \n",
       "108    [MOSTLY TRUE, MOSTLY TRUE, MOSTLY FALSE]                 9.0  \n",
       "123  [MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]                 9.0  \n",
       "132         [MOSTLY FALSE, MOSTLY FALSE, FALSE]                 9.0  \n",
       "138    [MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]                 9.0  \n",
       "4         [HALF TRUE, MOSTLY TRUE, MOSTLY TRUE]                 4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 statements with highest pipeline pass@3 errors:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>verdict</th>\n",
       "      <th>pred_verdicts_pipeline</th>\n",
       "      <th>pipeline_pass3_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>\"We’ve had 12 elections in 24 years in Wiscons...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>\"In February 2024, Nikki Haley lost the Nevada...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, FALSE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>\"Former U.S. President Donald Trump's margin o...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>[MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>“Less than three months ago, Kamala Harris and...</td>\n",
       "      <td>MOSTLY FALSE</td>\n",
       "      <td>[MOSTLY TRUE, UNVERIFIABLE, MOSTLY TRUE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>\"Typically you have three to four debates in a...</td>\n",
       "      <td>MOSTLY TRUE</td>\n",
       "      <td>[MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             statement       verdict  \\\n",
       "123  \"We’ve had 12 elections in 24 years in Wiscons...          TRUE   \n",
       "132  \"In February 2024, Nikki Haley lost the Nevada...          TRUE   \n",
       "138  \"Former U.S. President Donald Trump's margin o...         FALSE   \n",
       "29   “Less than three months ago, Kamala Harris and...  MOSTLY FALSE   \n",
       "31   \"Typically you have three to four debates in a...   MOSTLY TRUE   \n",
       "\n",
       "                         pred_verdicts_pipeline  pipeline_pass3_MSE  \n",
       "123  [MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]                 9.0  \n",
       "132         [MOSTLY FALSE, MOSTLY FALSE, FALSE]                 9.0  \n",
       "138    [MOSTLY TRUE, MOSTLY TRUE, UNVERIFIABLE]                 9.0  \n",
       "29     [MOSTLY TRUE, UNVERIFIABLE, MOSTLY TRUE]                 4.0  \n",
       "31   [MOSTLY FALSE, MOSTLY FALSE, MOSTLY FALSE]                 4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistral_df = map_df('mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Baseline: n=89/150, excluded 61 unverifiable) (Pipeline: n=101/150, excluded 49 unverifiable)\n",
      "(Baseline: n=96/150, excluded 54 unverifiable) (Pipeline: n=121/150, excluded 29 unverifiable)\n",
      "(Baseline: n=72/150, excluded 78 unverifiable) (Pipeline: n=135/150, excluded 15 unverifiable)\n",
      "(Baseline: n=89/150, excluded 61 unverifiable) (Pipeline: n=147/150, excluded 3 unverifiable)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAC3QAAAlQCAYAAAB+HpOQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAuIwAALiMBeKU/dgABAABJREFUeJzs3QncTOX///HLvm+JomSJNkoSESGJdlJpkdCmhValtNAi7VFalHZtZImQpFRUtkqFrEmSfd+3+T/e1/d/+8099zlzz3Jmzszcr+fjMY8y577PXPecc65znev6XJ8rXyAQCBgAAAAAAAAAAAAAAAAAAAAAQNLlT/5HAgAAAAAAAAAAAAAAAAAAAACEgG4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxDQDQAAAAAAAAAAAAAAAAAAAAA+IaAbAAAAAAAAAAAAAAAAAAAAAHxCQDcAAAAAAAAAAAAAAAAAAAAA+ISAbgAAAAAAAAAAAAAAAAAAAADwCQHdAAAAAAAAAAAAAAAAAAAAAOATAroBAAAAAAAAAAAAAAAAAAAAwCcEdAMAAAAAAAAAAAAAAAAAAACATwjoBgAAAAAAAAAAAAAAAAAAAACfENANAAAAAAAAAAAAAAAAAAAAAD4hoBsAAAAAAAAAAAAAAAAAAAAAfEJANwAAAAAAAAAAAAAAAAAAAAD4hIBuAAAAAAAAAAAAAAAAAAAAAPAJAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAAAAAAAAAAAAAAAAAAAPiEgG4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBf36YAAAAAAAkFMgEDCTJ0+O+OcrV65sTjjhhISWCUBq1hWrV682hx9+uN9FASKybt06U65cOVOgQAG/iwIAAIA0sWvXLjN16tSIf54+EgDIPBs3bjSzZ8+O+Odr1KhhXwAApKN8AY3+AAAAAACAlLBv3z5TqFChiH++c+fO5p133klomQCklvHjx5sHH3zQXHTRRaZv375+FweIiO5V/fr1M3369DEdO3Y0+fLl87tIAAAASHHLli0z1atXj/jn6SMBgMwzZcoUc+aZZ0b88+p3oL8MAJCuyNANAAAApJH169ebn376yfz88882I8Hff/9tNm/ebF9btmwxhQsXNiVLlrSv8uXL2wGPo48+2mYjOP744029evVM8eLF/f4zAABADFasWGG6d+9uPvvsM/tvBXQjcbZu3WqmT59u21xqey1evPhgu0svZZouVaqUbXcp83S1atUOZoE67rjjzCmnnGLKlCnj95+RUvQddurUyQwePNi+8kr2xJUrV9rMkmrHL1q0yCxdutRmLN++fbvNOqlzqGzZsvZ8OeSQQ8yxxx5r6tSpc/B16KGH+v0nAAAAAAAAAAASjIBuAAAQky5duph333036t87/fTTzbRp04zf2rdvb0aNGhXV71StWtVmBAH8yNY7YcIE89Zbb5lx48aZvXv3hv3ZHTt2mDVr1thAkZkzZ2bbrsAjBXafeuqpplGjRqZly5amVq1aSfgrAGSyk08+2cyZM8ekom+++ca0aNHC72IAcRs9erS57rrrzIYNGw6+l1uGY2Wm69q1a9SfpeBSTRpTYKmfXnzxRXP77bdH/Xt//fWXDa6OhRYzVOYntbtGjBhhdu7c6fqzapMpGHft2rX2MxX0HUzHp2bNmrbdddppp9l2l4Jzo81M/euvv9pJeakommek4L9bwc36Xl544QXTrVs3k4lWrVpl3n//ffPxxx/nODdCZU0SyKJzMJjOo7PPPtu+lJVMwd8A0oMmcdx9993m66+/NgULFjQXXHCBef75503FihX9LhoQsTvuuMMMHDgwYfvPnz+/nSRXunRpO7lJk+PU9tFL7SdtA5JBzwI611966SU7mVZtsAceeMBcddVVfhcNSFvKkvzII48k7P6hZyNNsNZL4x4ag2zcuLE56aSTPFsVSmMsvXr1Mj/++KO9J3Xo0ME8+eSTtu8EmUd9PY8++qjtU9M4m84lrTbWunVrv4sGAMgj8vtdAAAAkLf88MMPdhDLT3/88YcNiAHSgToJNYClDJw6b8MFc0di//799hpQZ9RNN91kjjnmGBuIo2CvL7/80rNyA4idAj00iBj6evvtt/0uGgCfaCDp4osvzhbM3apVK3Pbbbcl5PO2bdtmBgwYYPy0Z88e8/TTTyf1M+fPn2+aNWtmA4eGDh0aNpg7Eqq7Fcj30Ucf2UAoDQIefvjh5sorrzTDhg0zec3ll1+eLRhG36/aozfeeKOdlJgpli9fbm6++WY7qeDee+/NNZg70szmr776qp2YrGzdClJQMLwyfwNI7fpA1+vYsWNtRn5N3Pjggw/MGWecYVeBAPA/Bw4csNfHP//8Y/usxowZY4P/2rVrZ9tOWt1D/WPIm9SmSlYfiQI277zzTpskQ88j8+bNMx07djSvv/66558FwJv7h/pJlixZYmbNmmUn1OpZTIknFNw9aNCguNtcep5TP4ESRmRN6H755ZfNueeea8dakBxK1uF0L+jTp4/nn3X11Vebxx9/3E7s0b1A55aOt5IuAQCQDGToBgAASffYY4/ZIAm/aCa1HvSBVKaglp49e9qMMOqYDKXsEg0bNjT169c3tWvXthk0lcVIv7dlyxazadMmO+igjLnKIKGOp3CDzArw3rhxI1kGAABIIWqzajBy8ODB2d6/9dZbbcC1JoAkMju2MoqqfeEHBWj8+++/Sfs8ZdfSQKBbm6lu3bqmQYMGNii7fPny9nvR8dHgsAKQFixYYDNpq92loD03yu6kjM0KHldWr7ykaNGiNpDxhBNOMA8++ODB99944w0bmDxy5EhTuHBhk67UDleQtTLQacUcJ1oZp3nz5ua4444zVapUsRnd1NZX213nkTLj6xxS0IDa9E4UNKCgNr30vKD9KVD+kksusVnpAKSOZ5991qxbty7H+wsXLjRDhgyxQYMAwtM9VRPt1Ia49tprzTPPPMP9DgmhdrpWUHBy//332/Mvkc9fALylZ/QePXqYhx9+2LzyyivmiiuuiGk/er5TIHcorTqlCUiafI/MoWdxpwn4em6/7777bGA3AACJxlMHAACIiTKNaZZyqNWrVzu+H0zLR0+bNs00adLEJJsy5OWWDe+ee+5xDGotVqxYAksGZA8GUUCGOgRDFSlSxNx11112aXpl1o7E+vXrbWZIZblUtiMA8Nrtt99u2wChhg8f7klmUiCvUlbn0GDuG264wWaZikSbNm3MpEmTHLepze503WZRcKkmlgUH3iazLfTUU0+F/Rm119Vud6JMjtFQgLwGeJ2Wb1b2aA0CKwg50uzmn376qQ3imzt3rvFC5cqVTf/+/V2DS9KNlq1XUHJwJq1x48bZAXbdNwoUKGDSjQLSL7vsMrsiVagSJUrYtrteWh0nEhos/uKLL2w2yM8//9w185t+Tpni9FJAgSZpAkgdf/75Z0zbgFRzyy23mAsuuCDH+1rpTQHWTtQW0oQ4N1kT4/TShCYlJNC9TEG1bj//5ptv2n7l7777zraPAC9pRRS3NpcyACsrb6VKlZJeLiDdXXPNNaZp06ae3kNE9w+Ne+ja1fPQ7NmzHa9hJbHRKlkaa9FkYj2fRYP2XN7C8QYApAICugEAQEwU0OAU1LBs2bKIs3RrgDrZnnjiCcdsx8H0d2kJe8DPwEinYG5l9FNAR6SBIFmURbJ79+6mS5cuNtMmy4QC8FrXrl1dO7rdAroTtVqGluD89ttvE7JvIJkGDhxos2SHBmg7BR67UcCBW9CBsiXnRlnAFVSuLMLJpAyMf/31V9if0d/lRZtdA8hO32nFihVte+y0006Lan/6rtTm0tLsWqL30UcfjbuMKosyQUUT0N25c+eEBPfqea969epx70dZ0rSU/bvvvnvwvVGjRtmM08pynU50nzv//PPNqlWrcmxr3769nRgRbdCZJhOcd9559vXLL7/YjJDKAA8gvegZ3m1iVc2aNZNeHiBW6ody6otasWKF6+9oRTk9m0U7qW/ChAl28tfvv//u+DNLliyxbUAFf2u1OsArNWrUsG0wp7EDrc5ToUIFX8oFZMK1pVei7yEK7NYzePAzZjAlvNFKpRMnTowqqFvtOSWKckJ7LvPoeLvheAMAkiV/0j4JAAAgiDpNtJR0Min4QEt0AqlMmSmcgoqOOOIIm2ki2mDu0AAjZflUlkkAAJC6vv/+exvYGpp1+uOPP07qMt/KdvXqq6+aZFIAhSZhJoMyaCtgyKnNNHny5KiDuYMVKlTIPPLIIzazGJwpO1roJGFNItBAe7qYPn26Oeuss3IEc+fLl88e+xEjRsSdQbRevXr22fm6666Ls7QAkk0TqsuWLZvjfa22pRU3AGSndu6FF15os6xqlRQ38+fPt5PDAC/peUsJMZz07ds3qc9hAKKnYFtNav7www9dJ7Br5WBlDI/GQw89ZAoXLpzj/VNPPdVcfPHFMZcXqalBgwbmoosuyvG+nvE1YQAAgGQgoBsAAPhGWbqT6cknn7SZXoBU5pZ9UQEhCur2ggJlFBgCAABSz7Zt22x25dB26/PPP+8YFJZozz33nNm5c2fSPm/YsGFm4cKFSfksDczu3bs3x/sK8q5Tp45nwXxOg4H4X9C704SBW265xfz7778m1SmYTFnzN23alGPbyy+/bI+9VxRApFV2VDcASB/KRvnDDz/YLP7Fixc3pUuXNh06dLCZhf24pwPp1kY499xzXX9GCQsWLFiQ1HIh82mlmKefftquSKPz8NhjjzVvvvmmXbUIQHq48sorw65QOnLkSNcs3k4aNWpkE+00a9bMFClSxK6Gqol5WoWFiR6ZSf1SGqfT5GzdCzSW9tlnnxHADwBIGgK6AQBAQpUqVcq1U+Pzzz83c+bMSUo5FBAQvOQ4S3IiFf300092SfVQCuS+/PLLPfscLR86cOBAz/YHAAC88+CDD5q//vor23vKAKxByURyax+vXr3aZlJOhkAgkC07dyLb7Ho+0IBcKGXecsvMF09gCAO9zjQoHpohTQHSXh8Dr23YsMFmEN28eXOObVoN5+abb/b8M9WG17V44okner5vAIlz/PHH2/6v7du32zrjk08+MUceeaTfxQJSnu57mlio/zrR5EdWYoTXdL7dc889ZunSpWbPnj3mzz//NNdee63fxQIQpU6dOpnLLrvMdbsmce/evTvi/Z1++unm22+/Nbt27TLr1q2zAeNMzstcCtzv37+/7TfSveDnn3+2z/8AACQLAd0AACChFIRx9dVXuwZsJGuJqmeeeSZbB83tt9+elM8FojF69GjH95WRSEu6eemMM84wxx13nKf7BAAA8VGWwVdeecVxsDHRbrvtNtdtylIXzWBnrBRg/fvvvyelzT527Fhz4MABxwDjkiVLep6hVUH5cNa7d+8cbV21i5UFLVXdeOONZsmSJTneVxZHrayTKMoONmTIEFOgQIGEfQYAAKk0IUL9V24mTJiQ1PIAANLH/fff77pNgbqffvppUssDAAAQKQK6AQBAUjpO3LKpjBgxwi5VnUhr1qzJtsRakyZNbKAGkGp+/PFHx/ePPvrohHxeuCwVAAAg+R577DGzd+/ebO/Vrl3bnHnmmQn/7I4dO9qlxd0GO99+++2ElyF4sqdW+lGm40xpd3Xo0CEh+80ECoJu1apVjvcffvhhk4o+/vhj+xzrZNCgQTbLeyI1bNjQXHrppQn9DAAAUkWLFi1ctyljprKlAgAQql69euakk05y3T58+PCklgcAACBSBHQDAICEO+aYY1wDGJSlu1+/fgn9fC3PuXPnzqRmOARisWjRIsf3y5cvn5DPa9y4cUL2CwB+atq0qWnbtu3B16GHHup3kYCIrF692nFA8dZbb03K5xcsWND06tXLdftTTz1ll7ZPFGVYnD179sF/33zzzaZcuXIJ+zzaXbEpXrx4tjq2devWnuy3e/fuOd6bOnWq+fXXX00q0XNlz549Hbdp4rBTYHoiJKteAADAbzVr1nTdpn5lJfIAAMDtGc3Nd999l9SyAAAARIqAbgAAkBQKog5dRjs4w9nixYsT8rkbNmwwr7766sF/n3LKKebcc89NyGcBXpyvTrZv356Qz6tTp05C9gsAflKG39GjRx98UdchXQwePNjs2bMn23ta5SaZK2p07drVHHHEEY7bli1bZt5///2EfXbwJM+iRYuau+66y2RSu6tWrVqmSJEiJt1VrFgxWx0bvBJSPPSMVrp06Rzvv/TSSyaVvPjiizZjvZM77rgjaeU444wzzAknnJC0zwMAwC+5TbZbu3Zt0soCAEgvJ598suu2jRs3mlWrViW1PAAAAJEgoBsAACSFgqkuuugix2379+83TzzxREI+d+DAgWbr1q0H/927d++EfA7gBV0LTv7++++EfF6VKlVs5sPvv//evvr375+QzwEAAOHt3bvXvPbaazneP+2005KaZb5w4cKu2YdFbQW39ko8vvnmGzNt2rSD/77++uvNYYcdZjKp3aUM6MHtrnfeeSchn5OuChUqZNq0aZPj/Q8//NCsX7/epMp1+vzzzztuO+SQQ1yfdxPF6fsCACCvSeQKMgCA9KbntHBWrlyZtLIAAABEqmDEPwkAABCnBx980Hz22WeO24YOHWr69Oljqlat6tnnbdmyJVtGt+OPP960b9/e+G3JkiVm/PjxNqDjzz//NCtWrDDbtm2zy4RqCfPDDz/cVK9e3Zx66qmmRYsWpnnz5jYAJFF2795tg0q0xP0ff/xh5s2bZ7PbbN682ezYscNmylPHl14nnniiXaauadOmNstgqtB398MPP9i/Y/78+TbjojJaKgBKwUAqs77HkiVLmlRWtmxZs27dOscgJz+WHfRiUE3nul46v/766y97Xep81nml8/ykk06yGRnPOeccm40TmX2tZmV4nTx5svn9999tp7km3SiAUOUvU6aMOeqoo8yxxx5rX1paOZb6L9Xq2SzK+jJixAjz1Vdfmblz59p/69ipTDpuOlYq0/nnn2+PHdKPrstvv/3WXqNa+lvnneq2rPNb513W+V2jRg3X1UtiNWvWLHs/1Pml+4kyDZcoUcJ+vu4xRx99tP3sY445JuY2l+6zEydOtJ+zcOHCg9eXgmP1Wbq+9DkNGzY0LVu2tOey139nuh6fcL744gvz33//5XhfdVSy3XjjjXaypVO2w0WLFplPPvnEXHXVVZ5n1g8O7L3nnntMoumacKJ7rc7nAgUKeP6ZquPhTvfj4cOHZ3tv165d5qOPPjLdu3c3fhs5cqStO5zo3q32TDKpjn3hhRcStv90re937txpJkyYYH788Ufb7lSZVc+rvA0aNDCtWrWy/x8N7UPfhfb5zz//2PZruXLlbMb6unXr2ueZRE9CieTZ6+uvvzbjxo0zv/76q10FTc8JeobQc7Du+8pQqOeutm3bmmLFiiWsLHru0/1W7X3dc/VvlUUv1a367tT21YoQjRs3ts8t+q/axKlCK2ZMmTLFtq0WLFhgNm3aZN8rVaqUbTMoY7DaU3qprym3DMKJLqvOz59++sk+Ky5dutR+1yqznnF0v9P3fdxxx9lzQCvXNWvWzF7HiaQJUroWs57Jli9fbq8dTY7RZ6u/RnVI/fr1zZlnnmlfagP4LVP6lzJJbhO7vJj8qHubrqEZM2bYekuv1atX22tJ9wAdb11Hqr90zes80Et9L8mgPg49L+gaVx2vZ73gviCVq3LlyvaepOv89NNPt+VMJD1r6hpXXTlnzhxb96je0X1Hz1iqe6pVq2Zq165tv6vzzjsvqRNVM8Evv/xiv1/dT3UdHDhwwH6HFSpUsO2vs846yx77ZNHz6ueff27rRz1bq55Xva7rR/dwlUvP1PXq1bP3GbW5UqGvV1mYde3EStdWbivj6PuINjBY10alSpWi+u71d+i8CK2fdB7oHqV+dtUBmhiu45AKbXO/qd0WjurRdKE2qcZX1bZSG0V9Nln9Wrr+dJ6qD+nSSy+112Kyri+dm1l9bXr+0viP2qdZ/d26F+jc1CpTehZRexrp2y5N1XEXAMg4AQAAAA/99ddfATUxsl5Vq1bNtr1NmzbZtge/brrpJk/L0q9fv2z7f++99w5u++abb1zL8fbbbwcSYdy4cYGmTZu6fq7b69BDDw307t07sHbtWs/KsmXLlsAbb7wROP/88wPFixePukx6NWvWLDBmzJjAgQMHYi7HL7/8EtVn9unTJ9vv7927NzB48GB7nuX2u4UKFQpcf/31gX///TeQqo455hjX8k+bNi2QLnRcBg0aFKhcuXLEx7ZixYqBp59+2v5usGjOj+bNm2f7XZ0vkf5uaF0VTrh6LLdzNq9eqzJ69OjAaaedFtV+VFenaz0bbMWKFYEuXbrYeijSMtWuXTswYsSIbPvR/cnpZzt37hxIVSqb29+YyPZGNEaNGhXV+RLaTti3b1/g3XffDZxwwglR7Ud/g7zwwgtxXRc7d+4MDBw4MKJ7Yazfv/7GoUOHBurVqxf19XXEEUfYNtnmzZvT8vgky6233upYjrFjxybsM0PPmeC/uX///q7fjb7LeO4poX744Yds+7/22muzbXcrR7x1X+vWrV33rfM9XSTq+wltR8XbpomnvXHhhRcGUsG5557r+n1/8MEHSS/P1q1b7bWa9fr888/j3mc61/fbtm0LPPzww4EyZcqE/b2CBQsGLrnkksDChQsjaov37NkzULp06bD7zJcvnz0/5s2b53l7KZJ71JAhQwLVqlWL+PfLlSsXePTRRwM7duwIeGXmzJmBO+64I3DsscdGfe7oVbZs2cB9990XWLlyZVzlaNu2bVSfG+q///4L3HnnnbmeR27PodG0x+Jpv8q6desCd911l32Wifb7Lly4cKBVq1a2Hbpq1aqAl9R20TOorotoylS+fPnAvffeG3N56F/yh9szaizP825Ut7t9RrFixQK7du2Kab96jvrwww8Dl156qa2DYqm7Tj755MD7778f2LNnTyARdD2pjon2etJL94Wbb745MGnSJE/b7suWLQt069YtUKJEiajKo/vvRRddFPjxxx89K0u8fSThzt9Yxww2btwY1T6dyjps2DDbJ5Tb7+bPn9+2aRYsWBBIJPVJn3feefbzovnbSpUqZZ9z//7774Cfwo0DxXqM4m3LRXo+TZ061fYJFyhQIOr9H3744bYvUv2xof3teekeor8/3Pc0Y8aMHL8Tz3iEF+dgaBvlt99+C/ss6lQ3tGvXLvDnn38GEuX3338PXHHFFbZNGc3fVrRo0cDVV18dmDt3rmdlcRsDirQPI5oxpEjPzUxsl6bquAsAZCoCugEAgKdyC7D6/vvvXR/sihQpYoPevLB9+/Zsg1rVq1fP1nGVzIBudXSHCxiJ9KXBhddffz3usqgjL9pO93AvPcTHetzi6dhYvnx5oHHjxlGXV0Gx6nxIReEGnzVIpPM6HeqAaIN2g1/169fPdj7l1YDuTLpW16xZE7jgggtiKnOknfepVM+G+vjjj3MNAgr3Uue4BpqFgO7UC+heunRpTPcirwK6dW0ed9xxMX1+pDS4ddJJJ8V9fVWoUCGmgEe/j4/fk7r++ecfXwK6FcSogD+372f48OGelSN4YFIDj6FBlm5liLfuu/322133feSRRwZWr14dSAeZFNC9e/dux8lPCgZJVKBUpBQsrOdVt+9bbZF0l871vQIKatWqFdXv63i+9tprYQOUVRdEG6ymyZjJCujetGlTXG1gBV//+uuvgXgomNyL8yY4yCPccUlkQPdHH30UVSC3nwHdKquuNS++c523CvqZMGFCIB4KKNR3EW95SpYsGRgwYEDUAaj0L2VuMN4ZZ5zh+hlnn3121Ptbv359oHv37mHbutG+NOEx1kk9TtSP0r59e8/Kp+DuRx55xO43VupXf+yxx8K2hyJ9KZhPxyFemRbQvWHDBjuRMdrvM9b2RyQTh6666ipP7u2a9KeJg35Ix4BuPXvceOONMU3mcHoddthhgXvuuSfp/RypcA/J7TpXv1GoVAroVpso2qDprJd+TwkgvKTxKfWlRDvBI/SlSQqaDOrFBNNMC+hOtXZpKo+7AEAmY20DAACQVFqGWUvufffddzm2aUnGZ555xgwYMCDuz3nttdfs8pNZ7r33Xl+WdZo8ebLp0KGDXQorlJY41tLc+j60dJ+WltXyeVqabPTo0XapqmBatvLGG2+0S1kNGTIkpqVotUT0O++847pdS/FpuUYtGaql+rQsn5ZN0zJa06ZNs8s8ainnYCqPfm/EiBF22bRk0DKeWq5Ly11HS8votWvXznz66afmoosuMqlES6Jq2TwnWlL1kksuMR9//HGuSwX6RUustm7d2nUp+ipVqtjrQctS65zX8ntajvKrr76y14rOLS0dqe9By8gdddRRJq/KlGv133//NS1btjQLFy7MM/VssCeffNLcf//9jtt0zHQczj33XHttaDlilU3HcOTIkQe/M13zel/LqCO16DzSEsJa6tYPus4vuOACu9R1orzyyivmjjvuMHv37s2xrUSJEvZ+Wr9+fXPkkUeaIkWK2GtKS8AOHz7cnrfBtBRsx44dzddff233W7hwYZPJxycaWqraqZ5UvaDv1g9aArdHjx7m0Ucfddzer18/u4xvvH7++We7VHuWyy67zNSqVcskg9obAwcOdNymc1nLwo8ZM8beP5AcqheOO+44u6x8MC3lrraR7ud+UXtVz6tOtNxx1apVTTpL5/pezw9qF2vZ+WjoeN500032PnrXXXfluMeqDRntEuxqf6stqWWwr732WpNI+nu1fLWewbLalqrXVHepbanlzXUP1LOznn11vELpPfXRqF2s7zAW6sNx2rdo6e8LL7zQNGjQwJ47urfoetY5MmvWLHuO6B4YbNeuXfa4qD3+9ttvJ60fR3+H+o3Sge5dul5D6VrTs1ejRo3svbR06dL2XNRzopZDVz+cnrP/F6eU/bzVOfDLL7+YZcuWxVQmPb907tzZLrfuVC6dl1oS/ogjjrDngfoB9Nyjc1PnaDDtQ3/fpEmTzAcffJDw/o9M7V/KFPPmzbN9HG66du0a9T7V7h40aJDrdvVZnXPOOaZu3bqmfPnytl9A19Hy5cvNDz/8YPsgdu7cmaOcquveffdd23cXj1WrVtlref78+Tm2VatWzfYh1KlTxxx++OH2Hq1rRvX9jBkzbLvaqU9O13afPn1sP1uXLl2iLpP6W9ROV73tRG2Etm3b2vLpO1OfvL6vcePGmenTp+eod4YOHWq/y7Fjx5oTTjgh6vJkovXr19vj/ttvv0X9u6rHb7jhBtuOu/nmmz0pj9oXqtd0HEPpmtCz9tlnn23r9eLFi9vzVu0B9WsvWrQox739gQcesPehYcOG2fsT3Oma1n1T9+xQhx12mG2jnnTSSfa7V1tL9ZGuObWJv/jiixxtK1EdobaOjlXfvn1NXhKuT1rPNak8/qBj9cgjjxz8t9rT6ofRM7P6KdQOUfvlm2++sf3G+/fvz/b7e/bsMbfffru9NnXf0/NCPHRuqa7Pev4IpjanniuynkXUftP9SO0s9auE1m0qq8aBdZ7T75K67dJUHncBgIznd0Q5AADILJFkzPzyyy/DLpUZ73KvymRaqVKlg/vU/4cuv5mMDN1aFtNt9ryWklfGDjfKRPTOO++4Lp2rLC2xZLVQFie3rMg//fRTrr+vLJFaKtEpO4Qy2yxevDjqZcK19Gfoq1OnTq4z1ZUxMjj7mmaed+jQITBo0KDAiBEjAp999pnNEqZlxipWrOh6nLVNmU9SiZbPy20mu7LE6Th6uVyqF5YsWeKaIUxZzpSlb//+/a6/r6W1lRUs63e0zLuuZbfvwem8mTVrVo4yhf6MlpCPNzuaMixEc87GIhOuVWXsOPHEE7O9r39rib0333wz8Pnnn9trtm/fvoEaNWrElPEiFevZLC+++KLr+assG7ktiTtx4sRAlSpVDv7ONddcQ4buBGXoVpYyp/PbLfuIjoPaKpUrVz74nq41rU6grGc6r5QR5ZNPPgn06tXLLnPrtJ+szEjKvOL0+W7ZLnVdKPubshcGZ7Y588wzA/3797dLfo8fP94uH96jRw/XTJO50bXp9Hv6LC17rnrBja4d1ftuWe+0XHOkS6P7fXySQdnUnMpw6qmnJvRzw2XoFmXNU2Zkt2t4zJgxcZfh4osvznac5syZk+Nn3D4/3rpPy73mlmFQ5/Arr7zie3bocDIpQ7e4ZaN88MEHA3667777XL/rFi1aBNJZOtf3WlY6+BlEGUjvvfdeu011vcqmc7du3bqux09/55QpUw6WQ0uTK3tYcBbrBx54IPDuu+/afep5QuejsrG67VPPqH///XfEx0BLjjv97cqm6JZJMHh1A63Ek1vb8osvvsjWTxJa3unTpwdioe8ndH9qn+t7V3bJcPR8qMzsbs8CasdES/cRp+/S7ViJ2k7B76mNddlll9mMhmov6Lirj0H9DsqCGi4z49SpUxPyDJpFK2Q47UurO0XSlzZ//nzXayrW9vRbb73lmqWxW7duuWbh1bnplg1fbaFI+23oX8q87KqqI8455xzX/atuD9fP5ObHH3903F/NmjUjylSvrMW6PzqtKqK2pfYfTxZsrdAXul8924wcOTLX39c999VXX3Vtw8fS5602c2jfTtardu3auf69enZ1y7Kua0f1Uqzi7SNRX2S0bY9IjqHTPpUd2a2sak/pXp71ns6t888/367mNWzYMNt/ps/WfTHcShDKhB1tf5/bqi1u/QgXXXRRrqv/qbxu5Tz99NNzbR8kmvqL3TJfjx492rPP6dixY7Z9qw8zEjr2oeXSyn+6H+XWT6ntaru4tSGT9UyXShm6mzVr5voZqpucOF3Dzz77bMwZutVGiLaNEjw2cMghh9j2ZLjxID0XhVsh8+677477unFrr6n+yq0u/+GHH1zvJVqx7r///vMtQ7fTGFJu/bJ5oV2ayuMuAJAXENANAAA8FWmAlYJq3B441cEZDz3gBu/vueeey/EziQ7oVsClW4CIOtYjpUFgtw5QBSF5ESSqjp5IB9mzKIDH6WFeHfkKwo1XuE4YDVQGBziG65xQx8ktt9zieqxjGSBOtEiXKNZ3rQ4cBc36TYMVbkES6uhTp1iktHxr1u8q8Nft7/eqjsqtrkpWx2EmXqs33HDDwX/XqVMn8P3337vuZ/fu3Y4dmuE6SFO1nhUNaLoFNaje0jUTiU2bNtkg/qzfdVtCnoDu+K/haMqudoKW9g4euPj9999d96MO+VatWkUdMOx2P9DEOF3DwQOq4YLGNCjiVEdH05YKHkz49ttvA5FSsLoGZpz2pUGJdD4+Xrr55psT8h3FG9AtCox0u4YbNGgQ1+fruAQPpGt5cSdun+9F3Rfp0tj6rhRQkYrBWpkW0O0W7KLAYD851RPBg5npKt3r+7POOutgeV9++eWwQQ4KOnUKvtNLwXP6XQ0qZ/VVKEhdk6Pc6Of1/Skg3GmfCgiOl1sb9f777z/4/zfeeGPEAY2rV692DaRQsKDanvEGdGuSvoIVorF582bX4DkFW3vB7fpdtGiRPX/0/wrW1n0vXKCZnkFCg9xyC+Tx6hlU35NTUH7Pnj2j2o/OXS1xH295sgI9nJ579J4CyiKloG8F+Tl9T2rLxTOxKi/3L6VzMJ6Oueo3t33HMlE9XEC32rXhApPc6oPgCUBZL12nqm9j8cwzz+TYX/ny5QPLli2Laj+anOQU7BVtn7f6oNz68DWhONJ+JdU7ai857UfBa6rfYpGoSe/h2h6xClfW4Ml1avOGC5jWtfHoo4+6Xhtuz1SRUh+ugked9q0JjpHS9aRkHU77ufzyywN+c2t3aMKcFzTxI7jP8qijjoqoveY0cUv7mT17dlSfr0leTpPu8lpAt4Kc3dr/eilIO1Ju44mRBHRH20a56aabDt5f1D5T4pZI69pwE5F1fsVC7TS3CZhXXXVVxM8i6v9v06aNa3sv0v7yRI/L5NYvG8+5mS7t0lQedwGAvIKAbgAA4KlIA6wUZOj2sKlsSOr0ioU6NdVBFtzp7TQYl8iAbj1oH3HEEZ5llgseYAx+KQAmmkF2pyBRHR8FMnnZ4aaB9Hi5dWyoUzUr8Kdfv34R769r166uA0DRBsgm2tdff+2aJcTppQwdGhTRoHmsnV7x0rFwKps6fWLJ9KYsA/r9cB2ueS2gO52vVXXURnKdqVO3evXqEXWQpnI9q/uQAtidyqaBUP2d0VAwrlsGlKwXAd3JDejWAGvW/+v+EsmKCU6ZjmMN6A7+fA38RjqgHxpsEy7AwSn7pK6HWAZ/9Nlu2b2ef/75QLoeHy9lBSSGvu666y7fA7oVjKLgPLfrWFk1Y3XFFVdk25fbChSJrPuURUqZ7MLVscEv/awCEJS5zYuJUV7ItIDuAQMGuAb7+Mmt3aGXgmrSUabU95q8GGl7TavEuB1HZWVVRn79v/6OcJOBIjln9d0q63g83Aa/s56TlLk22sxmCoB0CkDUS8GT0QoNFtIqNbFQUELwhLWsl9rB0bafnbgd96zzSM+ukd7TXn/9dV8Cul966aUc+1AGzli+H7WPQieqRFseZYp3ywIcTYBUcJsjuD8v+HXbbbcFYpWX+5fSMRhP/VqaKOCWFTNrNYZff/015jKHBnSrztfEpFhMnjzZsQ8vlmQlCohzCsLWfSYW3333Xdx93m4TT48//viwK3hEm3H9uuuuC+TVgG4Fxmc9b0VzH9bqU07700QzZR6Phe4nbgk7lIQhluzuwStnJWLCVqyU7dapXOo3iXYChZPQbM6RPi8EJ3XIemkSVizU5gsNyMxrAd0KGHXbv667aCbfJDOgO6utr3EfrXIQbRtPEzuc9qsV6mKZpB7czxZaf0U7HrVjxw7XekaJfvJqQHcqtUtTedwFAPISAroBAICnIg2wUsdCuM75WJfUVsbiSDoBEhnQ7fYQrYHOWAciH3/8ccd9KrtWJIFKbkGib7zxRiAejRo1ylEmHfN4A4vdOjayXldeeWXUnRBOnQZ6RbJkabLdeeedYf9+t5cyqKiDXctxxprdJloLFy50DYRS5rhYqOxuy7FlvfJaQHe6XqvKdqiO2kg99NBDEXWQpnI965YpSYNCsS4l/MEHH2RkQLeXr2QGdAd3tkezxHdoFvpYA7pjDboKXebaia4fDc57NYCbZfDgwa4DaOrYT8fj46UqVap4FgzldUC3KLOP2/esLEqxth+CJxkoqN1Nouu+gQMHxlTvaBLqJZdcYpeRVbCCX5JRxyZz8F/tWLfgWL8mL+pz3Vbe0EtBwOkmk+p7BblGSm06tU/dnjGzBq5HjRoV8T4VUO2WnVwZvBMR0K2XAnRiDTjSxE6nfWpQXQGAsQZ0q7zxLJ2tgGqncimAOl651XHhsrGH0oSe4FWQkhXQ3aJFixz7UPa+WM2aNSuu8rhNSFObM1YTJ0707NxMx/4lnVtaqUzHQueYJjP1798/pZekDxeMp7askg+4vbTykOrbd9991/bfql3lFNCc9VLfk/rL4l0xJTSg+4EHHohrf6GTFLPaiZo0Go0pU6Y4/t2aPBErrfQWa5+3yuOWcELbvAowzbrGZ8yYkScDuoOfq6K51tU+dXuOjHUinVuQuBLmxJp8Z+jQoY771ISkcCtiJJq+P6dVL/RSPRyv4LahnmMiCbJXu86pPEr+Eqvu3bvn2YButXOcVq2MddwimQHdWS+1AWKhSaVubZVoJ9DoHu20H9XlCxYsiKl8WsXT7ZkxlglWmRDQnUrt0lQedwGAvCS/AQAA8EG+fPnMAw884Lr9pZdeMps3b45qn/v37zdPPvnkwX+XLl3adO/e3STTb7/9Zt555x3HbY899pgpXLhwTPu9++677d8T6vfffzefffZZTPssVKiQueqqq0w8br311hzv/f333+arr74yiVKsWDHz9NNPR/U75cqVMxdeeKHjtm+//dakGp3HF1xwQdS/t2HDBvP++++bDh06mAoVKpjWrVvba2nZsmUmUZ566imza9euHO8XL17c3HnnnTHtU+d67969PShdZkjXa1X1/Msvv2yv2UidfPLJaV3P7tixwzz77LOO29q3b2+OO+64mMp25ZVXmrp168b0u0iMokWL2vM7f/78np7fkapYsWLU98JIPn/w4MFm/vz5Od4vUKCAeeihh0ysunbtaqpWrZrj/Z07dyakvvf7+ERD38GKFSsctx166KEmFdx7772udevUqVPNlClTot7nE088YQ4cOHDw3+GeCxKtR48eplu3blH/3rZt28yIESNMly5dzGGHHWaaNm1q20VO1xAi53be79u3zyxdutT4YeXKldnO11S9VqORKfV9rVq1zE033RRV+7RTp06O2z766CPz77//mmbNmpl27dpFvE99Z9dcc43jtsmTJ5tE0d/t9F1H4tprrzWHH354jvcV96z2dKyuvvpq+33Eqk2bNuboo4/O8f6bb75pEknPzWpvR9POOPbYY02yzZw5M8d7RxxxRMz7q1+/vjnqqKNi+t2xY8e6nt99+/aN61icfvrpjudmz549TSb3L+3du9ece+65to2kZ/U9e/aYRYsWmfvvv9+en/+bl5BedMzOPvts15eO98UXX2w6d+5s7z1qV61ZsybH992qVSszaNAg27f1/PPP2/e8pLac1/0taicOGzYs7ms83utc/RCxUh+J03l33nnnmebNm8e0T9XxOt6h9Dn9+vUzeZXaJy+88EJU99CCBQu69hXGUhetXr3aPss46dWrlylfvryJRceOHR3vmfq8N954w/hF35/aQ07eeust++wRq6+//tosXLjw4L91z6hUqVLa1QHpTN//JZdcYu+lTo4//vi4nnmSQW3122+/Pabf1bjQjTfe6LhNfer//PNPRPvRmI/bM9x1111njjnmmJjKp74TtQGcnhmfe+45k1elQrs0lcddACCvIaAbAAD45tJLL3UNblMw94svvhjV/jQAu3jx4oP/vuWWW0zZsmVNMqnz2amzW4FX0QwKOw0YapDDiQKWIqVyqNNdLw1YKOg2HhpUSXaQtDqrjzzyyKh/r2XLlq6dFKlGHSOjRo2yAUaxUoflpEmTzG233WaqV69uGjRoYAcH/vvvP8/KuWXLFvPxxx87bssKKvcrGCDdZcK1es455zgOxoejoGUF9WW9nIJUUrme1fWg68JJPBOMNLjnFigEfyhgTXVrNHQ9B5/f8bRR1BFepkyZqH7noosuyvb5oTRgGTwxLpgmGSloLp6JKW73tE8//dT8+eefJpOOTzTUdnUL0DnkkENMKlC7yynwIku0wX8Kxhk6dOjBfzdq1MiceeaZxi+qY1977TUbQKWB/Vgo2HfatGnmvvvuMyeccIJ9Pf74474FIKezcEFaCmzzQ24TjeNtpyVbJtX3+qxor1un4IFgd911V0yByMl+1gxXL0fS7r3++usdt2nC55IlSyLeV8OGDQ8+tyjIL15Ozy2zZ8+2wZGJEksQu869rDaD2yQBL23dutVs377d8f146H4VC93j3J7n3PpeIuV2Dc6YMcNMnDjRZGr/ktoibpPkhg8fnieDXdRnrLpKQZfqQ9YEOi8ooCir3lIQVM2aNePan9qyJUqUiLu/xa2/Lp7rPNZrfMKECbbudeIWJBgptwk0Y8aMiTjIMNOoTjn11FNj+j2v6iJNllCSAqc+6ngnPVxxxRVxj2kkwg033OA4CXzVqlVx1bmaOBks0smHqVQHpCslfFJfw2mnnWYnRznReIWObzQJSPygsZV4yujWD63vaMiQIRHt4+2337YTXpN5L9CECgV250Wp0C5N5XEXAMhrCOgGAAC+UYeZMs24GThwYMSDdnrI7N+//8F/q7Mj1uzAsVq7dq0NwnXrgNHgdjzatm3rmvVBHY2RPtxrgEiv119/3XiRKUAP48kMEtUgTizcstv+8ccfJhUpMEGTGr744ou4giqyzJo1yw6MqlNIGSrcsn5E44MPPnAcVBZlloqHOlfPOOMMk1dlwrWqoPxYsjUpOCDrFRqQmer1bOigTfCAcZMmTeIqm65bpI5Yzm9lQQw+v2MNGFbwaSxZ+xUgFfz5ocaNG2ez0Dpxy/YSDbd9qA3ndu2k4/GJVri6xSl7jV8UqOwWNKk68scff4x4XwokDc545md27mB6Lvnhhx9iCqYIpczHyvil+5quvURm6c004a49LycmRsMpuCVYrFmq/JJJ9X0sg8rKJOc2cVTB+bkFfDupXbu2vT+H0qSO3M6fWKiNXK9evbj24RZ8reMUTcbM99577+BzS7STOZ04TcjSPUP1cyLoWVtB6bEEgWW1GZShMNHcgrliWSUjmIJl+/TpY1933HFHRL8zZ84cG1ydqDpEEyTc6lUFPWdq/5ImvYTj9hycyTQJ6JlnnrHPPpUrV7YTAOPNnJsV4JhVbymIOF5qI1epUiXu/pZEXOcKVs+6xvWKdFUit34otdPOP/98Ew+thOHU3tP9xy1pRKbzui7SBFq3/lq3FQLcMrKqXRRPwo5wfW2a3OxF/3SslERCySicxNpm1SoDwfV1jRo17IoEftUBWkkouA5o0aKFySQaO1y+fLl93n7wwQdt5m1NtNu0aZPjz+t46Pv0Yqwl0eLtB1ZfhFsGba1+o8DuWO8FderUiXslSSWdcLsOPv/8c5MX+d0uTfVxFwDIawjoBgAAvlKnvFsGxfXr15tXXnklov1oWc558+ZlG1zTrOFkGj16tO0AdeJFZ5nbg7kyAioTs1+clhhfsGBBQj5LnQaxZntSh53beZbKNJg5d+5c8+qrr0adbdTtfBk5cqQdtFandXBW+2i5ZSvRZA23jNDRyMsB3el+rSqgRRke81I9q45Ht8AG1VuxZn0NHmhyGiROZxqwjfX1119/+XoteRGwFKtTTjklpowtufnkk09ct3mRcVMDSW7Z73ILYEmn4xOtcJMXUynrr9pRbhmUosluquxOwcECqncTcb+IlVY0UV2uYJITTzzRk31qcFntImUJc8t0mKjsvfHUsxrw90O4LGSJzNAbzu7du8Nuj/cen2yZUt/rc2JpGylQ1O0+qgl4sWTC0+9UqlQpx/u6ltatW2e85sVzkp4H3TLi+5kJ2OmZJZHPLW7BJKnG7VgpUOPDDz+MK1iob9++9hVpQHe4OiTeQE8pWbKkzZzsljU4miDFdOpfym3Skl+TmuLxzTffRNTm0MQX/X3q+1IbTKsRhd5H9EyuIDxNoFBAdqoFuDvVXfqbosms67Y6z6OPPmp27doVU7nKly9/8BrXK5KAbrW3xo8f77itadOmcbd71Gfo1s5WUou8yG2lj9woa73TM6Ouq2jqI11bCkROVF+bJr65nTd+H/Nu3bp5smJJFk06Ce63VBZjp0l/0dQByp4ea3tSkxiD64B0CujWJB59d+FepUqVsv2let5WZuFwKzrp/qFJaemQtVzXS+PGjePej9uEBfXLKPlPOPouf/31V8dtXpxHOt/dnuX8rhf8kArt0lQedwGAvIiAbgAA4HvnRK9evVy3P/fccxEtsaUOm+AB2nvuucckW7iOBmUfiZeCeZ2W8Ex0lt3cOGU3UxYEPYh7TdkTYs2Ap8EVp85jldOv4JBoOnS0PKOCrzW4rkHneGfEi5YrPumkk2zHcLQ0OOAWvFqtWjXXTuhopEMHazpJ5rWqjsREZJZN5Xp2+vTpYQNwvcA1kRrizUITr0izqkVbp3/55ZeO24444gjHILVYg2WdrFixwrMVM/w+Ppka0J2VwdppSerclmUPpiyLwcGxvXv3NqlGA8OXX365XZ5WmYI0AdWLY6F2k4K69ewTSTasvMrtPixeBfFFq0iRImG3x5spNJkyqb6Pp11UpkwZz/fp9vyzZcsW4zUFRnnxbOB2z1Tw9ObNm40f3LKnJ2oidiLaVYmgSQNu2SSVZfv9999PWlncnsnURog3c3wWt9Uy1IZQkHAm9i9pVa14tqcznd/6+1QHqw327LPPmoULF9qg4mOPPdYxyKx9+/Z2ZcZUaVN5UXepf86J2qSaALl69WqTDLrG9uzZk7B+F3EL6NaqP27BZJnc9lU/aqzc6oZo2h+J7mtTXeuWKdjPMY2siUhOE/3UZo52xcTQVU70d3ft2jXuOkAr6yjLd7hgZRjXtomyAmvSwpAhQ+yksXSgNooXYz/h+qPD9WMno14Idy/wu17wQyq0S1N53AUA8iICugEAgO/UsaXBYyfKDpFb55mW4AqeLX7NNdckJHNlbn766SfH95UpPN6lCbOCS5R5w4myC6QSdRa4LW0XDy2bF6usrA1OosmY43cnpIK5FdStgRxluNQy4+ECX3KjCRPKfqSA8WgCe9WJvHHjRsdtbhn5knm84e+16lVW03SqZ8N1hHNNZJZEnd9+fr7qdLdgA7eB11iE21dug0npcnyiFS5ANdbBnERRHaTgGTePP/542N9Xuz54gFvnQ6xLyiYzK9gHH3xgyz5s2DAb3O2WJTUSCjp6+umnzcUXXxzRpNW8KNzAtV+TMHML6E+nwKNMqu+dgvziPabHHXec5/tMxLOmV21CtzZquMm7ftmwYUNC9ptO7YYLL7zQ8X3dT9QPpsztmrCh45co+iwFlzpRhszcJsCkUh2Sav1LubWJ1HbIS/Qdn3vuuXbCoFv7c8CAAbZtlsqiqbtat27teg1pxRcFe2kFlUQHdrv1u0idOnU8+Qy3IGTVMQrmz0vU9og0g3M0k9SiqYvCHXMvJpGFO+Z+j2loMoYmRjl5++23XSc3ONE9eOnSpdnq7WhWkNXqKW59khr7Upvl9ttvN8uWLYt4n3lR5cqV7QokShSlLOvKOuy28keqSnRbP5L2lJ/3Al1HqZ6AyWup0C5N5XEXAMiLCOgGAAC+yy2jtoIewi1zHRw8ok64++67zySblr1TtgS3mcdecXswnz9/flz7VQfOwIEDTZcuXWwmJGXX1cxuBVTktrTd33//nbSBa7fllyPltnx2bsuopyIFE3Xu3NkuM6ugjHHjxtmg7FgnMwwePNhmN4pUuCycbpnDYlkaFul5rXrRyZdu9SzXRN6RiPPb7893C8yJN2Aumn25LeWabsfHy4But0x/fnrggQdct2nC2e+//+66XQOqO3bsiCjjd6rR5LnLLrvsYHC3Mheq3RTrhJ2xY8eaK6+80vNyZoJw571fGbpzW3Uk+LxOdZlU38ezIpBbwFw8+0zms6YXqyHl1kb9+eefY96vnk81CUb1vJZ6V9ZdTeJXVsTcnlncMlkmahJ2OrUb7rjjDlO0aFHX7VOnTjVt2rSx19+jjz6akKDIuXPnumZETrc6JNX6l26++WbXgDMFe2syf16kNpjaX27Z31XX9O/f37N75GuvvWZuvPFG06hRI9vOUx2he0ZudZdbZslo6i6dk9ddd53rdu1L1/ZRRx1lOnToYEaOHGl27dplvBYuqMqrvhe3fhcv+rjTTSrURW7tQwXeebVqlNsxV1+fXn7Sdef0DLJ27Vrb7x5N33qwbt26RVUOZdhV0hU3OqYvvviiOfroo815551nV+dIlyQ1sdBqDZMmTcr1pVW11CerIGAlLfn333/Np59+au666664st/7KRXa+uHuBV59r271giYn/vnnnyYv8ftekOrjLgCQF+VcewEAAMAH6ix/4oknbIBEKD1IvvXWW3ZwI5Q6bYJnk6tDW51ayRYuM4IC1r/66itPPsdtOW8FN6jDqmzZslENsr7yyivm3XfftdkKvJaIrFDxLovnxVJ1qUiDS+rI1evVV1+1HXLKPqHOy2g6Q9QprIxAWm4yN+EyAkWTfSTWAZ68JB2v1dwCnzKxnuWayDsScX6n8vXllj0lFuH25TbpJN2OT7TCBYKmYrDzySefbJd81wo5TvcTTbT85JNPHLMTqo2SRYEoHTt2NOlIA+0tWrSwr+eff94ONCqYffjw4WEn94TS7+j+fssttyS0vJkU0O1X4LSyvOl6dFvNxu8glLxa38ezSlEy95kIXrUJw7VRY8kAq8yUCmbSPSKajJaRSFTW6XRqN1SpUsU+s6v/LLdM/Mriq5cyKCpDqIKBTznllLjLkEl1SKr1L2l/Wur+scceM0OHDjX//fefzXquQEMlwYgng2+600SGN9980/Uc1rmuiXKxBJjpOV8rQ2oFvHCTnpJVdz311FM2G/eCBQtcf0b1q9qdeuk81sQZXefqy3PL1uzVdb548WKzYsWKuD9j+fLlrtu82H868bsuUjvWLROuziev+trCBR7rmMcbzBgPJUfR9TNmzJgc2zTJ4/LLL891Hxq/0qTd4IlJWu0pWgpCVjvqu+++c/0ZPZNMmDDBvjQmcNZZZ9k6QKt6etX/mArq169vn7nzIq/a+pqUpPaD070ot7a+W3tLbedp06Z5Uj5NmghXLyiZTV7h970g1cddACAvIqAbAACkBM0gVpY7ZXBy69DW8nehD6bB2bnVOdG7d2/jh1WrVrlu+/77783ZZ5+dlDJE8sCrDhwNVGhAyG257VSVLgPsftMgl17K3KPg7iFDhtisHZEsVXfvvffawPDcBgvVweLGq+wt8XZkpTuu1fSqZ7km8g6/70XJvr68/Lxw57CCVjLh+HiVJVbcgkf9pizdTgHdoslkCkAJzayp1SWCB/HV3siUiXZaJl2vXr162WyoCjjSK5J798MPP2yz0bplU8qLwp334a6XRNK5qqButwCjdArozqT63qv2VaL3mQhetQnDHfNwbdtQ//zzj+nRo4edqJJu0q3dcMMNN5idO3fa/rNI2gl//PGHfSlIWJOpFPClCVUNGjQweb0OScVjr8Dlfv362ReyU4ZuBUdqlZRQe/futUlCFJgdDQVfdu/ePWxwcbLp3NffqCDtSALM1c+n9rdeaq8o+FHJTrSyTKzB3eGu8wsvvDCmfXr1+ZnI77oo3PetZ5tk9bX5Tdm0nQK6p0yZ4vh8G0rPf8GBirlNvgo3uVV1k+7XyjydG2XfHT9+vH3pb2jcuLG9/q+44gpPJ1ohPdv6Gt9RX4PTxOjNmzfbsQenMSDdW9zGkbZs2ZJn6oVkSuV7QSqMuwBAXkRANwAASBm33nqrefrpp83GjRsdZ4QrIPXaa6/N9iAZnK1AWQiUfcgPfi3/HSySYF0N+Cm7j4JE3dStW9ecccYZ9r/lypWzs+7DBfdeffXVMWXvikVezkgUKwV2K/ujBreURVLLFWoA2M28efPMxIkT7eBROOEG+L3qgAqXoTHTca2mXz3LNZFcyr6WqEyNqX4vSvb15eWgQrh9eXWN+318vPxOUjWgW0vQKwuYsgc6lVlBSO+99162Ab+XXnrp4L8PP/zwsMvJp7NjjjnGTkTt27evzeamADqnZ5ssCvrWChw33XSTSTX6G/RKtv3796fkIKeC9t0Cur3KFpsM1PfJ32cieLWCQ7gA9nB1VzCtTKABfrefV0CKVoFSALEyTOu5RVnewmX5fuaZZ0yypMsxD3bbbbfZ71NJD/T8HikFrWqClV4KTLv99tvtpCIFEUeKOgR+at++vWNAt3z00Ufm5ZdfjnjCoLJ6KwmCm1q1atngaPWplS9f3gZHh6t77777bs8yfFeqVMn89NNPtowvvPCCazZLp8B2rWSplybZXHLJJXbC4YknnphWfS+R9G9nEr/rIr+Pd6occ/WDa1UEp3a9Jos899xzrr+rZ2AlU8mi+2qXLl1iLov6d9Ue0mc+8sgjEa9SpHIoc7JeqpOUsEV1QJMmTWIuC/zh5Wptau87nUM6X9RX4zT5h3oh+bgX5L1jDgC5IaAbAACk1FJiGphSR5WT/v37m86dOx8MaFOARGi2QL8oG4Lfdu3alevPaLAuOMAmmDI4DBgwwDRs2DCqz41m8A/+0ex2DVbpGlKmjl9++cX1Z5WRJLeA7nAdLH5lT8wkXKvpV89yTSBTry8v645w+0qFazzVMi8pKCNVPfjgg44B3fLhhx/aQOAaNWrYfw8aNChboJ+WkU73e1Kkqw8pC6pe4ZanVbsrFQO6/RIuaMnPlSrU7nI7jkuWLDHpgvoekR6nSAbU9UypCT7K8OfU/lXfzi233BLVsvFuEyeQ85lQwaMff/yxnbT966+/RvX7yjaqY6N+NQWMXn755RH9HnUI/D7vw9VZM2bMiChwMVwwtyZwaSJiq1atoiqbJqt43ZZU0pObb77ZPPnkk+aDDz6IKthKfRf6HbXLdX2rDynSbL1+X2OR9G/DO34f71Q55gqg1SoYes4NpQm4SpTi1ren7NjBmf4vvfRSc8ghh8RVHo1/aVUrBYbrPq8M4Bs2bIhqkqwyfeuliXevvvqqOfroo+MqEzKzve8U0E29kPdwzAEg9Xg3vQsAAMADyhDkNti3ePFiO1gl6qRXxpEs6piKdclYL4QL1lNWXGURTfSradOmYcs4bNgw1wDRTp062Wzn0QaIIv2o81ZLNmYFWTmZOnVqrvsJF1BD50t8uFbTs57lmkCmXl9enr/hVojIqxMfwmWgDPd9+U0ZC90CZjR4rImYokxQChTLooFtBabkFRUrVrQD/Mpq7ubHH39M2WzsfgiXgc7PgO5wx3Du3LkmXVDfI9JjnlsQtiZfaLKwUzC3rlVlh1RWyGiCuRF9sJcmDSmwXi9NmDriiCOi2sd///1nrrjiCrufPXv25Prz1CHwU26rMs6cOTPXfSjzdWiCkCyaoKJVB6IN5k6k6tWrm8GDB5tVq1bZFdzU/x3Nyl3qx1Bf+gknnJBtlctwwl1jqvsT3e8SnOkYiRfueKsPLBl9berTSwVaGbZgwYKOqyoNHz7c9fd0jQbzcrKunic1uWPlypW2DO3atYv6PqgxNGXqD/c3IHPF0t4Pd44deeSRSakXnCZXIO+OuwBAXkRANwAASCnKaKIsQW60hLsCHh5//PGUyc6dWzBOKiwVpY4bZQl0og49dZY7dVgic7N1v/baa67b//zzz4j24SbSpSBzo4CwdKFOJy9wraZvPcs1gXSWjGXtc9uXn4Gaqfrde1V3JEq4ATZlMVOWMrU31q1bd/B9rcaT1451oUKFbACO27LJmzZtsgF1yP28D3e9JFrLli1ds5utWbMmW1a+VEZ9nxm8mgQS7jiFa9vKK6+8Yn7//XfHba+//rqpX79+3OVD5E4++WTz3HPP2bpoypQpplu3bubQQw+N+PeVxffiiy/O9dyiDoHfwUbKXO1m7dq1ue5Dfc5O/TfKXq1gx+LFi5tUpPNek2i+/PJLG9ipFXA0uTJfvnwR/b4y+7Zu3TqioO5U73uBtzje/6dSpUrmoosuiihoO8s///xjJkyYkG3iSSQrBcRS/ynz96hRo8zq1avN22+/ba/pSPuINVFKE7g++ugjz8sG73k54dutTaXJQW4B3dQLeQ/HHABSDwHdAAAg5SirkFsH/fz5883DDz9sPv/884PvqZOsefPmxu8Ov1R+4P3iiy9sh79bME7hwoWTXib4S1l9jj32WMdte/fuNVu3bo15OVmvBnJT4dpJdlm5VtO3nuWaQKZeX8kKzglXhkwWLtgqt3ux38455xzXgD21JR555BG7PHQWDRYqoDsvUptLbS830SyfnenCnffly5c3fg5wKmjCzQ8//GDSAfV9ZvCqTRjuOIVr28pbb73l+L6ywCpgCP7Q5CH1j2lClSYLjRs3zmbVCxcEm0UrSigoPBzqEPgt3GST4EmETn799Vebzd6tLzq3ei9VKGPvrbfealfXW7ZsmXnyySdzzV4uu3fvtvWz08oK6dT3Am9xvE1E2bV1vc2bNy/H+2+88Ua25AuaUJVoZcqUMV26dDETJ048OMEj3GpCwUHCN9xwg/nrr78SXkbEx6trTxOY3FY90XnkNilIk4jcJtDlxXohL+BeAACph4BuAACQkh3T6lwKl6U7OJtK7969jd+qVavmui23jvJkUPYGJwoOdcs8geRTh6qWQs16LV26NKGfd+aZZ7pu27JlS9jfVfYiN8oU4oVkBrJpYCsVysq1mr71bKZdE8hbwl1fXmYODrevcGXIZLVq1UrrIN9wq+Qo2C/4mGtwPJUCZZRVObjd5ZZpNhXaXXlJuPM+3PWSDNdcc03YSXnpgPo+M3g1qB6ujRqubavn1jlz5jhuu+SSSyLOGIvEUtbO8847z7z//vvm33//Nc8884ztbwunf//+YZ85qEPgN00adOO2Gkpu/S2izLfp6KijjjK9evWy7dhvv/3WXHjhhbleWwr+TOe+F3hLEybdsvTmxePdqlUrc/TRRztuC13tct++febNN988+G9l+O/UqZNJpgoVKtgJHj/++KP5+eef7eeHqws1YeqJJ55IahmRfm39cPcCnfepvpocose9HwBSDwHdAAAgJd17770RZaLVsrIaoEqFzs8jjjjCcduiRYuM35SFxkm9evVclw5H8mnw5corrzz4Gjt2bEI/z62DOmspx3BOPfVU121enfOJCGQrVKiQ4/u7du2Ka7+5ZYKKFNdq+taz6XpNAFK3bl3XbQsWLPDsc8LtK1wZMj3LoFuW7nS45tu1axdRRkDdw5T5MJUow1pwu8st22wqtLvyErfzXgGifgd063yvWrWq4zZlwQ0XZOa1JUuW2Gsv9KVzORzq+8ywfv16T/YTro3qtgJDuGcWOf300+MuF7ynCVU9e/a0dUf37t1df27jxo02U7eb2rVr20BxJ9QhSIZNmza5bnPLJppb3aWgtho1aph016xZMzNmzBjbt+jWXpGhQ4eG3U+4aywV+l7gvZNOOsnx/bVr1+a5QD49c7glGtIEqeBsx+q3D15lURnwlfXYL+o7fu+992xd53ZMRZOZFZSL1OV3W1+4F+QtqT7uAgB5EQHdAAAgJenhUUvHxZMVMNkaN27sOtgQ3Lnnh1WrVjm+f/jhhye9LIhcogO53DqZlcmjdOnSYX+3Zs2arsveL1682JPyzZ8/33jNbYAv3swSTstuxoJrNX3r2dNOO811WypfE5lIAXV//vnnwZcCV5B7oKkyOyU6OGfhwoWu2yJZIjhTHXPMMY7v+12vRTrgHclqOdddd13K38v8andlBfanE01kC65nvZrYFu681/Ohst75qUCBAjYg0om+g88//zxpZVFAydy5c3O83NpKWajvM4OuOy+4tVFVtzdo0CDqZxZJ9bo+r9Pz8EsvvWQeffRR15/5/vvvw07QUmIFJ8uXL497onQW6hC4TTgIF4SobNXh5JX+FgV2K1vvkUce6bg9t7ZbuLaE2hrIPBzz7Lp27eqYaEh9j5988olrxu5u3bqZVHDiiSeaadOmuQbkKvtzuMl58F+i2/rSsGHDsL9LvZD3pPK4CwDkRQR0AwCAlKUlI90y/8hxxx1n2rdvb1LFueee67rNbTniWGfoK+ND8OvFF1/MddAjluw1kdBSfUiMf/75x5fMRpUrV44oQ75bx9/ff//tSWCPV0HSkZzze/bsMWvWrIlpn7t37/YsYJRrNX3r2XAB3bNnz/akXIm4JjKRlpQ//vjjD77OOussv4uU8hS4dc4557gGPnh1P5o5c6Zr8MUJJ5xg8iq3gG6dy+mgQ4cOYTMna3UMrb6T6vxqd2kiXW4BSKlm0KBB2epZ/dsrbue923WSbDfddJN9DnUyYMCApJThwIEDNqDbKdP71VdfHfZ3qe8zgxdBFAqKdGs/6xwPN8HX7ZnFi+eWTHhmSQQ9r+oazXrp3/HQZKxjjz02pvaH2zNZIBAws2bNMomsQxRQfuaZZ3ryGUg/uZ1fud1fUr2/RZMLs67xeCcaVqpUyTz55JOu28Nd57rG3FaI87LfRR5//PFs/S5XXXWV7WNDZva17d+/33Tq1CnbMX/ooYdMqqlYsaK5+OKLHbcNHjzY/nfp0qVm0qRJ2bJj5xYgmxtlQ8+qA5QdPR6q115++WXX7enS15BXKSOyF6s/heuPDtePncx6QW6//fZs9YKeuZF8qTzuAgB5EQHdAAAgZWm5y3BLRt9///02ACJVaAlut+XSteSlV5T5Tdkggl/FihUL+ztuWf/iXTZRwQThBnMRHy2Tmkj//fef4/tuGbecznm3gdwvv/zSxGvq1KkmEZkV3YITFIgeC3Xge7VUJddq+tazGvBxy2TxzTffxB10oQArZb0DEuXyyy933TZ+/HhPMsxr0NMtIDgvc1sO+a+//jLpQPfW++67z3W7AkzTIWD5p59+iruujqXdpeCjSCbS5RVu532dOnVMKtCE44EDBzpu++677+w9P9G0lLpTfapr7ZBDDsn196nv05+yLsZL2VvdnjHcnvMiWVUg3ucWLzP+Z5IffvjBBmhmvYKDyGK9d7slSNi5c2fMdci4ceNMvLZs2eKaJfyCCy7I9bkMebPu0/05t+C0RPW3eFV36ZrMusbbtGkT9/4UkOqWKCXcda4VUXStOZk4caJn7WWVoX///tn6XdQv59bng8RmdXfLVO9lX5v6eYcOHZrtmKu/MhW5ZdvWM+Nvv/1m3njjDdv/ndvPRxvUmlUHeDEBskmTJjZpSyz3evhLwdzTp0+Pez9ffPGF4/tVqlQx9evXz3Vs9tRTT3XcNnbsWOMV1fsK5g2uF7Zu3erZ/pEZ4y4AkBelTgQUAACAS9Ygp6DtatWq2awdqUSD55deeqnjtlGjRnnWQRm8tJ/o+2nbtm2ugYZeBrAGz8xWdg0khgIhvMr87GTy5MmO77do0SKi39c1WKpUKcdtEyZMiHswLFEB7cok6STW5SZHjBhhvMK1mr71bLgBHC1nGm7p8mSfZ4ATZWx1C7r1ouPebcBH2WJvuOEGk5e5ZZEPtzxuqlGmtapVq+Z4X/VnuGDvVKJB7Xjr6kS2u/JSNjInrVq1MqmidevWpkePHo7bunfv7kk2NTe7du0yffr0yfG+2uTKchkJ6vv0p/uDgoni4fa8Fslxcntm8eK55eeff47r9/OKeDN3itN9Ww477LCwv1e7dm0bJJaoAB8FH7nVo14EzCE9qT/j3Xffdd2uAOjcMm271V2aPB1PX4Kyacdb9yXiGldg9qGHHhrTde52rSn4Pd4JJcH1xY4dO7K9l0orceYlCvy/7rrrXJ9hvEoQEdrXlsrHXJnq3VYIUvDpW2+9la0d3rFjR08/X9+5F/24bm3+3OoA+G/kyJFx/f7ChQtdx5V0vWtyX27c7gWawOvFikHpVi9kulQfdwGAvIaAbgAAkNK01O8ll1yS430t2+6WZSQVA9BXr15tPv7447j3r46S0Jn1etgNN6AqdevWdXz/jz/+iCuLjNssf3jn2WefTch+1fHmFASg8/eyyy6LaB8aLHPLov/pp5/aJSJj9eGHHyYsANktE+qUKVOi3pcyJn/00UfGK1yr6VvPZmWddMv69dJLL8VVrvfffz+u3wdyo8EcXV9OdE3Mmzcv5n0ri9ugQYNcszy6DZTmFSeeeKJjRjTdR9NldYVChQrZ9nkoDQal0/FNVLtLgT5uwS9aXhb/oyx3aqM6nV+pFvj+zDPP2KXVQ6mudLoWvKJAcqcVOx588EHXzIqhqO8zgzK1x0oBdEOGDHHcdvbZZ5vq1avH9Mwi8WSp14SFRK9SlSm8yNqoSaduWRtz89BDD0X8LBWt559/3vF9rYaUSpN7EmH37t32u1USC2VIPPbYY+33kaoZbJNJwT/Lli1z3X7zzTfnug+3uktByr/88kvMZVPW6uBMvV5QgHg8fWrhrnO1A9yy9mbRtdaoUSPHbW4rlcTb7lZ2Tq02An/ccccdjkk7NMHm1VdfjXv/6tN85513sr13yimn5Jol2E833nij4/tvvvmmWbNmTbaEJ7lNKImW+sRnzZrl670e/t/31C6IlSYeONE9wG0CR6hrrrnGdQKgF/cCPTuGllMZ6s8///y4943MG3cBgLyGgG4AAJDyhg0bZjvGg1+RdNT7QcvhuXX23X///XaAMlb6u7X0XuggwcMPPxxRFja3fboN5OZGHUpug/XwjjJ+xBNUEa6j3okmULhl73CigBWnJdF0rrsNxEbS2fzEE0+YRHFbhlcZCRVwFY177rknrs7VUFyr6VvPiq4FnRNu55cC82MxfPhwshUiKa699lobXBxKQSSPPvpozPvVgKcy3zlljevXr5/J65QN1S1AKdbVI/xwyy235GizO2VbSmUaWHLLpB0PZSl3Wta6YcOGrllO82rWYaegAwUUua0K4xcF2o0bN85xgHvAgAF2GXavaZ9O7cHTTz/dtW3vhvo+/b388stmxYoVMZ9LTtlfdT9yC9QNVqtWLbsMuxNNdnULHoqkXAqsRGSZbRUEEw+3Nsa5556b6+8qG7Lbs+sjjzwSV+Z4p2B1BZYkatJVqti3b58NYNJqCwrm1fFVds27777b1tl5mTKM3nrrra7b1ZaKJPjL7ZyVeO7bzz33nEmEeFfp0vfmVB8r87DaMblRn55TUJcmKY4fPz7uCeszZ87MkQmWrMH+UTb3Bx54wHHbk08+aYP54tGrV68cz0OR9rX5pUuXLhFdKzfddFNCPl/JUuKh699p1S+142rWrBnXvpF4K1eujDk5iK5XPZc56dq1qznyyCMj2k/hwoXNU0895TpmFWs/dxbt+99//81RVxQtWjSu/SIzx10AIK8hoBsAAMBj6ohQNp1QyqSmQJd4ssCFBpgoo9/JJ5+c6+9eeOGFpkyZMo7bNCgWy0CwOnnVsYTE0iDeBRdckC3zR7z69u1rvvzyyxzvq7Ms0qXasxx99NF2f04UzPLjjz9GXT513MU7UBDOxRdfbDM9OnV0RxLAkEXBLppwktvAYDS4VtO3ns2igG6nLPDqrFSnebQTABRskxUkHslAEhAP1Y1vv/22HbQJpcDcDz74IOp9KtuuglmdaPKOW0BYXtO6dWvH93/66aeklyWv00olCxYs8Gx/Guh0CsJV4KTuO8j9fHe7Pvym7GHKyqn/hlJQklcZLEX7clryWgHlWv7Yqd4Oh/o+/WkwXe3eaDP3KkDUbdBcg/dNmzaNaD+dOnVyfH/9+vVRP1OKAljjCQTOaxT8Es8KQHre/uyzz3K8r8ndmiQSicGDBzuuTqS6PJYJHCqTWwDJbbfdFnG50tXrr7/uOqns3XffjTuANl3Nnj3brhywadMmx+2lS5e2ba1I6BxSH5YT3RPnzJkTdfk0eV5lTATdO7du3Rrz7+sadeK20p5TVny3CWMKYHWawBXpfUjXdLBy5crZvkD4SxNINOE0lM5DZepVtu5YqO809DrV+XXRRReZVFa+fHm74lQ4+r6i6TOMxiuvvBLzdZa1motT8CUrRKUPtacWLVoU1e9k9T87HXut6BRtH4RWWWrfvr1jFnmtqhDrZMxp06aZxx57LNt7WiXIrS2I5EnlcRcAyEsI6AYAAPCYBhNGjx5tSpQo4ThAcOedd9oOj2i88MILOTq2NXgf6ZKHGmRz6xjXgKsCXKPJTKzPTVQGGuT0119/2Y7uqVOnxrUfdcAru73bQLmOaSzLoKvDX8tkhlJHv4KinLKBuHn66adth7UkqmP/kEMOcc3epM/u2bOnYxbNLBs3bjSdO3c2ffr0OTgY5pb1O1pcq+lbzwYHSClwT0tYhtJyqVrWMtJBMF2zmtChABcFXOnvAhJNSx5n1cOhrr/++qiyF6uzX0tnbtmyJcc21Z3KzIL/m9DjVKd99913vpQnL9N9/owzzrD3mXgn5anNpesmNAORqL3RrFmzuD4j0zid7wp8T+Wgg2OPPda20UMDxHTMFQTVoUMHs2rVqpj3rwDHjh072n2FnkdallhZemNdnpj6Pr3vGaLjr6A4pzrGic5FBWQ4HafKlSu7ZuBzctddd9lsnm7PdAoiipTKpfNHzzuInCZQ6ByIlp51FZjnFCiqZ0fVu5FQ8LeWXnd67tFE6aFDh0ZcJj3j6tx0msCse6XOqUyXNVk83KpNeYkC2HTvUxC2+sScaPUHTWqKtB9L52po8Fhwu039LXr2jpQmRag/LFE0MV91444dO6L+XU04Ux9HKAVN6fqPlO4LLVq0yPG+gkw14S7apAPK5qoM4aEB+sokq0BD+KtgwYI2M7zTsVBiELVJo83OqrpNQZ/BlMjiww8/jPh+4yenCZXJyM4tuvbPO+88s27duqh/V6v89e7d27G9l8h6C97Qcdc9S3Wlksj8999/Ef2engnUt60VT5xoMqDTZLzcqK9dmZtDaSKUnkvcJl25+fbbb+3fGNw/rv50tSudVoJFcqXyuAsA5CUF/S4AAABI30AHpwwkoVl11cn31VdfOe5DmZ8SvXyWOjvmzp2b4/1wWVfmzZvnWGZ1eDl1XDipW7euGTNmjO3QCO14V9bi77//3g6eO2W9CPbbb7/ZpaxCM/HooVqDOdF0wGggZOTIkTagMJTea9CggS2Tls51oyzRGphT5iBRB6+OsVOnjWbZhwbyKitbaGY2/Vxo8OzSpUsdP1/vhx4bZXFRMEIwlckp+Nmt09mprLVr13bMtucX/e3Nmze3AS3KVKD/j2aAXEtfaxa8WwegOlRinWGvDkZ1+Ddq1ChHHaDMYVnBIgrmcFquVfR73bt3P7icpJZZfe211+x1lKhMR7qunJap1iC2BhY0cKbrokKFCnb5Yw2mKdDo888/P3hdH3HEETYj04svvhjxOZtb/Zep12q4fcciFevZLDpG6iR3uqaU8VLft7KsKQjMjYKoFEy1bNmyg+essgM50XXt9H0nsx7T36Vs4k73VDepNHjn9P251Zdu7YRYv3O3e5baWk7U/lKd5ET1re6L8dLEAx1PXRuhZdXAvQKI9CpVqpTj76tjXxletTS6U72jiQrvvPNOWhyfZFFdo2ynuveFDnSpjlPASqzcvo/QdpHT/SBZ35vbdRCOW92nAcAmTZrEVR6d/wrq0QQwDdKfe+65jgFrTnTtqj2jdpdbNq2rrrrKLl0eTXmizZis+4xe6UKDz04Dz/ruU31JcLVrpk+fbif8jRs3Lts2tSXUxtCkSgWERPq36NxRcJOyazrVoyeeeKIN5NQgaDwyrb53emZ3asNGc491ars6lTOafUbTt+Ckf//+9hz5888/zcsvv2wD5DShMNz5pevr2muvdZxgoHpTQZFuKwU5UVtZn63n09CAcv1b14P+dj2TuAV+i9rzPXr0sJMC5LjjjrN/V6R1fqtWrSLqA4r2Hum070g4nW9uq0+59ZdF+ryk80rPrLr+dJ3q2TQ3utdqgoUCvULpWOaWiTSUnkl1r9HxDg7s0Dmgds2UKVPs/S7cOfDFF1+YG264wTEwVM9VCpp1WmHLSTr3L+W2qpb6VvykrMpZ12mkz3vhnltCv0tl+FRgv57PZ8yYYX7//fewk1WUOVKBojpHoqHzXP096tcJpcDxU0891T7La0KWW/+VJkPovNZLqyQo4EnnjtM5HO894JtvvrErgCkQSu1StzIFl01tUJUt9HNVTrUtFLQbKf2sgrp0rautE0x1tc5t3ZPUngi3opjaCLpnKKA+dNUy1V9qd4fjdh26nX9u94zQ52W3e0a0z5qh9wsd99B2QTRldXueibf9Ecn95cgjj7TB22eddVaOPh61a3/55RfbD6p2Yrg+HdWxmtwaOsFLyQref/99x+yvqUgTfY8//ni7Eo3TM7yyFyeS2nh16tSx17U+K7dVeXR9KUBSq7GETtzS86z6iaNp73lF54PT/TjWe4jT/dpLTtea23iirkGv+2R1b1PyHK16o+9N9wFdd7o3uV13aidozMhtRQ8F8kfbzgtu92uikCbkhLZx1P+ua0T3KSX2CddvoucQncvqtw9eZUh/k8ZWcuvDdxuTjnZ8InRcxu38jKZ+daq307ldmsrjLgCQV+QLRJq+AQAAIIgGZfQAHw91lCe6806Dx1pizAsaoIpmMDprqdlLLrnEdWBGWWSUhVgZ3RRwqY5yDWCoM1nfsQYxQikDmwa86tWrF/XfoIFEBd1mBQg6UXCAAifUWajgQXUGqPwqjz53+/bt9udUXnXw6qE+0gw2ymjct2/fbO/pHIgmA04oBTarbMH092mJtnhotnmXLl1MMqlDO9JMOep0yerc08CtOmMVaKEAZXXYKruZOn1//fVXO1jrNmteAy7PPvusDaaOlwbcNMjjNvBRpUoV2/ms814DWCqnOhsVuDpp0qSDHWH6O/Se/j63TkovHmM0kOS23Hgk9J1//fXX9hjovI5mifDc6r9MvFbD7TseqVbPBtO1dc8994Stv3QMdW3ofNJxX7JkiQ3oDw5kUWD3G2+8EfU9LZn1mDJ9xbJEdTIo2C3cteRlcHks37kX96zgAX+nDGqx0nmn+4PT5BcFGGvgXXW17kMaWNR1qHNXwRVu9wIF7GggKreByFQ5PsmkwUwNrIRScLDquVjF+/0l43vz8jqI5JoXDUBFmiFb91kF+OiljKSqszUApfaVMt1qoE+D/KoHdV9xumZEQTgKWtHAbDTHRe25eO9JieTFvV33c61I4xSEqqxk6UDtUw1qPvDAA65LTytYVeed/quAmZIlS9rzSIFOar/rWOsccgtu0HmjLIcKinILss7L9b3TM3u8bVin8zsR5XTiVnY9SygIQsEIWd+/yqR/Z7UttSqR2pb6WR2nBQsWOH6GzkEF6ylwKxbKnOy2wlBWAKHKpPNez4A6n5RpUm1xBVQGB9IpMFn1bDTt3dDnQi/7gGJ55vTimcnpnMutD07BM/qOtXKU6hfdpxRYomdCBeSpb0DPhArKdQtyVX9ENIGewTTBRHWTU/Z3BWMrEFVBcapDdE7ovNVzj9o4+q8TPb8qy7fut5FK5/4lfT/hJrcpQD6azPde08TzgQMHGr/p/NH96sEHH7T1Vyx0XeiZyWkSfRb1Hyh7qPp7NCFB92oFoukYafJWVpCX7sWqQx999FE7ETOee4DKFG4fun5UV6tM6tvQtaF7ga47BWvp79F17hTUpTpB5Y510qP673QOaoKFE5VFk7hUh6tsCkJTW0iZvH/44Qdbrqw+qtB+OR3LZD0rhD4ve3XPCL1f5HYsY32eibf9EU2bXe2Gdu3aOU6yErU1tF2JCnTMdc/RdaH7jJJh6BUcsJl1/aq/S8Hg6UR1n+rAUJqQ5pbgIxa6T4SbkKtn0rPPPtvWAXqOyArM1vWp80X9/+rLdLoXqz2uZC9aDcMP0fabx3K/9pIXfVCR3Pvdvhddq3ppIriyVgdfdwrKVgC1rjsFC6ue1Xeh52a38R9NLtbzY7x/l+6D6ptSve5E90vVCxqzUPl0j9Rzrp5HdP/UeE/o86basJpspHtjMsakncZlvDg/nertdG6XpsO4CwBkPAV0AwAAROubb75RT2Vcr7/++ivh5Xz77bfjLmfWq3PnzjGVYfXq1YEOHTp4Uoa2bdsGli9fHtd3ovK0bt06rnI0btw48Pfff9v9Va1aNeLf69OnT47yRPP7Tq/mzZvn2KfOrXi/a507fpg5c2bgvvvuCxx77LGenbturzZt2gT++OMPT8uv80LnR6xlOuqoowK//vrrwf25/ZxX9F3HUs4jjjgiMGPGjIP70bntdf2XaddquH3HK9Xq2WCffvppoEyZMjGVJV++fPYcPXDgQEz3tGTWY3Xr1vXk+0/ES+dubrz6rFi+cy/uWVkvtc+89vPPPwdOOeWUuMtWsWLFwIcffhhTGfw8Psl21lln5Sh3y5Yt49pnOnxvXl4HkVzzWebOnRt47LHHAieffLLndU/oq2HDhoFp06bF9P388ssvCS+f3/f2Tp065dhvrVq1Dt4D08mqVasCN998c6Bo0aKefs9q1/30008JK3cm1PdOz+zxtmGdzu9ElNOJW9mzniWWLVsWOPHEE2MuxzHHHJPt2StWQ4cODZQsWTLmcuhaefHFF2Nq7yayDygWXjwzOZ1zO3bsCLz++uuBU0891bO/Ty8dt+eeey6wf//+QLwWL17s2I6J9lWqVKnAwIEDY6r/07l/6YUXXgi7j9GjRwf8dPvtt3t67kXzyp8/f6BRo0aBAQMGBNavX+/J37N169bA1VdfHVe5jjvuuMCcOXPs/nTuxHsP0L66detmrwEvv78WLVoE5s+fH/d3pmty0KBBnpRP1+rYsWOT/qwQ+rzs1T0jVDTng9v34yTZbfYtW7YEbrnlFnsNxvvZZ5xxhn32SkcbN24MFCtWLMff5HWfuu6jPXv2DBx66KGe1gHqr0vkM0Qkou03j+V+7SUvyhjJvd/te8m6VtU+i+f+W7hwYXvv9NKePXsCDz/8sN13vN+RnmOi6SPxYkzaaVzGi/PTqd5O53Zpuoy7AEAmC782EwAAAOKmGcdahlrLWynDULQZjzRLXRmtNMteWV80Ez/e8mgG9FtvvWVn80dDGQm1bJ9m1Ov/4T1ls9FypcqAopnsygKi5fQiWT45EsqUoKVQlblH54FTJtB46LzIWnItmjIrY4MyGetv1pJuyaLvWtlhlFEmEsqsqew9ynoS7dK+0eJaTd96NpiyWChrq5a4jyZDpbKUKqOTzlGvshMDsVDGFGVU0dLksSxpq8xR/fr1M4sWLTJXXnllQsqYSZTZN5RWgwi3FDFip+XulRVQS4crS9Nrr71msw/mthR5NO0bZT3VMdQy9aeffron+800yh6rDMKhevfunZb3wMMOO8y2hZWNS/WfVlOJlTJcduvWzbavlQlN2XcThfo+/SgTnJb8fuKJJ0yFChWiOq+UTVYZ4b149urYsaPdl56Tomnv6tlKz7pa3UBZLuFOmU+V9X7mzJn2WVRZ0ZX9MJ7nJ2UbVb/DXXfdZY9FvJSZT0vFKwuwstNGW38r8+i9995r65DbbrstLev/eNxyyy02y74TrXTWtm1bk6l0/hUtWtSuLFCrVi37PSjju+opnU9qJ/z444/m9ttvtz/jBWX3Vlb6UaNGmYYNG0bdr6bndLUfTzrpJOMV7UttUWXBVPZMZdKNJkN9MF0/ug7VT6Ks1MrcHy/tUyspKKu+VpzR9xAtZfJ86qmn7LONMnojtelZRll9dZ/WvV7XabS0+oDalurf0rNXOipbtqwZPHjwwazJeqm/3us+dd1Hn3nmGbuapZ6N9J3Hcp0FjzGo3GorJvIZAom9Pw4YMMDW49EcQ9XXajf89ttv9t7pJa28omzWyuKvtkssq0bpuVPjFmrT0keS+lJ53AUAMlk+RXX7XQgAAIC8ZM2aNXZAQoGWCl7VMllahnLv3r22A0SdhAoC0PJ5Chht06aNXZo4EdQU1BKICjJReVQWLYmlpfo0EKsBNXUmZpWjZcuWngz0ITY6Phoo1+BJ1kudvDpe27Zts8uX6jxSx5qWsVSnr86dmjVr2sFedZCpw0ydKMmwb98+M378ePtSAPnSpUttWfX5Ote17JwCXFq1amU7g5wGqtwGcb1+jNGShJMmTbLLwOpa0PLkWgpcf4MGDDXooMEwDSp6scxrtLhW07eeDabzSsuKK9BBy55rqXEtj6ml53UMNbGgUaNGdqnCWAKpgGRQ4I864RVUoQGcFStW2HuQ6lHde3Rt6b6jwAjVRQrIyMv1USy0RG3ocuaaFKJlaJE8Wk5Yg99awj6r3aXljLPaXXrt3r3b3ocVaKd6vFKlSvZ+rMF9TczRdRBNcGNe9fDDD5vHHnss23u6RytwMVPqD50/astpyWIFK2oCgZalV/tdbWMtma6XAm3V7tTfn9VO8escor73n9sy3aHLhMuuXbvs8dLzjAKv9OyV1f5V4KImeepZUO1e3WfU/kxUe1ftcJ3vCtLQM5XOdT1XqRwK/FcdqQAvBSuq3kTs1B+gCR96tlCApO5Z+r6z7lUKvtNztp5/9KyhekUBQbpmow3GiNby5cvtuaDgD01w1b9VLp0Lum9qEoLumXruOfPMM22Z1JeRl+3YscMGMSv4Ue0QTdq4/vrrTc+ePZPWj5NXqc2hPiFdT7qO1N+iOlTnpO7N6gfS9aP+KwUlJeverDpcbQcFj6tvQ9e5+hF0LW3ZssXek9XHoTbE4Ycfbk4++WRbztatWyd8cr+u5cmTJ5spU6bYPj+1dRR8rz6OIkWK2Honq12j9ozqffV3IH3pmlAfr64TBYqqPbJp0ybbBtE9Xsdcdbsmi+mY63pRWxGxO3DggL2+9FyaVQeoTa7rX6+sdp7qgKzvXu29s846y5OJHEisvn372uDoUJo0oG2h1LYeO3asnSyu5zPVuWo76J6koNus8YtLL73Us0nquVGdP3HiRDtpQ+NWegZZv379wf5u1QsaW9GkJU0yUJvPy8lQSL5UHXcBgExDQDcAAACAlJWsgG4AAFKFgvc0EUuBGsH3Q2XnJQgCmUbBP5rgp0CQLAoyVBCxBnyBvCyagG4AAAAAmRvQDQAA8g5SZgAAAAAAAAApQtkYtYRz6ESmm266yWbCAzLJrbfemi2YW3r37k0wNwAAAAAAAAAgzyGgGwAAAAAAAEghXbt2Nddff3229+bMmWN69erlW5kAr73wwgt2eeZgWo734Ycf9q1MAAAAAAAAAAD4hYBuAAAAAAAAIMW8/PLLpmXLltnee/75580rr7ziW5kAr4wePdr07Nkz23u1a9c2H330kSlQoIBv5QIAAAAAAAAAwC8EdAMAAAAAAAAppnDhwmbMmDGmSZMm2d6/7bbbzHvvvedbuYB4KSt3x44dzYEDBw6+d8wxx5jJkyebcuXK+Vo2AAAAAAAAAAD8QkA3AAAAAAAAkIJKlChhvvzyS3PhhRcefG///v2mc+fOplevXtkCYoF0MGDAAHP++eebHTt2HHzvtNNOM1OnTjWHHXaYr2UDAAAAAAAAAMBPBHQDAAAAAAAAKap48eJm1KhR5v777zf58/9fV97TTz9tXnjhBV/LBkRjxIgR5s4777STErJ07drVfPPNN6ZChQq+lg0AAAAAAAAAAL8R0A0AAAAAAACksAIFCpgnnnjCTJo0ydSoUePg+1u3bvW1XEA0gs/XihUrmk8++cS89dZbplixYr6WCwAAAAAAAACAVEBANwAAAAAAAJAGWrZsaebOnWv69u1rSpcu7XdxgKgVKVLE3HbbbWbBggWmQ4cOfhcHAAAAAAAAAICUUdDvAgAAAADAvHnzzMqVKyP++a+++irHe+XKlTP169f3uGQAAKSWokWLmj59+pgePXqYZcuW+V0cIGKNGjUyS5YsMUcccYTfRQHS4llo165djj8/bdo0s3jx4mzv8SwEAAAApJ6NGzea2bNn53h/6dKljj+v953GPpo2bWr7gwAAQObLFwgEAn4XAgAAAEDe1qVLF/Puu+/GtY/mzZubKVOmeFYmAAAAAEg0noUAAACAzKQ2+plnnhn3fv766y9TrVo1T8oEAABSW36/CwAAAAAAAAAAAAAAAAAAAAAAeRUB3QAAAAAAAAAAAAAAAAAAAADgk3yBQCDg14cDAAAAAAAAAAAAAAAAAAAAQF5Ghm4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxDQDQAAAAAAAAAAAAAAAAAAAAA+IaAbAAAAAAAAAAAAAAAAAAAAAHxCQDcAAAAAAAAAAAAAAAAAAAAA+ISAbgAAAAAAAAAAAAAAAAAAAADwCQHdAAAAAAAAAAAAAAAAAAAAAOATAroBAAAAAAAAAAAAAAAAAAAAwCcEdAMAAAAAAAAAAAAAAAAAAACATwjoBgAAAAAAAAAAAAAAAAAAAACfENANAAAAAAAAAAAAAAAAAAAAAD4hoBsAAAAAAAAAAAAAAAAAAAAAfEJANwAAAAAAAAAAAAAAAAAAAAD4hIBuAAAAAAAAAAAAAAAAAAAAAPAJAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAAAAAAAAAAAAAAAAAAAPiEgG4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxDQDQAAAAAAAAAAAAAAAAAAAAA+IaAbAAAAAAAAAAAAAAAAAAAAAHxCQDcAAAAAAAAAAAAAAAAAAAAA+ISAbgAAAAAAAAAAAAAAAAAAAADwCQHdAAAAAAAAAAAAAAAAAAAAAOATAroBAAAAAAAAAAAAAAAAAAAAwCcEdAMAAAAAAAAAAAAAAAAAAACATwjoBgAAAAAAAAAAAAAAAAAAAACfENANAAAAAAAAAAAAAAAAAAAAAD4hoBsAAAAAAAAAAAAAAAAAAAAAfEJANwAAAAAAAAAAAAAAAAAAAAD4hIBuAAAAAAAAAAAAAAAAAAAAAPAJAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8U9OuDAURv06ZN5ttvvz347ypVqpgiRYr4WiYAAAAAAAAAAAAAAAAAAIB0sXv3bvPPP/8c/Hfz5s1N2bJlfS0TAd1AGlEwd7t27fwuBgAAAAAAAAAAAAAAAAAAQEYYPXq0adu2ra9lyO/rpwMAAAAAAAAAAAAAAAAAAABAHkZANwAAAAAAAAAAAAAAAAAAAAD4pKBfHwwgelWqVMmR5r9mzZq+lQcAAAAAAAAAAAAAAAAAACCdLF682LRr1841NtMPBHQDaaRIkSLZ/q1g7tq1a/tWHgAAAAAAAAAAAAAAAAAAgEyKzfRDfl8+FQAAAAAAAAAAAAAAAAAAAABAQDcAAAAAAAAAAAAAAAAAAAAA+IWAbgAAAAAAAAAAAAAAAAAAAADwCQHdAAAAAAAAAAAAAAAAAAAAAOATAroBAAAAAAAAAAAAAAAAAAAAwCcEdAMAAAAAAAAAAAAAAAAAAACATwjoBgAAAAAAAAAAAAAAAAAAAACfENANAAAAAAAAAAAAAAAAAAAAAD4hoBsAAAAAAAAAAAAAAAAAAAAAfEJANwAAAAAAAAAAAAAAAAAAAAD4hIBuAAAAAAAAAAAAAAAAAAAAAPAJAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAAAAAAAAAAAAAAAAAAAPiEgG4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxDQDQAAAAAAAAAAAAAAAAAAAAA+IaAbAAAAAAAAAAAAAAAAAAAAAHxCQDcAAAAAAAAAAAAAAAAAAAAA+KSgXx8MIG8JBALmwIED9r8AACC95cuXz+TPn9/+FwAAAAAAAAAAAAAAAPEhoBuA5/bv32+2b99uXzt27LD/1gsAAGSWAgUKmEKFCpkSJUqYkiVLmmLFihHkDQAAAAAAAAAAAAAAECUCugF4Qpm3N2/ebDZt2mR27tzpd3EAAEASZE3a2rVrl1m/fr0N8C5VqpQ59NBDbaA3AAAAAAAAAAAAAAAAckdAN4C4KYB71apVNpgLAADkXQru1uSuLVu2mIoVK5qyZcuSsRsAAAAAAAAAAAAAACAXBHQDiIuycq9cudLvYgAAgBRy4MABO9lLk74qVapEUDcAAAAAAAAAAAAAAEAYBHQDSEgwtwK3ihcvbkqWLGmKFCliChQoYF8EdAEAkP4CgYDNxr1v3z6zY8cOs23bNrN7927HtkL+/PnNYYcdRhsAAAAAAAAAAAAAAADABQHdAGKijJtOwdyFCxc2FStWNCVKlLABXAAAIDMVKlTI/leTt3Tv37Nnj1m3bp0N4g62ceNGU7RoUVO2bFmfSgoAAAAAAAAAAAAAAJDaiLYEEFNWzlWrVuV4v1y5cqZGjRqmVKlSBHMDAJDHaFJX5cqVzZFHHplj29q1a82BAwd8KRcAAAAAAAAAAAAAAECqI+ISQNSUeXPXrl05grkPO+wwky9fPt/KBQAA/KeJXQrsDrZv3z6zYcMG38oEAAAAAAAAAAAAAACQygjoBhC1TZs25cjISTA3AADIUqZMGVOiRIkcE8IAAAAAAAAAAAAAAACQEwHdAKKyf/9+s3PnzmzvVaxYkWBuAACQTYUKFbL9e8+ePWbv3r2+lQcAAAAAAAAAAAAAACBVEdANICrbt2/P9m8Fcodm4AQAAChatKgpUKBA2HYEAAAAAAAAAAAAAAAACOgGEKXQQKzixYub/PmpSgAAgMkx6UvthGA7duzwrTwAAAAAAAAAAAAAAACpiihMAFEJDcQqWbKkb2UBAACpLXQVj507d/pWFgAAAAAAAAAAAAAAgFRFQDeAqOzfvz/bv4sUKeJbWQAAQGoLbSeEtiMAAAAAAAAAAAAAAABAQDeAKAQCgRyBWAUKFPCtPAAAILXlz5/9cUPtCLUnAAAAAAAAAAAAAAAA8H8I6AYQsQMHDuR4j4BuAADgxqmd4NSeAAAAAAAAAAAAAAAAyMsI6AYQMaeMmvny5fOlLAAAIPU5tRPI0A0AAAAAAAAAAAAAAJAdAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAAAAAAAAAAAAAAAAAAAPiEgG4AAAAAAAAAAAAAAAAAAAAA8AkB3QAAAAAAAAAAAAAAAAAAAADgEwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxT064MBAACQmqZNm2Z27twZ0c8WK1bMNGnSJOFlAgAAAAAAAAAAAAAAADIVAd0AAADIpmPHjubvv/+O6GerVq1qli1blvAyAQAAAAAAAAAAAAAAAJkqv98FAAAAAAAAAAAAAAAAAAAAAIC8ioBuAABS0OjRo02+fPkS+ipZsqSpXLmyOf74402rVq1Mz549zdChQ82KFSv8/vORh0ycONE0bNjQFC1a1Bx55JHmoYceMnv37vW7WAAAAAAAAAAAAAAAAEDSFEzeRwFAbP64LJ/fRUCKqTM84HcRMsL27dvt67///jN//vmnmTx5sn0/f/78pmXLlua6664zl19+uQ3+Rt6ybNkyx/eqV6/u6ed8/vnnpm3btubAgQP23//++695/PHHzZIlS8yHH37o6WcBAAAAAAAAAAAAAAAAqYoM3QAAIBsF13711VfmyiuvNM2aNTPz58/3u0jIUL179z4YzB3so48+Mr/88osvZQIAAAAAAAAAAAAAAACSjQzdAACkoCZNmphJkybleH/16tXm6quvdvydTp06mWuuuSbsfnfs2GG2bNli9/Pbb7+Zn376ySxcuND156dOnWrq169vJkyYYJo3bx7DXwK4U2b4cNvq1auX1PIAAAAAAAAAAAAAAAAAfiCgGwCAFFShQgXTqlWrHO8vW7bM9Xdq1Kjh+Du5mTlzpunfv78ZNWqU4/adO3eaCy64wHz99demQYMGUe8fcFOrVi0zb948x201a9ZMenkAAAAAAAAAAAAAAAAAP+T35VMBAEDKUJD2yJEjzVtvvWXy5cvn+DPbtm0z119/vTlw4EDSy4fM9dhjjzmec23btmXyAAAAAAAAAAAAAAAAAPIMAroBAIDVtWtX88QTT7hu/+2338w777yT1DIhs7Vv396MGTPG1K9f3xQuXNhUqlTJ3HvvveaTTz7xu2gAAAAAAAAAAAAAAABA0hDQDQAADrrrrrtM1apVXbe//fbbSS0PMt8FF1xgZs2aZXbv3m1WrlxpnnrqKVOkSBG/iwUAAAAAAAAAAAAAAAAkDQHdAADgIGVJ7tKli+v2H3/80WzatCmpZQIAAAAAAAAAAAAAAACATEZANwAAyKZFixau2/bv32+mT5+e1PIAAAAAAAAAAAAAAAAAQCYjoBsAAGRTs2bNsNtXr16dtLIAAAAAAAAAAAAAAAAAQKYjoBsAAGRTvnz5sNvXrl2btLIAAAAAAAAAAAAAAAAAQKYjoBsAAERl3759fhcBAAAAAAAAAAAAAAAAADJGQb8LAAAAUsv69evDbj/00EPj/oz58+eb77//3vzxxx/m999/N//884/ZvHmzfRUpUsSUK1fOHHLIIaZq1arm9NNPN02aNDENGzY0hQsXNom2ceNG8/nnn5uZM2ean3/+2axcudKWa8uWLaZ48eK2bBUqVDB16tQxJ598smnQoIE57bTTTIECBRJWpgMHDphvv/3WTJ482cyYMcMsWbLEZkrfsWOH/U5KlSplqlSpYo455hjTuHFjc84555hatWolrDyZSOfgxIkTzS+//GJWrVpldu7cac9Bne+1a9c2Z599tqlWrVrSyrN9+3YzYcIEM2XKFDNnzhyzdOlSs2nTJrN7925TtGhRU7ZsWVselU3Xx3nnnefJtQkAAAAAAAAAAAAAAIDkI6AbAABks2jRorDbFTQcrUAgYAOSP/74Yxukunz5ctef3bt3r9m2bZsNsFUg65gxY+z7lSpVMj169DA33XSTDar2moLL+/XrZ0aNGmWDZp0oqFuvv//+28yaNevg+ypP69atzUUXXWQuvvhiU6xYMU/KtHXrVvPSSy+Zl19+2QaWO1HgsV5r1qwxs2fPNh999JF9/9RTTzU9e/Y0l112mcmf3/9FWZYtW2aqV68e8c937tzZvPPOO7n+nILqdZ5EQhMEVI5g+s4eeOAB8+WXX9rzNBx9p/379zetWrUyiaJzS58xdOhQG9TtRO/r9e+//5pp06aZ119/3RQsWNAGdd9///2mUaNGCSsfAAAAAAAAAAAAAAAAvOd/dA8AAEgp33zzjeu2EiVKRB0s+uSTT9ps0WeeeaYZPHhw2GDucP777z/Tu3dvU6NGDfPZZ58Zr+zfv98G9J5yyik24NwtmDu3rN6ffPKJ6dixow08v/nmm22AeDwUWK7geZXNLZg7HAWcX3HFFTbDebxlyUTKev7www/bzO/KzJ1bMHfWd6pM3ddff709b7y0b98+8/jjj5tjjz3WXiduwdzhfl+TH5ShvVOnTmbDhg2elg8AAAAAAAAAAAAAAACJQ4ZuAABw0J49e8y7777rul0By4UKFYpqn3379nUNki5fvrzNal2/fn1z+OGH24DxzZs3m9WrV5vp06ebyZMn2/8PtmnTJtOuXTvTq1cvGyweDwXBXnXVVWb48OE5tpUtW9ZmPFYG6GrVqpmSJUva70eBsr/++quZNGmSmT9/fo7fU/lfe+01mzU7kgzToRQofO+995rnn3/eNcv0pZdeao4//nj7nSmLt4LdFYivMu3atSvbz+t7VBD+e++9Z9q3bx91eTI1mLtLly7m/fffj+n333zzTZtFXhMAvKBzSpnUv/76a8ftuj7atm1rz0NdM+vWrbMTI8aNG2ePb2gwurJ7//DDD2bs2LHmhBNO8KSMAAAAAAAAAAAAAAAASBwCugEAwEHPPfecawZtBVsrW7QXSpcubfr162duvPFGU7hwYdefUwD1Rx99ZO655x6zdu3abNueeuopU7FiRXPXXXfFXI477rgjRzC3yqNMybfddpspUqRI2N9XALWycS9ZssR4GWisgNxQhxxyiHnhhRfMNddc4/i7d955p80UrmBwBRwHB/kq23OHDh3sd6nAYT8o+FzfV6gvv/zSPPPMMzHvV9msFdQeTJMArr76atff0XkcHMzdoEEDGzCtYHl9z/oeFy1aZD7//HMze/Zsx30oI/uFF15oJznEQ8HZLVu2NL///nuObbVr1zZDhgxxzYr/4IMP2kkF3bp1M99//322bUuXLrVZ8b/99ltz3HHHxVVGAAAAAAAAAAAAAAAAJBYB3QAAwFI2abeA7Xz58tnM3UcddVTcn3PooYear776ytStWzfXn1VwdefOnU2bNm1stuxffvkl23YFL59yyimmRYsWUZdjxowZ5tVXX83x/ogRI8wFF1wQ0T7OPvtsm637nHPOMdOmTTPxuvvuux2DuStXrmwDdmvUqBH298uVK2feeOMN06RJE3PttddmC+pW5m8Fg9eqVctmHU+2okWLmlatWuV4f8WKFXHt97TTTsvx3rJly1x/fsqUKXYygCjQ+a233jKNGzd2zS4/cuRI+10q83ooTSa45JJL7N8WC2Wu17nmFMytYOzx48fnum9lalfQ9vXXX2//lmBr1qyxGfBnzZplJ1EAAAAAAAAAAAAAAAAgNeX3uwAAAMBfCvZUUGrXrl2zBQBnKV68uPnwww/tz3hBQaeRBHOHZnceN26c/W8wBSkrqDsWTz/9tM2IHaxdu3YRB3NnKVmypBk7dqypUKGCicewYcPMgAEDcryvgF5lsc4tmDuYsnwrA3qoXbt2mSuvvNLs3bvX5EU6X5R5Xee5JgLMnDnTNZg7S/v27c2oUaNM/vw5m80KmB4zZkzM5VFW9enTpzsGaWu/kQaKa8KFAvk1sSCUMo3Hk8UeAAAAAAAAAAAAAAAAiUeGbgAAMsTSpUtt5utwdu7cabZu3WpWr15t5syZY3766SezYMEC159v27atefLJJ20mYy8oc/SFF14Y0+9WqlTJZky+6aabsr2voNxJkybZbNmR0vcwYcKEHO8rm3EslBm7V69epmfPnjH9/vr160337t0dt913332mdu3aUe9TZXn//ffN/Pnzs73/559/moEDB8Zc1nSmbOB66XhpgoCC8SOhbNkdOnQwH3/8cY5tH3zwgd0WLWXVfu211xy3KXN8pGXLooDzQYMG2XNFmb9DJ1F069bNNGjQIOpyAgAAAAAAAAAAAAAAIPEI6AYAIEMoeFeveCgj8IknnmjOO+88c/nll9tMwV5S5uh4KIu4sg3v2LEj2/tvvvlmVAHdc+fOzbEPOeKII2IumzI5xxok/cQTT5i1a9fmeL98+fI2UDwWhQoVMg899JC56qqrcmx79tlnTY8ePUyRIkVMXvTAAw/kyPaem86dOzsGdH/33XcxleHuu+92zIiva6958+Yx7fPoo4+25Xz99dezva/PUcb20aNHx7RfAAAAAAAAAAAAAAAAJFbOteMBAECeVKFCBXPNNdeYG264wdNg7mbNmtkAVb1atmwZ174KFy5szjjjDMdsx9H477//HN9X9vJYVa9e3RQrVizq39uwYYPNyOykU6dONsg+Vso4Xrx48RzvK0P7p59+avIiZb6+5ZZbYjqPCxQokOP9TZs2meXLl0e1L2WHnz17tuO2G2+80cTjyiuvdHx/zJgx5p9//olr3wAAAAAAAAAAAAAAAEgMAroBAIClDNHK7KuA0hNOOMHUqVPHPP7442bjxo1x7ffLL780U6ZMsa8aNWrEXU4FTodatWqVWbhwYcT7cAvcVhnj8dhjj5k+ffrYV7t27SL6HWVV37lzp+O2q6++Oq7ylChRwrRq1cpx24cffmjyovPPPz+mwHsFxteqVctx2x9//BHVvkIzaGcpW7asLV88FHiu/YRSlm6nDOMAAAAAAAAAAAAAAADwHwHdAABkCAURK2gzt9eePXvMmjVrzKJFi8y4ceNM3759TZMmTXLsb+7cueahhx6yQdhPPvmk2bdvn0kFhx56qOP7CxYsiHgfhxxyiOP7b731llm8eHHMZbv77rvt96lXpAHdw4YNc3y/TJkypl69eiZedevWdXz/m2++sedCXtOmTZuYf9dtQsK6desi3se2bdvM+PHjHbc1bdrUFCxY0MQjf/785sQTT3Tc9sUXX8S1bwAAAAAAAAAAAAAAACQGAd0AAOQxhQoVMhUqVDA1a9Y05513ng0Enzp1qpkzZ45jduBNmzaZ+++/37Ru3TqqwNVEKVCggOP769evj3gfJ510kuP7O3bsMGeffbaZOXOmSQZlP58+fbrjNgXZKzg3Xm7BvcoKPmPGDJPXKPN8rA4//HDH97ds2RLxPsIF0iu7thfcjvmPP/5o9u7d68lnAAAAAAAAAAAAAAAAwDsEdAMAgINBzmPHjjX9+/d3DURVkLECvFPRhg0bIv7ZypUrm1NOOcVx27Jly0yjRo1M586dzR9//GESadasWWb//v2eBx5HEoQsCuLPa44//viYf1dZ051s3bo14n389NNPrtsSfcwVxL9w4UJPPgMAAAAAAAAAAAAAAADeIaAbAAAclC9fPnPfffeZm2++2XG7gkGvvPJKc+DAgbg+599//zVDhw41d999t2nZsqU57rjjTKVKlUzx4sVtGcK9HnnkkbiDakV/pxv9fe+9957NdKysya+99lpCspOHC6iuXr26J59RqlQp123z5883eUmRIkVMyZIlY/79YsWKOb6/e/fuiPfBMQcAAAAAAAAAAAAAAECogjneAQAAed7zzz9vRowYYdasWZNj2xdffGE++ugj07Fjx6j2GQgEzKeffmqGDBlivvrqq7iDwp32H43LLrvM/g0ffPBB2J/7/vvv7at79+7mjDPOMBdffLFp166dOeqoo+Is8f+ygbtZv369/Z7ipeB5NytWrDB5STzB3FKoUKGEHvPFixd7ckyWL1/uui2vHXMAAAAAAAAAAAAAAIB0QEA3AADIoWjRoubWW281ffr0cdz+2GOP2Uzd+fNHttjH3LlzzU033WSmTp1qUsnbb79tA8E//PDDXH92//79ZsqUKfZ1++23m3r16plLLrnEBoVXq1Ytps9ftWqV67YHH3wwpn169fmZqESJEn4XIex3fuGFF/r6+QAAAAAAAAAAAAAAAPAHAd0AAMBR+/btXQO6FyxYYGbMmGEaNWqU637Gjx9v97V7927H7Yceeqhp1aqVadCggalUqZIpW7Zs2EzI7733nnn//feNF/Q5ytCtz+/Zs6fZsGFDxL/7yy+/2NdDDz1kmjdvbu655x5z3nnnRfX527dvN37atm2byUvy5cvndxE45gAAAAAAAAAAAAAAAMiBgG4AAODohBNOMKVLlzZbtmxx3D558uRcA7q/+OILc/HFF5s9e/bk2FamTBnzzDPPmGuuucYUKVIk4nIlIst3165dbbbtQYMGmZdfftmsXLky4t9Vhu+szN2nnnqqGTx4sDnllFMi+l23IPdk2bVrl6+fnxdxzAEAAAAAAAAAAAAAABAqf453AAAA1EjIn9/Url3bdfvMmTPD/v7mzZttoLRTMHflypVtdusbbrghqmDuRFLweu/evc3y5cvNhAkTTMeOHU3JkiWj2sesWbNMw4YNzYABAyL6+XB/+1dffWWDxRP5Wrx4cVR/H+IX7pjv27cv4cd8yJAhSf17AQAAAAAAAAAAAAAAkDsCugEAgKuyZcu6blu7dm3Y333kkUfMqlWrHLcNHz7cVK9e3aSiAgUKmHPOOccMHTrUrFmzxgwbNsxmGS9atGhEv79//35z5513mqeeeirXny1RooTrtm3btkVVbqQHjjkAAAAAAAAAAAAAAABCFczxDgAAQAQB3evWrQsb1Pzee+85bjv33HPN6aefbtJBsWLFzGWXXWZfW7ZsMSNHjjQffPCBmTx5ss12HM4DDzxgmjVrZho3buz6M5UqVXLdRnBvZtIxX79+vesxL1OmTNLLBAAAAADIbH9cls/vIgD4/+oMD9+nCAAAAAAA8i4ydAMAAFd79+513ZY/v3sz4rvvvnMNWr300ktNOipdurTp0qWLmTRpklm4cKHp0aOHKVKkSNig9ocffjjsPqtVq+a6bfPmzXGVF6mJYw4AAAAAAAAAAAAAAIBQBHQDAABXmzZtct1WsmRJ122//vqr67Z0yc4dTs2aNc2LL75oFixYYFq0aOH6c1999ZVZtWqV6/a6deu6blu0aFHc5UTq4ZgDAAAAAAAAAAAAAAAgFAHdAADA1Zo1a1y3HXXUUa7bwgUxH3744SZTVK1a1Wbsbt26tevPTJ061XVbgwYNTIECBRy3zZ0715MyIrU0btzYdRvHHAAAAAAAAAAAAAAAIG8q6HcBAABAatqxY4eZN2+e6/YTTjjBddvGjRtjyuwdie3bt5t4bdu2zb68CDIvWLCgef31183RRx9t9u/fn2P7v//+6/q7ZcqUsRnLv//++xzbfvvtNxMIBEy+fPmMF0aOHGmGDRuW7b37778/bMZoeO/MM880RYsWNbt27cqxbc6cOZ5+1uOPP27++OOPg//Onz+/efvtt02RIkU8/RwAAAAAAAAAAAAAAADEh4BuAADgaMaMGWbfvn2u25s2beq6rWzZsq7bNm/ebMqXLx9zudatW2fi9eyzz5pHHnnk4L8V3F2iRIm4MnU3bNjQ/Pjjjzm27dy5M+zvXn755Y4B3atXr7bH4LTTTjNeeOmll8yUKVMO/ltBvUOGDPFk34hc8eLFzQUXXGA+/fTTHNsmTpxo9uzZYwoXLhz35+i869+/v52YkUWTBwjmBgAAAAAAAAAAAAAASD35/S4AAABITW+99ZbrtnLlytlMw24qVqzouu3vv/+Oq1w///yz8dratWvj3oeCup0cdthhYX+vY8eOrsHkTkG/sVCW8KlTp2Z77+yzz447Wzpi061bN9fJDpMmTfLkM8aOHZstmFvat2/vyb4BAAAAAAAAAAAAAADgLQK6AQBADsuWLTMff/yx6/brrrsubBbhunXrum775ptvYi7XypUrze+//268Nn369Lj3oSzfTqpUqRL295TNvEePHo7b3nnnHRvkG6+BAwfmyLZ+ww03xL1fxKZVq1amUaNGrsfKC8pCH6xYsWLm6quv9mTfAAAAAAAAAAAAAAAA8BYB3QAAIJvdu3ebDh06mL1797pm577nnnvC7qNZs2amePHijtvefPNNc+DAgZjK9txzz5lAIGC8Fm8mbJVpzpw5Od4vVaqUadq0aa6/36tXL3P44YfneH/dunWmX79+cZVt4cKFZtCgQdneq1evnrnooovi2i/i8/zzz5v8+XM2xZWhe/z48XHt+/333zczZ87MkRU8t2zxAAAAAAAAAAAAAAAA8AcB3QAA4KA1a9aYCy64IEcwaLBXXnnFVKxYMex+ihQpYi677DLHbfPnzzevv/561GWbPXu2efnll00ijBgxwsyaNSvm358wYYL5559/crzfrl07U7Ro0Vx/X1m6Bw8e7LhtwIABNsg3Ftu3b7dZmXfu3Jnt/SeeeCKm/cE7jRs3NnfccYfjtptuusnxfIo0gP+2227LMQlDkwYAAAAAAAAAAAAAAACQmgjoBgAAZsOGDeaZZ54xJ510kvnqq69cf65///7miiuuiGifffv2NYULF3bcdvvtt4f9nFALFiww7du3t9nDE0EZttu2bWuWLFkS9e8uW7bMBuCGKlGihHnsscci3o8yZj/44IM53lem9Isvvth8++23UZVr48aNjsH5CvY955xzotoXEuOpp54yLVq0yPG+grlbt25tVqxYEdX+/vjjD3PmmWeaTZs25ciK75QBHgAAAAAAAAAAAAAAAKmhoN8FAAAAOa1du9bMmTMnx/urV692/Z2lS5dGFCS9b98+s3nzZhv0qUDpGTNm2OzU4YKlFZw8aNAg06VLl4j/hmrVqpl+/fqZe+65J8e2PXv2mHPPPdc8/PDD5s477zQlS5Z03MeBAwfM+++/b+666y4bdC7HHnusLXckf3+xYsVMkyZNIirvypUrTb169czjjz9urr32WtcyZdm/f7/54IMPzL333ut4XJ577jlTtWpVE41HH33UrF+/3rz66qs5Mm23bNnS3HrrraZPnz6mfPnyYY/v8OHDTc+ePe3fFKx58+Y2cD8306ZNy5HV2+3c27Vrl+N5V6NGDfsK/rmpU6fm+Ll58+Y57ve///5z3G/9+vVtxung465XPGVt2rRpjkzqygivoPhgoZ8T/H7oflVGlTWcggULmtGjR5s2bdqY6dOnZ9v2559/mtq1a9tJFNddd53Neu9G17Ky12sCQeh1fN9999kJAQAAAAAAAAAAAAAAAEhd+QJKSQkgLcydO9fUqVMnWyZOBXsli4IEFy1alO29WrVq2YC0RPrjsnwJ3T/ST53hmX/rUpBnKgRh5suXz1x66aXmySefzBacG41bbrklR4BysEMOOcScd955NvD6sMMOs1m9161bZ37++WczduxY89dffx0si8qxY8cO88gjj0T02QqoVgZtp+zh4fahYO6zzz7bBuQqML1MmTKmQIECZtu2bTZ7soLtJ0yYYAPvQ6mcAwcOND169DCxUNNMgbkqo1MzTXWusjArwLtSpUqmYsWKNvhaQcwKzv/yyy9zBHLLhRdeaIYNG5YjcNmJ/ua///7bxEOB5/obsug4VK9e3cTrm2++yZbVOrdjGQmdY/qbg+kzos2KHho8P2XKlIh+duvWraZTp07ms88+c9xeunRpm2391FNPtZm2FSyuSRk6F3/44Qd7zBX0H0rnkVPW92Tyq+0AAAAAAPg/9K8CqSMv9G0DAAAAAJAO5voci+mESAoAAJDDMcccY9q3b28zVSv4Mh7KHKxg8N69e5u9e/fm2K7M20OHDrUvN2XLljWDBw82HTp0yBYkHKtu3brZwOs333zTBsWGUuD2qFGj7CsaCljW36vs47FSuZS5XAHuKueSJUtyBMhOmjTJviKh7Or6zpQJXUHpSD2lSpWy59orr7xi7r//fhvgHWzLli3mww8/tK9IJzIoo76CwAEAAAAAAAAAAAAAAJD6COgGkPLIWAF4SwHDhQoVMsWKFbOZfitUqGCOOuooU7NmTVOvXj3TqFEjGxDq5ef17NnTtGrVyjz++OM2cPXAgQMR/a6ydXfp0sVme65cubJnZVJma+3zoYceMhMnTjQjR440X3zxhVmxYkVM+zv++ONt8Putt95qv1cvnHXWWXY24Ntvv20GDBhgFixYENXvly9f3nTs2NF+91WqVPGkTEgcXSc6fzRp4fnnnzdDhgyxmeqjnYhx3XXXme7du5vixYsnrKwAAAAAAAAAAAAAAADwVr5AIECkJJAm/E7zr6ywixYtyvaeMvcWLMjcEACRW7ZsmRk/frz57rvvzO+//27Wr19vNm7caNQkUaZiBZOfeOKJpnnz5ubiiy+2QefJovJMnz7dzJs3z9a5y5cvN5s3b7YZknft2mVKlixpSpcubYOlVR+fcsopplmzZqZ+/foJL9vs2bNtVu6ffvrJ1sUrV660mcSVdbtMmTI2i7kyoTdo0MAG5SuAXgHxSE+6506ePNlMmTLFzJo1y2ZqX7t2rdm5c6cpUqSIPd66Nk444QR7zM844wx73FMNbQcAAAAA8N8fl+XzuwgA/j8S2AAAAAAAkBrm+hyL6YRICgAAkFTVqlUzt9xyi32lGgWS65WKFDSejMBxpAYFPLdp08a+AAAAAAAAAAAAAAAAkNny+10AAAAAAAAAAAAAAAAAAAAAAMirCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAAAAAAAAAAAAAAAAAAAPiEgG4AAAAAAAAAAAAAAAAAAAAA8ElBvz4YAAAAAAAAAAAAAPB//v53oRk5cYhZuHSO2btvT1z7Klemgql/YnNz6bk3moIFCnlWRgAAAAAA4D0CugEAAAAAAAAAAADAZ6vWLjePDLzBbNux2ZP9bdm20QaIr12/0vTo3M+TfQIAAAAAgMTIn6D9AgAAAAAAAAAAAAAiNG32RM+CuYNNnTXebNuxxfP9AgAAAAAA7xDQDQAAAAAAAAAAAAA++/vfBQnZbyAQMMtXLkrIvgEAAAAAgDcI6AYAAAAAAAAAAAAAnwUC6bpzAAAAAAAQLwK6AQAAAAAAAAAAAAAAAAAAAMAnBHQDAAAAAAAAAAAAAAAAAAAAgE8I6AYAAAAAAAAAAAAAAAAAAAAAnxDQDQAAAAAAAAAAAAAAAAAAAAA+KejXBwMAAAAAAAAAAAAAAADw1x+X5fO7CACC1Bke8LsIAAAfkKEbAAAAAAAAAAAAAAAAAAAAAHxCQDcAAAAAAAAAAAAAAAAAAAAA+ISAbgAAAAAAAAAAAAAAAAAAAADwCQHdAAAAAAAAAAAAAAAAAAAAAOATAroBAAAAAAAAAAAAAAAAAAAAwCcEdAMAAAAAAAAAAAAAAAAAAACATwjoBgAAAAAAAAAAAAAAAAAAAACfENANAAAAAAAAAAAAAAAAAAAAAD4hoBsAAAAAAAAAAAAAAAAAAAAAfEJANwAAAAAAAAAAAAAAAAAAAAD4hIBuAAAAAAAAAAAAAAAAAAAAAPAJAd0AAAAAAAAAAAAAAAAAAAAA4BMCugEAAAAAAAAAAAAAAAAAAADAJwR0AwAAAAAAAAAAAAAAAAAAAIBPCOgGAAAAAAAAAAAAAAAAAAAAAJ8Q0A0AAAAAAAAAAAAAAAAAAAAAPiGgGwAAAAAAAAAAAAAAAAAAAAB8QkA3AAAA/h979wEmVX3uD/xlWXoXBERExC5ix4Yo9iheMRK9UaNoNESN95piYjSxRqNpxpJobIlR440txt4RAmKLAgoWREFA6b3X/T/n3D9cYGeWXXaWs+XzeZ7z7M75zfmd3ztznB31O+8AAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQkeKsTgwAAAAAAAAAAABUzMgPh8eoj4bH/IVzKjVPo4aNY9cd9okD9zo6GjRoWLD1AVBxAt0AAAAAAAAAAABQAzzy7B3x2PN3Fmy+V15/PF7/9wvxo/N+J9QNkKGiLE8OAAAAAAAAAAAAbNzc+bMKGuZe470xQ2PkR68XfF4Ayk+gGwAAAAAAAAAAAKq5seNHVdncH382osrmBmDjBLoBAAAAAAAAAACgmluxYnmNnBuAjRPoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGirM6MQAAVWfp0qUxbNiwct+/U6dOsdtuu1XpmgAAAAAAAAAAgNIEugEAaqGpU6fG0UcfXe77DxgwIO67774qXRMAAAAAAAAAAFBaUY59AAAAAAAAAAAAAABsBgLdAFANJZ2S69WrVyVbUVFRtGnTJrp16xb77rtv/Od//mfccsst8fbbb8fKlSsLsv5PP/00TjzxxGjevHm0bt06vvWtb8X06dMLMjfVT0lJSdx8882x/fbbR6NGjaJ79+7x0EMPZb0sAAAAAAAAAACoEYqzXgDAxky5avusl0A1s9U1n2W9hBofvp07d266jR8/Pt5777145JFH0rHOnTvHBRdcEAMHDox27dpt0vwTJ06Mgw8+OGbOnLl239/+9rd455134t///ne0aNGiYLWQX9euXdPnOteHBc4555yCnuvSSy+N3/zmN2tvf/jhh3HGGWfEwoUL02sJAAAAAAAAAADIT4duAGCtyZMnx89+9rPo0qVL3HrrrTkDwRvz29/+dr0w9xpjx46Ne+65p0ArpbpIOq/fdNNNOccuu+yygnV9BwAAAAAAAACA2kqHbgCoho499th4+eWXS+0fNWpUXHLJJTmP+fGPfxzHHHNMmfMuXrw4Zs2aFRMmTIjBgwfHm2++GcuXLy91vyVLlsTFF18cTz/9dNpdu3379uVe+8cff7xJY9RM48aNi1WrVuUcmz17dsyYMSO22mqrzb4uAAAAAAAAAACoKQS6AaAaSgKwuUKwxcX5/3TvtttucdRRR1XoPFOmTIlf//rX8Yc//CFnJ+VXXnklDjvssDT83aFDh3LNueOOO+YMoyd22GGHCq2P6q9bt25RVFQUq1evLjXWqlWr2HLLLTNZFwAAAAAAAAAA1BRFWS8AAMhOEhr//e9/Hy+++GJsscUWebtq9+vXL1asWFGuOX/0ox9F69atS+3fdttt4zvf+U6l10z10rFjx7joootyjl199dVlfggBAAAAAAAAAAAQ6AYAIuKII46Ixx9/POrXr59z/K233opf/vKX5e7YPHz48Ojbt280bdo0WrZsGaeeemoMGzYsZ9Cbmi/5UEDS6X277baLBg0axM477xz33ntvfP/73896aQAAAAAAAAAAUO0JdAMAqT59+sQPf/jDvONJYHf69OnlmmvXXXeNZ555JhYtWhTz5s2Lhx9+ODp37lzA1VKdFBUVxY9//OP4/PPPY/ny5WlX929/+9tZLwsAAAAAAAAAAGoEgW4AYK0k0N2wYcOcY4sXL4677757s68JAAAAAAAAAACgNhPoBgDW6tixY3zta1/LO/7oo49u1vUAAAAAAAAAAADUdgLdAMB6evXqlXfs/fffj7lz527W9QAAAAAAAAAAANRmAt0AwHr22muvvGMlJSXx8ccfb9b1AAAAAAAAAAAA1GYC3QDAerbYYosyx7/66qvNthYAAAAAAAAAAIDarjjrBQAA1UurVq3KHF+8eHHUBBMnTownn3wyhgwZEh9++GFMnTo1FixYEA0aNIi2bdvGLrvsEr17946TTz45dt99982ypkWLFsXzzz8fgwcPjlGjRsXnn38ec+fOjWXLlkXjxo2jdevW0bVr1+jevXv06tUrjj/++GjXrt1mWVttMWLEiPTx/eCDD2LWrFmxevXq9DHccsstY//9948jjzwy2rRps9nWM2XKlHjmmWdi6NCh6XX4xRdfpNfhqlWromnTpum6tttuu9h7773j0EMPjaOOOiq9FgAAAAAAAAAAqDsEugGACgW2GzVqtN7tJICchFTLY9ttt40JEyaUeZ9kPAm4lteAAQPivvvuW3s7Wcull14ajz32WBqa3dDKlStj8uTJ6fbKK6/EVVddFX369Ilf//rX0bNnz6gKyZpuuOGGePDBB9NQdy7J/mT78ssv4/XXX4+77roriouL01D3ZZddFgceeGBUB8ljfc4555T7/n/5y1/i7LPPLvM+Sai9IiHrDZ/zxKOPPhrXXHNNjBkzpsxji4qK4utf/3r88pe/jJ122imqyvDhw+P666+PF154IQ2V5zJ//vx0++yzz9Jr8Te/+U20aNEizjrrrPjJT34SXbp0qbL1AQAAAAAAAABQfRRlvQAAoHqZM2dOmePVuWP0I488Ej169IiHH344Z5g7n6Sj8wEHHBCXX355hY7bmCQ8ft1118XOO+8cd955Z94wd1nHP/XUU3HQQQfFmWeeGbNnzy7Y2mrT9XriiSfGqaeeutEwdyIJVz/++ONpF/R77rmn4OtJuoKfccYZaYf15557Lm+YO5+ke/cf//jH9JpJQueFvB4BAAAAAAAAAKiedOgGANYzduzYMsd33HHHqI6STtDnnXfe2gBt27Zto3///rHnnnvG1ltvHStWrIhJkybFm2++GU8++WQsW7ZsveNLSkrSLtofffRR/P3vfy/VibyikvD1KaecEoMGDco5vu+++0a/fv3SDufJWmfOnBkTJ06MZ599Nt566610PetKunsnXZ+ffvrp2G233Sq1ttoiCU8fccQR8f7771f42CQs/53vfCe9Li644IKCrGfUqFFpuDx5HjfUoEGDOOqoo+Loo49Or8emTZvG1KlT45NPPkmvx08//XS9+y9dujR+9rOfxb/+9a/0gwotW7YsyBoBAAAAAAAAAKh+BLoBgPW8/vrrece22Wab6Ny583r7/va3v8WSJUvW2zdt2rT41re+tUnn79ixY7z88sul9r/00kvxm9/8JucxQ4cOje9+97tpmLtJkyZpMPt73/teFBcX5+3qfMkll8Sf//znUmP//Oc/027YSZfvevXqbVINSTg7CRp/8MEHpcbWdIY+8MADcx7785//PA2VJ/Ukda3r888/j8MPPzyGDBkSu+yyS2Th2GOPzfn8JM9N8hxtiubNm1f4OU86V5900klrw9xJYPqYY45JQ9NrAtMzZsyI9957L+1y/sUXX+Sc54c//GF63Pbbbx+V8c4776Rh7Xnz5pUaS0Let99+e7quXJIakyB/cs1uuM4XX3wxjjvuuPSxaNasWaXWCAAAAAAAAABA9STQDQCslQSzn3vuubzjSUfpDfXq1avUvgkTJmzyGho3bpyGcjc0efLkvGs+++yz007Lbdq0ScPOPXr0KPMcyf3uvffe2G+//eLCCy8sNf7oo4/G/vvvn4a+Kyrp/H3CCSfkDHMnYezk8U1qLMuuu+6a1pF0HN8wdD59+vQ0IPzvf/87k67NW221VbptKOkgvqmS4H1FnvPEddddF8OGDUt/P/744+Ouu+7KGZhOro3f/e53ceONN8aVV15ZajzphP2DH/wgDX1vqiRo/7WvfS1nmPunP/1p+gGDjenbt2/6z1LyQYARI0asN5Z0Zj/33HPTzvEAAAAAAAAAANQ+RVkvAACoPv7617+m3aVzSbpVf+c734nq5oknnkgDtfXr109/31iYe10XXHBBXHTRRTnHkvBvMm9FJeHgt956K2dIOwkNbyzMve7jfffdd6dB4Q19+umnaWfpumrixInxq1/9Kv194MCBaXfrfN2v13TvvuKKK+Kaa67JOZ6E7KdMmbJJa1m+fHmcfPLJMXv27FJjSaf38oS512jdunXaibtTp06lxpKO8ZUJzQMAAAAAAAAAUH0JdAMAa0Oyl19+ed7xb37zm7HHHntEdZN05k4k3awPO+ywCh+fdG/u3Llzzs7fSVi4IpKu2n/6059yjt1xxx3RvHnzCs1XVFQUf/jDH6JRo0alxpLO3e+8807URa+99lr6/BxyyCFx++23l/u45PreZpttSu1ftWrVJne/Tjp/jxo1qtT+tm3bxu9///sKz9euXbv49a9/nXMs6Ri/aNGiTVonAAAAAAAAAADVl0A3ABBTp06Nk046KebMmZNzvH379psUTt1ckq7XV1999SYd27BhwzQom8urr74aw4cPL/dcP/rRj6KkpKTU/uOPP36TwuaJ7bffPgYMGFBqf3Ke66+/PuqqpIN5ck0mndnLq7i4OE4//fS8YfyKmjZt2tpO4Ru69NJL01D3pjjjjDNi5513znm+pGs7AAAAAAAAAAC1i0A3ANRxzz77bPTs2TNGjBiRc7xp06bxxBNPRIcOHaK6OuaYY6Jjx46bfHzS3TsJhedy5513lmuO559/Pt59992cYxXt9L2h0047Lef+p556KiZNmhR10RFHHBH77bffJh2Xy/vvv1/huW666aZYvHhxzg8JnH322VEZSUf8XP74xz9Wal4AAAAAAAAAAKofgW4AqEOWLFkSkydPjmHDhqXdnffee+844YQT0n25bLnllvHCCy/EwQcfHNVZ//79K3V8s2bN4tBDD8059uijj+btXL6uu+66K+f+1q1bR9++fSu1vmRtyTy5unT//e9/j7roG9/4xiYdt+eee+bcP2HChFi0aFG551mxYkXcd999eT9gkPyzUxn9+vXLuX/cuHHxzjvvVGpuAAAAAAAAAACqF4FuAKglzjnnnKhXr16ZW9Jte5tttonevXvHz3/+8xg5cmTe+U488cQYPXp0et/qrhBr/NrXvpY3BP/SSy+VeezChQvjueeeyzl2yCGHRHFxcaXWVlRUFD169Mg5lgTu66Jjjz12k45LOs0n/xzkCsfPmjWr3PMMHjw4pk+fnnOsT58+UVndu3fPe93U1eccAAAAAAAAAKC2EugGANZz+OGHx5NPPplu7du3j+ou6a7dtWvXSs+zzz775B176623yjz2tddei+XLl+ccy9f5u6LyBbrfeOONtFt0XVLZ57xjx44598+fP7/cc5QVqi7Ec96wYcPYaaedco4NGTKk0vMDAAAAAAAAAFB9VK5dJABQ47Vt2zYOPPDAOPjgg6Nfv35pZ+CaZOedd067j1fWDjvssMmB7jfffDPv2O677x6FkC+EnHQQHzt2bI173ipjl112qdRz3qpVq5z7FyxYUO45ynrOC/VcJM/5hx9+WGr/qFGjCjI/AAAAAAAAAADVg0A3ANQSP/7xj+OYY47Z6P2KioqiZcuW0aZNm3TbYostoiYr1Po7deoUTZs2jcWLF5caGzFiRJSUlOQNEZcVsN1uu+0Ksr4WLVrkHfvoo4/qVKC7Xbt2lTq+SZMmOfcvW7as3HO8//77Ofd36NAhvY6q8jmfOXNmulX2cQAAAAAAAAAAoHoQ6AaAWmK33XaLo446KuqasoLOFZGEtbfccsv44osvcnbBTro3J0H4XCZMmJB33nHjxsXkyZMrvb6JEyfmHSvE/DVJ8+bNK3V8gwYNKnV8EqZeuHBh3u7fr7zyShRCWR3Dk+dcoBsAAAAAAAAAoHYQ6AYA6nS4d13NmjXLOzZ37ty8ge6pU6fmPe4//uM/oqqVdf7aqKznaXMo6/EeO3ZsHH300ZmuAQAAAAAAAACAmqUo6wUAAFRGUVHh3s40bdo079icOXPyji1atCiylK9bdG2VdFPPUtbPd118zgEAAAAAAAAAajOBbgCA/69x48abFKBdtmxZZGnp0qWZnr+uyfr5TnjOAQAAAAAAAABqj+KsFwAAUF2UFZJt0aJF3rFGjRrlPXblypVRv379gqyP6iF5vvM55JBDYujQoZt1PQAAAAAAAAAA1Gw6dAMANdrq1asLNteiRYvyjrVu3TrvWLNmzTapszc1k+cbAAAAAAAAAIBCEugGAGq0QgZoywp0t2nTJu/YVlttlXdMwLf28XwDAAAAAAAAAFBIxQWdjcyNHTs23n333RgzZkyMHj06Pv/885g7d266LVmyJFq0aBEtW7ZMu4zusssuse+++8Z+++0XvXv3juJilwMANU+hArQlJSUxY8aMnGNNmjRJ/4bm07Vr1/Tvbi7z5s2LrbfeuiBrpHpo27Ztej0sWLAg5/MNAAAAAAAAAAAVIcFbw82ePTueeeaZePXVV2PQoEExefLkMu8/Z86cdPviiy9i1KhR8fDDD6f7O3ToEGeccUZccMEFscMOO0R1dvbZZ8df//rXKj/P+PHj04AeANXbrFmzCjJP8jc0+fBTLskHoMqy5557pn+Pc/n0009jt912K8gaqT722GOPeP3110vtTz4UkIS6W7Vqlcm6AAAAAAAAAACoeYqyXgAVN3/+/Lj//vujb9++0bFjxxgwYEB6e2Nh7rJMmzYtbrrppujevXtcddVVsXTp0oKuGQCqyieffJJ2166scePG5R3bf//9yzz2oIMOyjuWfGsGtY/nHAAAAAAAAACAQtGhuwZKgtfXXHNNmfdp165dHHHEEWkArVOnTtG6des0pJ109P7ggw9i8ODBaYfuDS1fvjyuvfbaeOqpp+Lll19O5wGA6mzRokXpN09U9lsV3n333bxjBxxwQJnHHn744dG4ceOcH4jK9fe2Mq677roYPXr02ttFRUXxl7/8JRo1alTQ81C24447Ln7729/mHEue84MPPrgg51m1alX67SQrVqxYu2/HHXeMX/ziFwWZHwAAAAAAAACA7Al01zLt27ePG2+8MU4//fSNBrveeOONuPTSS2Po0KGlxkaOHBl9+vSJQYMGpXMCQHU2bNiwSge6n3/++Zz7mzRpEsccc0yZxzZt2jROOOGEeOyxx0qNvfjii+kHpho2bBiVtWTJkrjhhhti8eLFa/clwWFh7s3v0EMPTb8pZerUqaXGkg/GXXDBBQW7th988MH19l1++eUFmRsAAAAAAAAAgOqhKOsFUDg9e/ZMO3aec8455Qp2HXTQQfHaa6/FhRdemHN8zJgxBQsjAUBV+sc//lGp4xcsWJAGZ3P5z//8z/SbLjbmu9/9bs798+bNS7/1ohCefvrp9cLciZNPPrkgc1MxxcXFce655+Yce/XVV2POnDkFOc/DDz9cap/nHAAAAAAAAACgdhHoriWSrqRJZ9Ett9yyQsfVr18/brvttjjuuOPyBuSeffbZqK623XbbKCkpqZKtsp1eAdh8XnjhhZgxY8YmH3/nnXemXbRzGThwYLnmOOqoo+LAAw/MOXbLLbdEIfz2t78t1T38W9/6VkHmpuK+//3vR4sWLUrtX7FiRdxxxx2Vnn/mzJlx3333rbdvn332iX333bfScwMAAAAAAAAAUH0IdNcSN998c7Rt23aTji0qKkpD3Um4O5dkDACqsyVLlsQ111yzSccuXbo0brrpprwh7eQbLcormSf5u7qhpEP3c889F5XxwAMPxDvvvFOqK3iHDh0qNS+brl27dvGzn/0s59iNN94Y06ZNq9T8l156aXptr+vKK6+s1JwAAAAAAAAAAFQ/At21wE477RT9+vWr1Bzbb7999O3bN+fYoEGDYu7cuZWaHwCq2l133RXDhw/fpC7LU6ZMKbW/adOmaefuikjC38l8uZx//vkxadKk2BRjx46N//7v/15vX5s2bdLAL9n60Y9+FPvvv3+p/QsWLIizzjor7da9KR555JH485//XOr6OvHEEzd5rQAAAAAAAAAAVE8C3bXAqaeeWpB5jjzyyJz7kyDSm2++WZBzAEChHX744dG8efP071USdv3www/Lfewf/vCHvKHtpON3t27dKryeX/3qV9GnT59S+5Mw9zHHHBOTJ0+u0HyjR49Oa9zww1X33ntvdOzYscLro7CKi4vj8ccfz/lcvPTSS3HGGWekXeArGub+1re+td6+Vq1axUMPPRT16tWr9JoBAAAAAAAAAKheirNeAJWXqyvkpthzzz3zjuXqXApA1Uled8eMGVNq/6hRo/IekwSZX3nllZxjTZo0iV69ekVVeP3112PJkiXr7Zs2bVrO+ybB1lxrTILTmxKeTnTp0iUNzZ533nkxa9as9O/ijTfemHbETsK2ucyePTvtrHzfffflHP/GN76Rjm+K5Jz//Oc/49hjj4233nprvbGPP/44unfvHjfccEOce+650ahRo7zzJAHuP/7xj/GLX/wili1btt7YT3/60/j6179e5jqSx3rYsGGl9ucLvCfXXK7nZt999027gW/s2sz3XiHfdXnUUUetd/vdd9+NOXPmbPJa813juc694XnWXcPKlSsrfG127tw5DW8nH46bMWPGemOPPvpojBgxIv3wQBLoLyuQ/fnnn6cfJLj//vvX29+wYcN44IEHomvXrmWuAwAAAAAAAACAmkmguwbaa6+9YsCAAWtv77fffgWZt23btnnH8gXzAKgaL774YpxzzjkVOuY3v/lNuuWy7bbbxoQJE6IqJGHqL774olz3Tf6eHH300aX2X3XVVXH11Vdv8hqScPTYsWPj17/+dSxatCj+67/+K6699tro379/7LHHHtGpU6c0qJt0x06+deKJJ54oFZJeo1+/fvHggw9WqhNy0k355ZdfjjPPPDOefPLJ9cbmz58f3/ve9+Kyyy6LE044If07nnR3TkLT8+bNSzt5Dx8+PA0IJ7VsKAl4//znP9/oGqZOnZrzsc4nOV+ybei1115br+N4Ra/NfNdlSUnJereTAP2QIUM2ea35rvGKPAaXXHLJJl+bPXr0iKFDh8ZJJ52UBvfXNW7cuPja174W22yzTTq+8847p895EkJPwuXJtfuvf/0r3VavXr3esc2aNYt//OMfaRgcAAAAAAAAAIDaSaC7BkqCQMlWaA0aNMg7lgSOAKA6+9WvfpV+OOlnP/tZGt5OOiX/6U9/KvfxSYD7Jz/5SVx//fVRv379Sq+nRYsWaXD89ttvT8PbCxYsKBXsfuihh9KtPJLActLlOQmBUz0lQe2333477aCeXHsbhrOTsP5tt91W7vl69+6dzrPbbrtVwWoBAAAAAAAAAKguBLpZKwm+5bPllltu1rXAura65rOslwDUEEkgO+lknPxMOmSXV9KBOgmE77///gVdTxIST7pxn3rqqXHTTTfFPffcEzNnzqzQHDvttFPagfyiiy6Kpk2bFnR9FF4S5P/jH/8YF1xwQdx4443x+OOPx9KlSys0RxLkTo7/5je/WalO8QAAAAAAAAAA1AwC3aw1efLkvGP77rvvZl0LQF139tlnp1tNMGHChKhO9tprr3jppZfik08+iaeeeiqGDRsWH3/8cUydOjUWLVqUfiNF0sl7l112SYOzJ598cvTo0aNK15R8MOqGG26IX/ziF/Hqq6/G4MGD49///nd89tln6QeqlixZEo0aNYrWrVtHmzZt0o7MPXv2TNd34IEHbtI5u3btGiUlJTXm2kwek6pQFY9Beey+++7x4IMPpuHu5557LoYOHRrvv/9+jB8/PubOnZuGvJs3b54+58n1seeee6bP+VFHHRU77LBDJmsGAAAAAAAAACAbAt2slQTMctlmm21i55133uzrAYDKSP52/fjHP0636qK4uDiOPfbYdKNuaNWqVZx22mnpBgAAAAAAAAAAuRTl3Eudk3SJfPbZZ3OOfe9739vs6wEAAAAAAAAAAACAukCgm9Rtt90WU6ZMKbV/q622EugGAAAAAAAAAAAAgCpSXFUTU3MMHz48rr766lL7i4uL4+GHH47mzZtHTbBixYp4+eWXY+jQoTFy5MgYO3ZszJs3L+bPnx/16tWLJk2aRIcOHWLbbbeNPffcMw466KA4+uijo0WLFlkvHQAAAAAAAAAAAIA6SqC7jnvqqadiwIABsXjx4vX2N2jQIO67777o3bt3VHdJaPuXv/xl3HvvvTFr1qy891u+fHl63yTonQS/Ew0bNoyTTjopLr744jj44IM346oBAAAAAAAAAAAAIKIo6wWw+ZWUlMRrr70W/fv3j379+sXcuXPXG+/UqVM8//zzcfrpp0d1N2PGjNhhhx3i17/+dZlh7rJC3o888kj06tUrfSwmTZpUJesEAAAAAAAAAAAAgFx06K6lko7bw4cPT8PbCxYsiPnz58eXX34ZI0aMiLfeeismT55c6phmzZrFwIED4+qrr46WLVtGTalzw+7iPXr0iCOPPDK6d+8e7du3j+Li4jT4PXXq1Bg6dGgMGjQolixZkrNb+ZAhQ+L++++PE088scrXPn369HRdFTFu3LhSgfRly5Zt9Lh69eql3cg3lByfXCPlleu+yb7Vq1eXe44160m2DeepyFoSRUWlP5OS1VrUlJ+aqnYtdammfOvJN1YTaqqNz5Oa1LThfdY914ZrScZXrFhRobUk3ySz4XpWrlwZq1atKvcchXpvVL9+/fS95rrU9L/UlJua8lNTfmrKTU35qSk/NeWmpvzUlJ+aam5Nq+s3itXFjcs9R72SVVF/+cJS+1c1bB4l9eqXe56ilUujaNX6/y27pF5RrGrYIiqi/vIFUa9k/X8fVZOaalJNuaz7GrF6dflfcypq+YoV6/0/pbryuqcmNZVFTfmpKbe6VFNN/5tbG99HqKlu1bShdV8jVqys2GtFRSSvH+u+Z6xLr3tqUlNZ1FQ7a1q+fHlUNwLdtdTEiRPj6KOP3uj9GjVqtLY79YABA6JVq1ZRUyUh7CSMvvfee+e9z6WXXpoGwG+55Zb45S9/GQsXrv9Gat68efH1r3897rrrrjj33HOrdL233357XHPNNZWaIwnmN2nSZKP3S144u3XrlvP4irwwtWnTptS+5MW3PKHyDa+7DQNdyYtnspVX8uKbzLOh5IW8IqG55I9B8mK+LjX9LzXlpqbqV1Oyxg3PUdNrykVN+amp+taU/IvR//0PydWl/iUsmWP8+PFREdttt12p9STfODNz5sxyz1Go90bt2rVLt3Wp6X+pKTc15aem/NSUm5ryU1N+aspNTfmpKT811dyalrXbMZZ02L3cc9RfOi9ajX2+1P6FXXvHqsbl//8JTaaNTrd1JQGL+TsfFxXR8pPno3jZvPX2qUlNNammXNZ9jVi0aFFUlSlTpkTjovF17nVPTWoqi5ryU1Nudammmv43tza+j1BT3appQ+u+RsyYXrHmkRWRNAxd97WoLr3uqUlNZVFT7axpco6myFkr3WqPOqV3795x+OGHx2677ZZ26K6JmjdvHg8//HA8+eSTZYa512jatGlcdtllMWbMmNhpp51KjSdho6RT+T//+c8qWjEAAAAAAAAAAAAA/C8duuu4V155Jd0S7du3j9NPPz2+973vxQ477BA1QYsWLeLll1+OAw44oMLHdunSJYYMGRKHHnpofPrpp6VC3WeffXaMGDEi/eQGAJvP0qVLY/jw4evtS7oJf/zxxznvP3Xq1Bg0aNB6+5KOxAcddFDObxYAAAAAAAAAAACoTuqVrPkOdGq1xYsXx+zZs9P28e+8804MHTo0nn766bSlfK5W+D/4wQ/i5z//edr9urp58MEH480330x/79+/f9phvDJef/31tFN5rn8U+vbtG88880xUhenTp8eMGRX7GpRx48bFSSedtPb2e++9l3ZX35gk2Jg8rxtKvtqgIi8ByX03/CqDJPxfv379cs+xZj3JtuHcFX05Kioq/SUDSRg/i7WoKT81Ve1aamNNX3zxRc6vZKmo1157Lfr06bPePs+TmjbHPGpa/8MYyfuXddez4447RnFx8do5kq9KqogGDRqUWk9ynlWrVpV7jkK9N0reA62pZQ01/S815aam/NSUn5pyU1N+aspPTbmpKT815aemmlXT6FP+798LV9dvFKuLG5d7jnolq6L+8oWl9q9q2DxK6pX/vwsXrVwaRauWrbevpF5R+lXoFVF/+YKoV7L+v4+qSU01qaY9/7agzNeIW//603jn/deiKlx+4R2x6/b71InXvXWpSU1lUVN+aqp7Na37nrE2/M2tje8j1FS3atrj70vzvka8MeKluP3BK6IqHN3rlDjr5EvqxOvehtSkprKoqXbW9OGHH8Y++/zfvyePHj06unfvHlkS6K7DFixYEHfeeWdcd911MW/evFLjSVD4ueeei2233TZqu/POOy/uvffenGPDhg2LXr16RXUwZsyY2H333TN7EUle4DbsZr5uKAugECZMmFCQb0fIFegGNi/vHQAAALK3YTgHyM7uj5b9v2V/d88l8dbI//1m3UK7+uJ7Yrcd96uSuQGo+bxnhJrzvvH1f78Qt9z30yo579cO/WZ8+9SqmRuguhmTcRYzl9Kt9qgzWrRoEZdccknasTvXhZh8AuGggw6Kjz76KGq7iy66KO/YzTffvFnXAgAAAAAAAAAAAEDdIdBN2iVxyJAh0aVLl1JjU6ZMif79+8eiRYuiNttrr73SLZdnnnmm1tcPUJ107do1/eqVym66cwMAAAAAAAAAADWBQDeptm3bxmOPPRZFRaUviaRD98UXXxy13eGHH55z/9KlS2Pw4MGbfT0AAAAAAAAAAAAA1H4C3azVs2fPOPnkk3OO3XffffH5559HbbbPPvvkHXvzzTc361oAAAAAAAAAAAAAqBsEulnPJZdcknP/qlWr4re//W3UZj169Mg79sknn2zWtQAAAAAAAAAAAABQNwh0s579998/tthii5xjTz/9dNRm+epOTJw4cbOuBQAAAAAAAAAAAIC6QaCb9dSrVy8OPvjgnGOTJ0+OcePGRW3VqlWrvGPz58/frGsBAAAAAAAAAAAAoG4Q6KaU7t275x0bNWpU1FbNmjXLO7Zs2bLNuhYAAAAAAAAAAAAA6gaBbkpp06ZN3rFZs2ZFbbVw4cK8Y40bN96sawEAAAAAAAAAAACgbhDoppS6GuieN29e3rGWLVtu1rUAAAAAAAAAAAAAUDcIdFPK0qVL844VFdXeS6assHqXLl0261oAAAAAAAAAAAAAqBtqbzq3lurZs2d07Ngx3Xr06LHZg82tWrWKrHzzm9+Mrl27rt2WLVtW0PlHjRqVd2znnXcu6LkAAAAAAAAAAAAAIFHsYahZZsyYEdOmTdto8Loyxo8fn3dsu+22i6xMnTo1vvjii7W3Z8+eHVtttVXB5n/33Xfzjh1wwAEFOw8AAAAAAAAAAAAArKFDdw22cuXK+PLLLws+7+DBg/OO7bXXXlFdTJw4saDzvfrqqzn3N27cOA477LCCnqumqlevXql9JSUlmawFAKj+Vq9eXa73EwAAAAAAAAAAdZlAdw33yiuvFHS+kSNHxqRJk3KOde/ePTp06BDVRVnB84p644034qOPPso51rdv32jevHnBzlWTFRWVfslYtWpVJmsBAGpmoDvX+wkAAAAAAAAAgLpMmqKG+5//+Z+CznfDDTfkHTvttNMqPN+YMWOiX79+0apVqzQUfcwxx8Q777wThfDYY49Fodx66615xy6++OKCnaemSzpq1q9fv1SneACAXDb84FfyPkKHbgAAAAAAAACA9Ql013AvvvhiDBkypGAdr/OFpFu0aBHnn39+heZ777334sADD4ynnnoq5s+fH4sWLYqXX345evfuXZDO4v/+97/j2WefrfQ8L730Uvz973/POXbcccel6+X/NGjQYL3bixcvzmwtAED1tuH7hOLi4szWAgAAAAAAAABQXQl01wJnnnlmTJo0qVJzfPHFF3HqqafG6tWrc45fddVV0bZt2wrNOXDgwFi4cGGp/cuWLYtvf/vbsXz58qis7373u/Hll19Wqu7zzjsvb4j9tttuq8TqaqdmzZqtdzvXcwwAkEg+0FfW+wgAAAAAAAAAAAS6a4UkzH3EEUfEu+++u0nHDx8+PO2kPWPGjJzjxx57bPzgBz+o0Jzjx48vcz3Jmt98882orCTMfcwxx8THH39c4WM/+eSTOPTQQ3OG4YuKiuIvf/lLbL/99pVeY23TvHnzUgH9QoTzAYDaZdWqVbFkyZIy30cAAAAAAAAAACDQXWuMGzcuDWVffPHFaVC5vN2pzz///OjTp09MnTo1530OPvjgePTRR9OAc0VMmTJlo/epTGftdX344Yex7777xrXXXps3lL6uefPmxc9+9rPYc889Y+LEiaXGk1rvvPPO6N+/f0HWV9s0adIk6tevv96+mTNnZrYeAKB6mjNnznq369Wrl76PAAAAAAAAAABgfcUb3Kaa69atWxrEzmXlypVx6623pttee+0VBxxwQOyxxx6xxRZbRKtWrWLp0qUxa9asNPw9aNCgtIP26tWrc86VBG4GDhwYv//97zcpeNOhQ4eN3merrbaqcO1DhgzJObZ48eK46qqr4pe//GX06tUrDaIn3bXbtGkTxcXFadA7CZm/9tpr6Rz5Okon93/ggQeib9++FVpbXZJcGy1atIi5c+euF5JP9iUbAEDyDR4bfuCrWbNmFf6QIAAAAAAAAABAXSDQXcMkQeyhQ4fGHXfcEY8//njeYPLIkSPTbVPsuuuucdddd8UhhxyyyetMwtRJB+xRo0blHO/cuXMauq6IP//5z/H9738/7Z794IMPxvz583OGh5LHKNkqGlI+5ZRT4uabb65w0LwuateuXfr4r/uBgMmTJ0enTp3SDw8AAHVX8n4s+RaUkpKS9fZvueWWma0JAAAAAAAAAKA60yKvBurdu3c89NBD8eWXX8Zf/vKXNIjcunXrSs3ZuHHjOO200+LFF1+M0aNHVyrMvcbdd9+ddmLcUKNGjdJwdsOGDSs8Z9Jx/I9//GPabTsJtJ9zzjnRsWPHTV5j0lH63HPPjffeey8efvhhYe5yatCgQbRv377U/q+++ioNcC1ZsqRUiAsAqN1WrVqVduUeP358+s0xG34LSvJ+EwAAAAAAAACA0nToruFdks8+++x0SwI0SSj5gw8+iA8//DDGjBmTBr7nzZuXdlJeuHBhGsJNuie3bNkyDYDvuOOOsc8++6TbfvvtF82bNy/o+nr27BlvvPFGXH755TFkyJB0jb169Yrrr78+HauMpk2bxsknn5xuSXD4/fffTzuSJ/UnW9ItOqk92RYvXpyGyJPwdhLY7tatW9o9PAmtJ+H4TQmWE+k1lAS3k8d4XYsWLUq3+vXrp89TEupPHv+ioqJ0X9INHQCo2ZJv6Ui25P1d8l4r+dufvC/IJQly684NAAAAAAAAAJCfQHctkQRlk5B0ZYPShdajR494+umnq/QcSUA4CWgnG5tP8rgnAfkkqD1nzpxS40nAa8GCBekGANRNSZi7S5cu6XtVAAAAAAAAAAByE+gGKhXq7tChQxrWmjFjRqxcuTLrJQEA1USbNm3SztzC3AAAAAAAAAAAZRPoBiod6m7dunW0bNkyZs2aFfPnz4/ly5dnvSwAIKP3Bc2aNUuD3MkHvgAAAAAAAAAA2DiBbqAgioqK0vBWsq1YsSIWLVoUixcvjiVLlsSqVavSDQCoXZLu28XFxWmIu3nz5tGkSZP0PQEAAAAAAAAAAOUn0A0UXIMGDdKu3cm2RklJSaxevTr9CQDU/E7cSXA7+QkAAAAAAAAAQOUIdAObRRL4Srp4AgAAAAAAAAAAAPB/fB86AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwIdAMAAAAAAAAAAAAAZESgGwAAAAAAAAAAAAAgIwLdAAAAAAAAAAAAAAAZEegGAAAAAAAAAAAAAMiIQDcAAAAAAAAAAAAAQEYEugEAAAAAAAAAAAAAMiLQDQAAAAAAAAAAAACQEYFuAAAAAAAAAAAAAICMCHQDAAAAAAAAAAAAAGREoBsAAAAAAAAAAAAAICMC3QAAAAAAAAAAAAAAGRHoBgAAAAAAAAAAAADIiEA3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADIi0A0AAAAAAAAAAAAAkBGBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkRKAbAAAAAAAAAAAAACAjAt0AAAAAAAAAAAAAABkR6AYAAAAAAAAAAAAAyIhANwAAAAAAAAAAAABARgS6AQAAAAAAAAAAAAAyItANAAAAAAAAAAAAAJCR4qxOTNUYO3ZsvPvuuzFmzJgYPXp0fP755zF37tx0W7JkSbRo0SJatmwZrVu3jl122SX23Xff2G+//aJ3795RXFw7LocpU6bE008/HYMHD44PPvggvvrqq1iwYEFaX6tWrWKHHXaIvfbaK44//vg44ogjolGjRlkvGQAAAAAAAAAAAIA6qnYkeOuw2bNnxzPPPBOvvvpqDBo0KCZPnlzm/efMmZNuX3zxRYwaNSoefvjhdH+HDh3ijDPOiAsuuCANPNdESYD9uuuui8ceeyxWrVpVanzFihVpqH3q1KkxbNiw+MMf/hDt27ePiy66KH74wx9Gs2bNMlk3AAAAAAAAAAAAAHVXUdYLoOLmz58f999/f/Tt2zc6duwYAwYMSG9vLMxdlmnTpsVNN90U3bt3j6uuuiqWLl0aNUUS3r7iiitin332SQPqucLc+UyfPj2uvPLKtO4kFA8AAAAAAAAAAAAAm5MO3TVQEry+5ppryrxPu3bt4ogjjoj9998/OnXqFK1bt05D2klH7w8++CAGDx6cduje0PLly+Paa6+Np556Kl5++eV0nuoebj/11FPjxRdfLDWWdNzu379/9O7dO30Mli1bFmPHjk07micduteVdCw/9thj4+abb047dgMAAAAAAAAAAADA5iDQXcu0b98+brzxxjj99NOjUaNGZd73jTfeiEsvvTSGDh1aamzkyJHRp0+fGDRoUDpndbRkyZI44YQTcq7/jDPOSMPZuQLpSc3/+te/4tvf/nZ89tlna/cnnb3/67/+K/1dqBsAAAAAAAAAAACAzaFos5yFzaJnz54xevToOOecczYa5k4cdNBB8dprr8WFF16Yc3zMmDFxwQUXRHWV1JkrzH355ZfHgw8+WGZ38UMPPTQNtO++++6lxv77v/87nnvuuYKvFwAAAAAAAAAAAAA2JNBdS3Tt2jWef/752HLLLSt0XP369eO2226L4447Luf4P/7xj3j22Wejurn77rvj4YcfLrW/f//+cf3115drjuSxeuaZZ6Jly5br7S8pKYkBAwbElClTCrZeAAAAAAAAAAAAAMhFoLuWuPnmm6Nt27abdGxRUVEa6k7C3bkkY9XJ9OnT48c//nGp/a1bt4477rijQnNtu+22ccMNN5TaP3PmzLjkkksqtU4AAAAAAAAAAAAA2BiB7lpgp512in79+lVqju233z769u2bc2zQoEExd+7cqC6uuOKKmDdvXqn9P/jBDyrcoTzxne98J7bbbrtS+x966KF48803N3mdAAAAAAAAAAAAALAxAt21wKmnnlqQeY488sic+1esWFFtgs1fffVV3HfffaX2N2jQIM4///xNmrOsY6+//vpNmhMAAAAAAAAAAAAAykOguxbYf//9CzLPnnvumXdsypQpUR384Q9/iOXLl5faf9xxx0X79u03ed6zzjor6tWrV2r/s88+G5988skmzwsAAAAAAAAAAAAAZSkuc5Rqaa+99ooBAwasvb3ffvsVZN62bdvmHZs2bVpkbfXq1XH//ffnHDvxxBMrNXfHjh3jgAMOKNWJvKSkJD2nTt0AAAAAAAAAAAAAVAWB7hropJNOSrdCa9CgQd6xJk2aRNaGDBkSX375Zc6xI488stLzJ3NsGOhOPPTQQwLdAAAAAAAAAAAAAFSJoqqZlppoxowZece23HLLyNpzzz2Xc//WW28dXbt2rfT8hxxySM79EyZMiI8++qjS8wMAAAAAAAAAAADAhgS6WWvy5Ml5x/bdd9/I2ssvv5xz/1577VWQ+ffee++8Yy+99FJBzgEAAAAAAAAAAAAA6yqOGmrp0qXx6aefxieffBJjx46NcePGxZw5c2LhwoWxaNGi9GdynyZNmkTz5s3TrVmzZtG2bdvYcccdY+edd46ddtopdthhh2jQoEHW5VQLr776as7922yzTfp4ZSl5Tj/44IOcY7vvvntBztGhQ4e0E3muTuVvvPFGXHzxxQU5DwAAAAAAAAAAAADUuED3qlWr0lDtiy++mG7vvfdelJSU5L3/umP16tXLe7/i4uI44IAD4thjj023/fbbL+qiJPz+7LPP5hz73ve+F1kbMWJErF69OufY9ttvX7DzJHPlCnS/++67BTsHAAAAAAAAAAAAANSYQHfSefvWW2+NBx54IObPn5/uKyvIva4kyJ3ct6z7r1ixIl5//fV0u/LKK6Ndu3Zx3nnnxYUXXhhbb7111BW33XZbTJkypdT+rbbaqloEut9///28Y127di3YeZK53nzzzVL7P/vss1i8eHE0bdq0YOcCAAAAAAAAAAAAgKKopoYNGxYnnHBC7LLLLvHHP/4x5s2bt144Owlrb2wr7/3WzJtsSXfmG2+8Mbp16xannXZajBo1Kmq74cOHx9VXX52ze/nDDz8czZs3j6wlgep8OnXqVLDzJAH2XJJrY/z48QU7DwAAAAAAAAAAAABUy0D33Llz49xzz43DDjssnn/++Vi9enUaps0V1i6UXAHvpHP3I488Ej179oyf/OQnsWTJkqiNnnrqqejbt2/afXpdDRo0iL/+9a/Ru3fvqA7KClNvueWWBTtPWXN9/vnnBTsPAAAAAAAAAAAAAFS7QPcTTzyRduS+77771nbM3jDAvW437Q23hg0bRuvWraNjx46x9dZbxzbbbJN2b+7QoUO0atUqDSmXdfwa6wa7V65cGb/73e+iR48eMWTIkKgNkrpee+216N+/f/Tr1y8N0a8recySMP3pp58e1cWUKVPyjm2xxRYFO0+7du3yjk2dOrVg5wEAAAAAAAAAAACARHF1eRiS0PSll16aduROrAlxrxu0TgLae++9d+y6666x0047pbeT4Hbbtm3TUG9x8cbLSTpvz5o1K2bOnBlffvllTJw4MT755JP48MMPY8SIETFt2rS19113DUl35mOPPTb+/Oc/V6ugcz5Jx+3hw4ena1+wYEHMnz8/rTep8a233orJkyeXOqZZs2YxcODAuPrqq6Nly5ZRnSTPWS6NGjUq1/NeXsljUNE1AAAAAAAAAAAAAECNDnT/6Ec/iptvvnltR+7Emo7bffv2TbtIH3HEEdG5c+dKnyvp0p108E623XffvdT4uHHj4tVXX41//vOf8fLLL6cB8zVrWr58eZx11llpt+hkzdVZElQ/+uijN3q/JBDdq1ev9DEeMGBA2sm8Opo9e3bO/c2bNy/oecqaL98aNtX06dNjxowZFTomuT7XlVyTy5Yt2+hxyTWc/PO0oeT4dT80sTH169cvFaBP/hlJPihR0X8Oi4rW/4KApBv+qlWryj2HmvJTU35qyk1N+akpPzXlpqb81JSfmnJTU35qyk9NuakpPzXlp6bc1JSfmvJTU25qyqam1fUbxerixuWeo17Jqqi/fGGp/asaNo+SevXLPU/RyqVRtGr9/5ZdUq8oVjVsERVRf/mCqFfyv42B1lCTmmpSTbms+xqxenX5X3MqavmKFev9P6W68rqnJjWVRU35qSm3ulRTTf+bWxvfR6ipbtW0oXVfI1asrNhrRUUkrx/rvmesS697alJTWdRUO2tK1lndZB7ovuyyy+L3v//92ic4eSI7dOgQF1xwQZx//vnRvn37zbqeHXbYId2++93vxoQJE+K2225Lu3LPmzcvXV9yofzkJz+Jpk2bpmus6Xr37h2HHXZY7LbbbmV2p87akiVLcu7P9YJQGWXNl28Nm+r222+Pa665plJzJJ3WmzRpUq66unXrlvP4irwwtWvXLt3Wlbxwjh8/Pipiu+22Sz9MsK65c+emnfPLS035qSk/NeWmpvzUlJ+aclNTfmrKT025qSk/NeWnptzUlJ+a8lNTbmrKT035qSk3NWVT07J2O8aSDqWb3eRTf+m8aDX2+VL7F3btHasal79BTJNpo9NtXUnAYv7Ox0VFtPzk+SheNm+9fWpSU02qKZd1XyMWLVoUVSVpWNW4aHyde91Tk5rKoqb81JRbXaqppv/NrY3vI9RUt2ra0LqvETOmV6x5ZEXMnz9/vdeiuvS6pyY1lUVNtbOmyZMnR3Wzfhx9M3viiSfiV7/61doO2Ek6/vvf/37ahfjKK6/c7GHuDXXt2jV+97vfpev51re+tbaDePLzBz/4Qbz55ptR073yyitxxRVXpN28t95667SuDbtAVwf5/gFPPp1RSBt+0qM8awAAAAAAAAAAAACAGhfoHjt2bJx99tlrA9KdO3eOt956K2666aZq1ym6bdu2cf/998cLL7wQrVq1StechHtPOeWUmD59elRHu+yyS/q4rtmST/RPmjQpRowYEXfddVeceeaZ0bp16/WOSWq5+eabo3v37vHTn/40Fi4s/TUgWUla32+OQHdZ81W0hT8AAAAAAAAAAAAAbEy9kiTtm4GjjjoqBg0alIajd99993j++eejU6dOUd19+OGH0bdv35g4cWJ6+6yzzoq//OUvURMtWLAg7rzzzrjuuuti3rzSX/O22267xXPPPRfbbrttZK1BgwY5Q93bbLPN2ueiUB3Lk27luQwcODB9vAolCdDPmFGxr0FJuqefdNJJa2+/99576fO0Mck/Z8lXHGwo+WBCRV4CksD7hl3MV69eXeGwe/J8Jh3515U8v6tWrSr3HGrKT035qSk3NeWnpvzUlJua8lNTfmrKTU35qSk/NeWmpvzUlJ+aclNTfmrKT025qWnz1TT6lHr/N0/9RrG6uHG556hXsirqLy/d/GVVw+ZRUq/8DVeKVi6NolXL1ttXUq8o/Sr0iqi/fEHUK1m93j41qakm1bTn3xaU+Rpx619/Gu+8/1pUhcsvvCN23X6fOvG6ty41qaksaspPTXWvpnXfM9aGv7m18X2EmupWTXv8fWne14g3RrwUtz94RVSFo3udEmedfEmdeN3bkJrUVBY11c6aPvzww9hnn//79+TRo0enzZDrXKD7xRdfjOOOOy59Qrfffvt4++23S3WLrs4mTJgQ+++/f8yaNSu9AJKu10kovab69NNP4+tf/3qMGTOm1NhWW20Vr776auy6666RpaRr++LFi3Ou76uvvirYeZIu7Mm1mctFF10Ut912W2QpeY7Wvdaqw4sIAAAAAADV14bhHCA7uz9a9v+W/d09l8RbI1+pknNfffE9sduO+1XJ3ADUfN4zQs153/j6v1+IW+77aZWc92uHfjO+fWrVzA1Q3YyphlnM9ePom0GSH7/00kvXhnSffPLJGhXmTnTt2jUeffTR9FMBSXr/xz/+cdRkO+64YwwZMiS6dOlSamzKlCnRv3//WLRoUWSpSZMmOfcnn+YopLI+1dG4cfk/dQcAAAAAAAAAAAAA5bF+n/LNIGmFnoS41wS627VrFzXRYYcdFhMnToylS5emncZrurZt28Zjjz0WBx54YPocreujjz6Kiy++OO65557M1temTZu0I/qGFi4s/VUllbFgQemvultjiy22KOi5AAAAAAAAAAAAAGCzd+iuX79+bLvttulWU8Pca3To0CGtI1dn65qoZ8+ecfLJJ+ccu+++++Lzzz+PrOS7VpYtWxYrV64s2HnK6kSehN4BAAAAAAAAAAAAoEYHuqneLrnkkpz7V61aFb/97W8jKx07dsw7Nnv27IKdJ1cX8PKsAQAAAAAAAAAAAAA2hUA369l///1jiy22yDn29NNPR1a22267vGMzZswo2HmmT5+ed6xbt24FOw8AAAAAAAAAAAAAJIprysMwbty4GDhwYN7x6667Lg4++ODNuqbaqF69eunj+Mwzz5Qamzx5cvo87LDDDpt9Xdtvv33esa+++iq6d+9ekPNMmTIl75hANwAAAAAAAAAAAAB1NtD9+OOPx+DBg9PA8bpKSkrSfTNnzsxsbbVNEo7OFehOjBo1KpNA9x577JF3bMKECQU7T765kkB506ZNC3YeAAAAAAAAAAAAAEgU1ZSH4dVXX10vxJ1sVI02bdrkHZs1a1ZkYe+9946iotyX62effVaw8+Sba7/99ivYOQAAAAAAAAAAAACgxnXofvvtt9d2505+JoHuJk2axCmnnJKGfffZZ5+sl1hrVMdAd/PmzaNHjx5ph/ANjR49uiDnmDZtWsyYMSPn2IEHHliQcwAAAAAAAAAAAABAjQt0T58+PebPn782yJ3YYYcd4pVXXokuXbpkvbxaZ+nSpXnH8nXJ3hyOPvronIHukSNHFmT+suY55phjCnIOAAAAAAAAAAAAAFhXduncCtiwK3QS7L7zzjvrZJi7Z8+e0bFjx3RLOlZXhbK6cLdq1Sqycvzxx+fc/+WXX8YXX3xR6fmHDRuWc/+2224bu+22W6XnBwAAAAAAAAAAAIAaGeguLi4uFSo+/PDDoy6aMWNGTJs2Ld0+/vjjKjnH+PHj845tt912kZXDDjsstt5665xjr776aqXnTzq+53L66adXem4AAAAAAAAAAAAAqLGB7i222GK92506dcpsLdXJypUr0+7UhTZ48OC8Y3vttVdkpaioKM4888ycY08++WSl5p46dWq89dZbpfYn3eAHDBhQqbkBAAAAAAAAAAAAoEYHutu2bRvt27dfe3vBggWZrqc6yddVelONHDkyJk2alHOse/fu0aFDh8jSRRddFA0bNiy1/4UXXki7l2+qBx54IEpKSkrt79u3b+y8886bPC8AAAAAAAAAAAAA1PhAd6JPnz5rA7fTp09Pu1NXpffeey/+9a9/pVt19j//8z8Fne+GG27IO3baaadVeL4xY8ZEv379olWrVtG8efM45phj4p133tnk9W299dZx9tlnl9q/fPnyuOOOOzZpzuRaynfs5ZdfvklzAgAAAAAAAAAAAECtCnSfeeaZ64V3Bw0aVKXnO/fcc+Pwww+PI444IqqzF198MYYMGVKQuQYPHhyPPfZYzrEWLVrE+eefX+FQ/IEHHhhPPfVUzJ8/PxYtWhQvv/xy9O7du1KdxX/xi1+kAfEN/f73v9+kLt133313jB8/vtT+008/PQ466KBNXicAAAAAAAAAAAAA1JpAd9++feOAAw5Ye/v222+v8nMmHcHXdAWv7mH3SZMmVWqOL774Ik499dRYvXp1zvGrrroq2rZtW6E5Bw4cGAsXLiy1f9myZfHtb387DeZvivbt28dvfvObUvvnzp0bF154YYXmmjhxYlx22WWl9rdr1y5++9vfbtL6AAAAAAAAAAAAAKDWBboT999/f7Rs2TL9/emnn46HH3446yVVC0mYO+kk/u67727S8cOHD087aefrbn3sscfGD37wgwrNmXS8Lms9yZrffPPN2FTf+c530gD6hpIO41dccUW55pg5c2accMIJMW/evPX216tXL/7617/GVltttcnrAwAAAAAAAAAAAIBaF+jecccd49lnn40WLVqknbPPOuus+Nvf/pb1sqqFcePGpaHsiy++OD755JNyd+U+//zzo0+fPjF16tSc9zn44IPj0UcfjaKiil0qU6ZM2eh9vvzyy6iM++67Lw455JBS+6+77rq0a/ns2bPzHjts2LD08frggw9Kjd1yyy1x/PHHV2ptAAAAAAAAAAAAAFAexVHD9OrVK95+++047bTTYsSIEWmo+3/+53/i5z//eRrQre26deuWBrFzWblyZdx6663pttdee8UBBxwQe+yxR2yxxRbRqlWrWLp0acyaNSsNfw8aNCjtoL169eqccyVdqgcOHBi///3vo0mTJhVeZ4cOHTZ6n8p2wE7WlQT8v/GNb8TLL7+83tiDDz4Y//znP9Ox3r17p+datmxZjB07Nj3mX//6V6n56tevn9b7X//1X5VaFwAAAAAAAAAAAADUykD3uiHc3/zmN/HnP/85DXM///zz6da1a9c0vJuEmTt16hQtW7aMpk2bbtK5Fi5cGNVREsQeOnRo3HHHHfH444/H8uXLc95v5MiR6bYpdt1117jrrrtydr8ur+233z723HPPGDVqVM7xzp07p92/Kyt5jp977rm48sor02siCbWv+xwmXbyTbWO6dOkS99xzTxx99NGVXhMAAAAAAAAAAAAA1MpAd58+fdLO0RsqKSlJf44fPz4mTJgQDzzwQNRmSWg92WbOnBnPPPNMGmhOOlTPnTt3k+ds3LhxfP3rX4+zzz47jjrqqCgqKqr0Ou++++44/PDDY9GiRevtb9SoURrGb9iwYRRCcXFx/PKXv4zTTz89rr322jTonq/z+IbatWuXduT+4Q9/GM2bNy/IegAAAAAAAAAAAACgVga6Nwxwr7FuyHvDsU2VKzhe3SRh5CSAnWyrVq2K9957Lz744IP48MMPY8yYMfHll1/GvHnzYv78+Wm36gYNGkSrVq3SrtatW7eOHXfcMfbZZ59022+//QoeaO7Zs2e88cYbcfnll8eQIUPSNfbq1Suuv/76dKzQdt9993jkkUfSup966qn0nKNHj05vJ/Unwe+k/qR7eNLF/bjjjkvD60mYHQAAAAAAAAAAAACyUCMD3RuGrdeEuJP9NSGIXRXq16+fhqSrIihdGT169Iinn356s55z6623jgsuuCDdAAAAAAAAAAAAAKA6q5GB7g3V1RA3AAAAAAAAAAAAAFCz1chA95qO3AAAAAAAAAAAAAAANVlR1EB77bVXrF69ukq3PffcM+syAQAAAAAAAAAAAIBarkYGugEAAAAAAAAAAAAAagOBbgAAAAAAAAAAAACAjAh0AwAAAAAAAAAAAABkpDhqmJKSks1ynv322y9at269Wc4FAAAAAAAAAAAAANRNNSrQPWfOnPRn/fr1q/xcd999d5WfAwAAAAAAAAAAAACo22pUoLtVq1ZZLwEAAAAAAAAAAAAAoGCKCjcVAAAAAAAAAAAAAAAVIdANAAAAAAAAAAAAAJARgW4AAAAAAAAAAAAAgIwURy0zceLEeOedd9Lt888/j7lz58a8efOic+fO8fjjj5e6/6uvvhq9evWKxo0bZ7JeAAAAAAAAAAAAAKDuqhWB7mnTpsW9994b99xzT3zxxRelxktKSmLFihU5jz3vvPNi5syZcfLJJ8fPfvaz2GmnnTbDigEAAAAAAAAAAAAAIoqiBluyZElcdNFF0aVLl7jiiitiwoQJaXh7w21jFi1aFA8++GB07949Lrzwwli2bNlmWT8AAAAAAAAAAAAAULfV2ED3G2+8EXvssUfccccdafftJLhdr169nNvGrLnPqlWr4s4774wDDjggJk6cuBmqAAAAAAAAAAAAAADqshoZ6H7++efjyCOPjM8//3y9IHcuG+vQ3blz57WdvJM5kp/vv/9+fO1rX4vZs2dXUQUAAAAAAAAAAAAAADUw0D169Og49dRTY+nSpentdYPca4LZ624bM3To0DTA/e1vfzuKi4vXzvfJJ5/EmWeeWYWVAAAAAAAAAAAAAAB1XY0KdK9evTrOOuusWLRoUakgd6NGjeLoo4+OH/3oR3H33XfHo48+mnbyTuTr3r3G7rvvHvfcc0+8/fbbsdNOO62d84UXXognn3yyiqsCAAAAAAAAAAAAAOqq4qhBHnjggRg5cuTagHYSuu7SpUtcddVV8Y1vfCNatGhRqfn33HPP+Ne//hWHHXZY2qE7mf/666+Pfv36FagCAAAAAAAAAAAAAIAa2qH75ptvTn8mQetkO+ecc+Kjjz5Kf1Y2zL3GlltuGY888kg0aNAgvf3uu++m5wAAAAAAAAAAAAAAqLOB7rFjx8aoUaPS7tzJdu6558a9994bTZo0Kfi5dt999zjzzDPX3n7mmWcKfg4AAAAAAAAAAAAAgBoT6B48ePDa3zt37hy33HJLlZ4v6fq9RtKlGwAAAAAAAAAAAACgzga6R44cufb38847L5o2bVql5+vZs2c0bNgw/f2DDz6o0nMBAAAAAAAAAAAAAHVTjQl0T5gwYe3vffv2rfLzNWjQILbaaqsoKSmJ2bNnV/n5AAAAAAAAAAAAAIC6p8YEuufOnbv292233XaznLN58+bpz3nz5m2W8wEAAAAAAAAAAAAAdUuNCXSvWLFi7e8tW7bcLOecNWtW+nPVqlWb5XwAAAAAAAAAAAAAQN1SYwLdTZs2Xfv71KlTN0uAfMaMGet16gYAAAAAAAAAAAAAqJOB7o4dO679/Z133qny8w0aNCjtzF2vXr3o3LlzlZ8PAAAAAAAAAAAAAKh7akyge+edd177+wMPPFDl5/vb3/629vfddtutys8HAAAAAAAAAAAAANQ9NSbQfcghh6Q/S0pK4qmnnornn3++ys711ltvpYHupDt3onfv3lV2LgAAAAAAAAAAAACg7qoxge4+ffpE69at05B1Euo+7bTTYtiwYQU/z2effRannnpq+ntynqKiovj6179e8PMAAAAAAAAAAAAAANSYQHfDhg3jrLPOSkPWSah7/vz5ccwxx8Q111wTy5YtK8g5nnzyyTj00ENj0qRJa89zwgknxFZbbVWQ+QEAAAAAAAAAAAAAamSgO/GTn/wkmjVrlv6ehK2XLl0a1157bXTt2jUuu+yyeP/99ys856JFi+Khhx6KI444Ik4++eSYMmVKOnci6c595ZVXFrwOAAAAAAAAAAAAAIBEcU16GDp16hTXX399fP/7309D18mWdNKeNm1a/PrXv0631q1bx4EHHhg77rhjdOvWbe2xs2fPjnvvvTcWLFiQdvceP358fPDBBzFmzJhYvnx5ep81XbnX/Pzv//7v2HvvvTOsGAAAAAAAAAAAAACozWpUoDuRhKyHDx8ejzzyyNpQdyIJYSfmzJkTL7zwQrqtkYxNmjQpBg4cuN5ca45ZY81ciV69esWvfvWrKq4GAAAAAAAAAAAAAKjLalygO/HAAw/EsmXL4sknn1wbwl43jL1hUDvfvg2PW3O/pMP3M888E8XFNfLhAQAAAAAAAAAAAABqiKKogRo0aBBPPPFEXH/99VG/fv1S42s6d68b1l53X67xJMidbBdddFEMGTIkWrZsudnqAQAAAAAAAAAAAADqphoZ6F7jsssui8GDB8d+++23NpC9oQ2D2xtac9wuu+yShsRvvfXWNDAOAAAAAAAAAAAAAFDVanSgO3HwwQfHW2+9Fa+99lqceOKJ0bRp07Uh7Y1tDRs2jCOOOCINcn/44YfRr1+/rMsBAAAAAAAAAAAAAOqQ4qglDjvssHRbtWpV/Pvf/45hw4bFpEmTYvbs2TFr1qxYvXp1tG3bNrbYYovYaqut4qCDDkq3Ro0aZb10AAAAAAAAAAAAAKCOqjWB7jXq168fBxxwQLoBAAAAAAAAAAAAAFRnRVkvAAAAAAAAAAAAAACgrhLoBgAAAAAAAAAAAADISHFWJ65uTjvttCgqKoqdd945DjvssHQDAAAAAAAAAAAAAKhKNSrQ/d5778XChQvT3w899NCCzv3KK6/E7Nmz197u0aNH3H333dGzZ8+CngcAAAAAAAAAAAAAoEYGus8999x4//33o169erFy5coqO09JSUl6nqRL90svvRSHHHJIlZ0LAAAAAAAAAAAAAKi7iqKGScLWyVZV8yZbEhhPtqVLl8a3vvWtWL58ecHPBwAAAAAAAAAAAABQ4wLdVeXtt9+O4cOHx+233x777rvv2tD4pEmT4pFHHsl6eQAAAAAAAAAAAABALSTQ/f9169YtDjzwwDj//PPTcHfSmXuNZ599NtO1AQAAAAAAAAAAAAC1k0B3HjfddFMUFf3vw/Pee+9lvRwAAAAAAAAAAAAAoBYS6M6jXbt2sd1220VJSUlMnz496+UAAAAAAAAAAAAAALWQQHcZGjRokP5ctGhR1ksBAAAAAAAAAAAAAGohge48li5dGhMmTEh/b9asWdbLAQAAAAAAAAAAAABqIYHuPK688so01J3YYostsl4OAAAAAAAAAAAAAFALFUc18P7778fIkSM3er/Zs2ev/f3+++8v2PlXr14dy5cvT+dPunK/9tprMW7cuHSsXr16scceexTsXAAAAAAAAAAAAAAA1SrQ/cQTT8S1115b7vuXlJTEOeecU2XrSeZfV58+farsXAAAAAAAAAAAAABA3VUtAt25QtSFvn9FJF2515yjadOmMWDAgCo7FwAAAAAAAAAAAABQd1WbQPe6QeryBLnLc9/KWHOeP/zhD9G6desqPRcAAAAAAAAAAAAAUDcVZb2A6iIJcK+7dejQIR5++GHduQEAAAAAAAAAAACA2t2hO+mAve222270fl999VWsWLEi7c7dpUuXgp2/QYMG0aJFi2jXrl306NEjDj300Dj++OOjuLhaPDwAAAAAAAAAAAAAQC1VLRLLF198cbptzN577x2jRo1Kfx8/fvxmWBkAAAAAAAAAAAAAQNUpqsK5AQAAAAAAAAAAAAAog0A3AAAAAAAAAAAAAEBGBLoBAAAAAAAAAAAAADJSHDXI+eefH1OnTs16GQAAAAAAAAAAAAAAdS/Q/d3vfjfrJQAAAAAAAAAAAAAAFExR4aaq2UaMGBGffvpprFixIuulAAAAAAAAAAAAAAB1hED3/3fMMcfELrvsEq1atYrDDz88/vGPf2S9JAAAAAAAAAAAAACglqtRge6BAwfGEUccEUceeWSVzF9SUhJLly6NIUOGxCmnnBLf+MY3YtmyZVVyLgAAAAAAAAAAAACAGhXofuedd2Lw4MHpVhXq1au3dkvC3U888UR8//vfr5JzAQAAAAAAAAAAAADUqEB3Verfv38cffTR0aFDhzTMvSbUfdddd8Unn3yS9fIAAAAAAAAAAAAAgFpIoPv/+9Of/hQvvPBCfPXVV/Hkk09G27Zt14795S9/yXRtAAAAAAAAAAAAAEDtJNCdw3/8x3/Efffdt/b28OHDM10PAAAAAAAAAAAAAFA7CXTncfzxx0ebNm2ipKQkPv3006yXAwAAAAAAAJRE2/kAAQAASURBVAAAAADUQgLdZejUqVP6c968eVkvBQAAAAAAAAAAAACohQS6yzBnzpyslwAAAAAAAAAAAAAA1GIC3Xm899578dVXX6W/t2rVKuvlAAAAAAAAAAAAAAC1kED3BqZOnRoPPfRQ9O/fP71dr1696NKlS9bLAgAAAAAAAAAAAABqoeKoBm655ZZ025g1HbMT3bp1K9j5V69eHcuXL485c+akPxMlJSVrx/fff/+CnQsAAAAAAAAAAAAAoFoFuufOnRsTJkwo9/2TsHVF7r8pks7ca0Ldp5xySpWeCwAAAAAAAAAAAACom6pFoHvdEPXGrAlZl+e+hVhPnz594tBDD63ycwEAAAAAAAAAAP+PvfuAjqJ6/z/+EEINNUCo0nuv0ruCBRBQitKxIAgqCIqKggUbICgqVaogRaSqNCmCivTeO0oLnQAhpPzPc7//3d/WZDd9k/frnDnZuTsz997Z2ZWYzzwrAAAAqY5fUg8gOdLQuC5VqlSRefPmJfVwAAAAAAAAAAAAAAAAAAAAAKRQyapCt6X6dnxv621V7qpVq0qvXr3k5ZdflnTp0iVIPwAAAAAAAAAAAAAAAAAAAACQLALdbdu2laJFi8a43fvvvy/nzp0zoetp06bFW/8a2s6aNavkzp1bKlasaB4DAAAAAAAAAAAAAAAAAAAAQKoIdFepUsUsMRk3bpwJdKsePXokwsgAAAAAAAAAAAAAAAAAAAAAIOH4JeCxAQAAAAAAAAAAAAAAAAAAAADRINANAAAAAAAAAAAAAAAAAAAAAEnEpwLdUVFRST0EAAAAAAAAAAAAAAAAAAAAAIg3/uJDdu/endRDAAAAAAAAAAAAAAAgQYSFhcmePXvkyJEjcv36dbl165ZkzpxZcuTIIUWKFJGaNWtKtmzZ4v3v8Lt27ZLLly9LpkyZpGDBgtK0aVMJDAyMl+OfOHFCZs+ebV1/5JFHpEGDBvFybAAAAABIKXwq0A0AAAAAAAAAAAAASB5Onz4txYoVi7fjbdu2zQSWPbFhwwYTOo4vwcHBkjt3bkkKkZGR8ssvv8jEiRPl999/l/v377vd1s/PT6pXry4vvfSSPPfccxIQEBDrfufPny/vvPOOnDx50um5tGnTylNPPSWjRo2S4sWLS1z069dPVq9ebR5nz57drAMAAAAA7Pk5rAMAAAAAAAAAAAAAgERw9OhRadiwobRp00Z+/fVXpzB3+vTpncLf27dvN4Hu8uXLy5o1a7zuU4/xwgsvSOfOne3C3BritoiIiJCff/5ZKleubELmsaXHsIS51UcffSRBQUGxPh4AAAAApFQEugEAAAAAAAAAAAAASGRbtmyRGjVqyF9//WVtS5MmjXTs2NEEtW/dumUC3qGhobJjxw558803JXPmzNZtz549Ky1atJBJkyZ51e+QIUPk+++/t643aNDAjEX7unPnjkyfPt1U0la6/uSTT8ru3bu9nt/du3dl4MCB1vUqVapQnRsAAAAA3PCXFEp/sTxz5ozcuHFDbt68ae4m1l9mAQAAAAAAAAAAAABISqdOnZLHH39cQkJC7Kpxz507V55++mm7bTNkyCDVq1c3S48ePeSxxx6Tc+fOWZ/v27evFChQQFq3bh1jvxoeHzt2rHW9du3apgK3pRK4BsZ79uxpqn9r5fCwsDAT9O7evbvs2rXLrop3TEaOHGlC55ag+rfffuvV/gAAAACQmqSYCt337t2TWbNmSe/evaVSpUqSI0cO81N/yWzVqpUMHTrU5X76S61+rdPx48cTfcwAAAAAAAAAAAAAkBIMHz5coqKi4rTUrFkzVn1rRem49p07d25JTIMGDTLFyWx99tlnTmFuRxq0/umnn8Tf//9qt+n4+/fvb/5mHhPtQ7e3mDBhgjXMbevhhx+WF1980bq+b98+Wb58uXjq2LFjMnr0aOt6t27dpH79+h7vDwAAAACpjc8HurUK96uvvioFCxaUXr16ycyZM+XAgQMSERFh9wu4O/v375cRI0ZIuXLlzC+RJ0+eTNTxAwAAAAAAAAAAAABSjxMnTsiSJUvs2h566CEZMGCAR/tr2PqZZ56xa9NK2PPnz492v+DgYPnll1+s67Vq1ZJq1aq53b5Pnz5OwXlP6d/wtbq3yp49u3zxxRce7wsAAAAAqZFPB7onTZpkqnDrVzPp3cu24W39yibL4gkNgOvXV1WtWtX8BAAAAAAAAAAAAAAgvi1btsypTStz21bdjkmnTp08Oq6tv/76SyIjI63rzZs3j3Z7/Vt83rx5ret//vmnR2PTsPrKlSut6x9++KHdcQAAAAAAKSTQrXfytm3bVvr16ychISEmxG0b4LYNcUdXndt2G8s+ejyt1P35558n6BwAAAAAAAAAAAAAAKnPvn37nNpq1Kjh1TFq1qzp1LZ3795o93F8Xr/FOiZly5a1Pr569ar8+++/0W5/7949GThwoHW9cuXK8sorr8TYDwAAAACkdj4X6Nbwdbt27czdxbZBbnfbxmT06NHSvn17SZs2rfV4+vOdd96RH3/8MQFmAAAAAAAAAAAAAABIrS5duuTUlidPHq+OkTt3bqe2ixcvRruPBrJjOkZM/Tgew9Gnn34qp0+ftq5/88035m/xAAAAAIAUFujWr2P67bffXFbitgS4CxQoIOXLl5c6deqYdXeBb8tXV/30009y7NgxeeaZZ+xC3f3795crV64kwqwAAAAAAAAAAAAAAKmBq8JknhQri+sx9NuqbWXMmDHGfjJlyhTtMWydOHFCvvjiC+t6165dpWHDhjH2AQAAAADwsUD38ePHzR29jkHuIkWKyNtvvy1//vmn3Lx5U86dO2e+puqvv/7y+Nh6jAULFsjXX39tDXXfuHFDPvjggwSaDQAAAAAAAAAAAAAgtcmfP79T2+XLl706RnBwsFNbvnz5ot0na9asduv37t2LsZ+7d+9Gewxbr776qty/f988zpYtm4waNSrG4wMAAAAAfDDQ/fnnn0tYWJh5rKFr/SVw/PjxJug9cuRIqVu3rmTJkiVOfWhV7uHDh1srfs+ePVtCQ0PjaQYAAAAAAAAAAAAAgNSsfv36Tm3bt2/36hiutte/l0cnV65cduuXLl3yOjjueAyLZcuWya+//mpd18JpMQXMAQAAAAD/x198hIaq58+fbypna9A6b968snr1aqlUqVK89zVs2DD56aef5MCBA3L79m1Zvny5dOjQId77AQAAAAAAAAAAAICURqs0699ajxw5Yr5h+cGDBxIYGGjCwGXKlJFixYolWN9aUXrv3r1y4sQJ03dkZKTpVxf927Kr6tiJrV27dvLaa69JSEiItW3RokUyZswYSZcunUfHmDt3rlNbly5dot2nSpUqduv6GsXk0KFD1sd58uSRggULuvxb/uuvv25d1/OshdQAAAAAACkw0L1+/XrrL7Qa6p43b16ChLlV2rRpZcCAAfLyyy+b9b/++otANwAAAAAAAAAAAABEY9OmTeYbllesWCH37t1zu13hwoWlZcuWMnDgQClXrly89K1FuiZOnGiKgoWHh7vdrnTp0vLkk0+aALKOIynkzJnTFBkbOnSote38+fMyduxYefPNN2Pc/++//5YlS5bYtTVp0kQef/zxaPfTCt76t/CIiAizvmbNmmi337Vrl1y5ciXayuLqs88+k1OnTlnXv/nmG/H395koAgAAAAAkC37iI7Zs2WINc7dv314aN26coP3pL/EWO3fuTNC+AAAAAAAAAAAAAMCXaZi6UaNGsnDhwmjD3Ors2bMyZcoUqVChgnTq1EmuXr0ap77ffvttadOmjfz666/RhrnV0aNHTXC6ZMmS0q9fvxjHmlA0uN25c2e7Ng15z5kzJ9r99uzZI08//bQ1lK204nlM+6ncuXNLq1atrOv79u2TzZs3u91+woQJduu9e/d22kaD3J9//rldlXC9DgAAAAAAKTTQrb9YWzj+YpsQ9Kui9BfaqKgoOXPmTIL3BwAAAAAAAAAAAAC+6tKlS+ZnpkyZpGfPnqZi9smTJ+Xu3bty48YNOXTokHz33XdSp04d6z76t9gFCxZIjRo14lRk6+LFi9bK1xrS1srT+jfe0NBQExbXEPTo0aNNgNziwYMHJrCsVat1nIlNC5lpCHvEiBGSIUMG65i6du1qwunLli0z51Tbrl+/boLX+i3TtWvXlgsXLliP88gjj5hvnC5QoIBH/WpVcD+//4sJ9O3bV+7cueO03R9//CHTp0+3rletWtWuKJrFq6++as6zypo1q4waNcrLMwEAAAAA8KlAt+0vpTVr1kyUPjXQrW7dupUo/QEAAAAAAAAAAACAr2rSpImp+qxBYK0ErZWjNeCdPXt2KVu2rAkP//333zJt2jRJnz69dT8NX2sw+dixY7HuW6tWHz58WL799ltzrMKFC5ugdGBgoFSuXFneeOMN2b17t3zyyScmTG2hYe/mzZvLlStXJLFpsHr48OFm3EOGDDFjVhqGf+qppyRfvnzmPOkcGjZsKN98843cv39f/P39pUWLFrJ06VITXtftPKWBeu3LYv/+/VK/fn1zHA12X7582ZzDJ554wlrtXF/D2bNn2wXB1YoVK8xioeH0/Pnzx8OZAQAAAIDUx198hN65beHNL6RxYfkFNSQkJFH6AwAAAAAAAAAAAABfpAHgxYsX2wW13enVq5fkypVL2rZta6p0K61CrZWpt2/fLgEBAV71/eKLL8rkyZNj3E6D0G+//bZkyZLFVJa2OH36tAmEr1+/3im0nBiKFi0qHTp0MBWuf/rpJ9m7d2+0Rcl0vo899pgJecfGp59+Krdv3zYV0y2hdg2Iu6Jj0tB2xYoV7dq1Kvdrr71mXdfq57bn1F0RN606roF6DX4HBQXFavwAAAAAkBL5TIXutGnTWh/fu3cvUb8WzJP/6QAAAAAAAAAAAAAAqYkW4tLKzrosWrTIq7+ranj7+eeft2vTStVff/21R/tXqVLF9Ltu3TqZOHGiV+MeMGCAqeJt648//pD58+dLYtNq3BqWfvjhh+X99983YW6tLK5VwwcNGiQjR46Ud955R7p06WLOt1YS10B248aNTQV0rdodERHhVZ8aqNYq3EuWLHEKalvoGLTPAwcOSKNGjZye/+KLL+TkyZPWdR2HBuYdXbt2Td58800z1gIFCki1atWkatWqkjdvXilTpox88MEHpjI4AAAAAKR2PlOhW7+Gy/YOaf0lLyHp13rpXcn6y6zeHQ4AAAAAAAAAAAAA+D8ZM2Z0CkZ7Y/jw4TJt2jSJjIy0to0ZM8YErrWKdnRy5swZp74//PBDWbt2rVNb586dzd+IE9qDBw9Mpe2ZM2fatevcNditlbgd6XmaO3euDB482BQn079p6/YzZsyQZcuWmcC0N5566imzHD16VHbv3i2XL182r2nBggVN9W93r4H+vf6zzz6zrj/77LPSpEkTp+1+//136dixowl1u6L9jhgxQqZOnWqqgGtIHwAAAABSK5+p0K137NrepZzQ9G5kixIlSiR4fwAAAAAAAAAAAACQmhQqVEiaNWtm13b16lVZvXp1gvddt25dKVWqlFOF8F27diV431FRUdKpUyenMPf06dNNhXJXYW7l5+cnXbt2lS1btkjRokWt7Tt27DDzuXjxYqzGU7p0aRO87t+/v7zwwgvy+OOPRxuof/31163fqp01a1YZPXq00zbr16+XJ5980hrm1r/3//zzz3Ljxg3zGs+aNUvy5Mljnvv3339NIPzgwYOxGj8AAAAApAQ+E+iuXr269Zdb/cqsmzdvJlhfoaGh8tVXX1nXa9asmWB9AQAAAAAAAAAAAEBqpZWgXVV2TgwNGjRIkr71b9GLFy+2a+vXr5/07NnTo/01zK2VujXgbXH27FlTKVv/np6Qfv31V1m6dKldlXXHyuD6t/zu3bvL/fv3zbo+//fff0u7du3MN3MHBgZKt27dZOPGjRIQEGC20aC37hMeHp6g4wcAAACA5MpnAt3Nmzc3P/XrrfTOYv36Kduv3opPb731lvmaKIuWLVsmSD8AAAAAAAAAAAAAkJpVq1bNqe2ff/5JsX2HhITIRx99ZNeWIUMGGTFihFfH0Yrcbdq0sWvbsGFDgn7btQa0X3vtNet6+fLl5dVXX3Xa7ptvvjFVty2++OILyZs3r9N25cqVk6FDh9pVGv/pp58SZOwAAAAAkNz5TKC7ePHiUqdOHfNY7ypetGiRuYPX8hVN8WXIkCEyfvx4Exy33C3s+DVfAAAAAAAAAAAAAIC4cxX0vXz5cortWwPLjn/jfvzxxyVPnjxeH8tVRe9vv/1WEsqoUaPk+PHj1nX9u3q6dOmctps8ebL1cc6cOaVz585uj9mnTx+7SuMTJkyI1zEDAAAAgK/wmUC3Gjx4sAlza9haf65YsULKlCljvpJKv4IpLv744w+pWbOmfPnll2bd0s/AgQOt4W4AAAAAAAAAAAAAQPzJnj27U9uVK1dSbN/r1q1zaqtfv36sjlWvXj2ntk2bNplK2vHtzJkz8sknn1jXO3Xq5LIwmga+z549a13XbdKmTev2uBpkr1q1qnX977//ltDQ0HgdOwAAAAD4Ap8KdLdv314aNmxoF+q+evWqDBo0yFTS1uc13L1t27YYA976C6cGwocNG2ZC4U2bNpVdu3ZZj61LyZIlpX///ok2PwAAAAAAAAAAAABITfTvs44Sq+BWUvR99OhRp7aiRYvG6lgahs6cObNd27179+wC1fFFC6HpsVWWLFlkzJgxLrfbsWOH3Xr58uVjPLbtNg8ePJC9e/fGebwAAAAA4Gv8xcdMnz5datWqZQLbll+m9RdtvUt36dKlZnF1R/XBgwfloYcektu3b0tISIjdL+e2jy1B8QwZMsgPP/wg6dOnT7S5AQAAAAAAAAAAAEBqcvPmTae23Llzp9i+r1275tSWI0eOWB9P9717965TlfFSpUpJfFm1apUsXrzYuv7+++9LwYIFXW57+fJlu/W8efPGePygoCC79eDg4FiPFQAAAAB8lc8FuosXLy4///yzPPnkk9Y7gG2D3bYsVbq1PSwsTP777z+Xx7S9y1q39fPzswbHAQAAAAAAAAAAAAD29u3bZw1E699VtWBWbFy6dMll5enobN++3RT8Ug0aNIhVv7HtO64cK2ory1xiw9W+mTJlkviif2d/9dVXretly5aV119/3e32t27d8nosjufEVdAeAAAAAFI6P/FBjRs3NncBFyhQwKm6tuMS3XOO2+ixtKr3Tz/9JJ07d070eQEAAAAAAAAAAACALxgwYIA0bNjQLEePHo31cXbu3OnUFlPhrWeeecbat35Dc2L2HVeuAuNXr16N1bHCw8Ndhp8dK17HxejRo+1e32+++UbSpUvndnvHALcnYXVLIbfoQu8AAAAAkNL5ZKBb1a9fX3bv3i1PPPGECWI7Vuf2hmX/GjVqmF/a27ZtG69jBQAAAAAAAAAAAICUyt03JXti06ZNTm2PPPJIovS9efPmOPUdG1rh2pNguSf27NkjERERdm3ZsmWTvHnzSnw4e/asjBw50rresWNHad68ebT7BAYG2q0HBwfH2M+VK1eiPQYAAAAApAY+G+hWuXLlkhUrVphFq3ZbgtmuqnY7st22atWqMnfuXPnnn3+kWLFiiTwLAAAAAAAAAAAAAPBdf/zxR6z2O3PmjKxfv96uLUeOHPLoo48meN+634kTJ+zaSpQoIdWqVfP4GFu2bJHPP/9cpk2bJiEhIR7t89hjjzm16bdTx4ar/TSQnjZtWokPgwYNkrt375rHAQEBMmbMmBj3qVixot36wYMHY9zn0KFD1sf6t/0KFSrEarwAAAAA4Mt8OtBtoVW69Rf9Xbt2ySeffCItW7aULFmy2IW2bZf06dNLo0aN5P333ze/qOsdz507dxY/vxRxOgAAAAAAAAAAAAAg0cycOVNCQ0O93m/EiBFO38Q8cOBAU2XaU5MmTfK6X0vfjvTvx57+zfijjz6SunXrytChQ+X55583RcQuXboU437NmjWToKAgu7YjR47IypUrvRi9yP3792XixIlO7fp37/iwZs0aWbRokXX9vffek0KFCsW4n54H/Vu9hf4dPzIy0u32N27csKtQrhXMtbAbAAAAAKQ2KSrBXKVKFfML82+//SY3b96Ua9euyfHjx03l7b///luOHj1qvq5J7yLesGGD+SW9QYMGST1sAAAAAAAAAAAAAPBZ58+fl7fffturfX7++WeZMWOGXZt+m/Lrr7/u1XE0DPzVV195tc+XX37pVBn84Ycfli5duni0v1aU/uCDD+zatNr3m2++GeO+mTJlchkm79u3rwk3e+qtt96Sc+fO2bXVqlVLnnnmGYmrBw8eyIABA+xC1lqt2xP+/v7y7LPPWteDg4PNa+2OVjcPDw+3rvfs2TPW4wYAAAAAX5aiAt2O9Ou4ihcvbn5xrV27tpQsWVICAwPN1zQBAAAAAAAAAAAAAOLHuHHjZPDgwSYMHBMNcjtWks6aNassX77cq+rcFm+88YaMGTPGqdq3o4iICPn000/N9rYKFiwoS5YskbRp03rUn4bB9ViOVq1a5dH+L774ojz22GN2badPnzbVu//7778Y56BFzhxD7NmzZ5epU6fGy9/CNfCuVcMtxo8fL+nSpfN4fw1/67dmW+h1ocXYHJ05c0ZGjhxpXc+TJ4+88MILcRo7AAAAAPiqFB3oBgAAAAAAAAAAAAAkDg1VlylTRr744gtTOfvevXumXUPeZ8+elenTp5tvUO7Vq5dd8Dt//vzmW5grVKgQq3415Kyh4cqVK8u3334rBw4ckPv375vn9Kd+q7O2V6tWTd555x27fbX69Jo1a8wYPOUuOB5ToNy2ivWiRYukcePGdu27du0y52/IkCHy119/yZ07d0y7niutAD5x4kSpWLGifP75505h7hUrVpj5x9W///4rH330kXVdK34/8sgjXh1Dz+mHH35oF9yuX7++rF271lTjDgsLM+H9hg0b2gW9J0+ebAq0AQAAAEBq5J/UAwAAAAAAAAAAAAAA+JbZs2fLlClT5Pvvv5fz589b20+dOiVvvfWWdV0rNWsg2VXYWatJP/XUUzJp0iQJCgryuG+thK3h5lmzZtkFgvfv3y/9+/e3rmfIkMEa7HYVqu7evbupdJ0lSxbxhgax/fz8JDIy0q69RYsWHh8jc+bM8vvvv8uoUaNkxIgR1nFqiHv06NFmsZw/DUC78+ijj5rX4KGHHpL4oNW1LUHygIAAU607NvQauH79ujV8fvjwYTNWPW/K9txpm14Dbdu2jZc5AAAAAIAvokI3AAAAAAAAAAAAAMArGiDWKsxaeVuDyRoELleunAlp29IwsmOYW/d98cUXTQB78eLFXoW5lVaxHjt2rPz333+mMvXLL78sxYoVc9rOVZi7ZMmSMnDgQFO1W4PQ3oa5lVbJ/uCDD+zmWrRoURPO9kbatGll6NCh5hx+9tln5riuzp+j3LlzS8+ePeXvv/+W1atXx1uYW1/HhQsXWteHDRsWp2PrnH755RepVKmStU2D3LZhbq3YrvN44YUX4jByAAAAAPB9VOgGAAAAAAAAAAAAAMSKhpKbNWtmljFjxpjqzhrU1sC0Vmi+deuWqTKdM2dOE0SuWrWqFClSJF76zpgxozz55JNmUTdu3DB9nzx50jy+ffu2ZMqUSQIDA03fNWvWlHz58sVL3xp2fuyxx2TTpk2SLVs26dixo2TNmjVWx9JAu1a01kXP144dO+TcuXPWOWil8Rw5cpg5VKlSxWV4PT5cvHhRhg8fbq1griH9uHriiSfMcuTIEfnnn3/k0qVLJrSeP39+adiwoRQuXDgeRg4AAAAAvi9FBbpv3rwpf/75p2zZskW2bt1qvtpL/yeBLnqXr/5PAl30F2L9Zb1OnTpSv359yZs3b1IPHQAAAAAAAAAAAAB8XkBAgNSuXdssiU1Dz1rxWZfEoH9z1iU+aTi8adOmkhS6dOmSYMfWquq6AAAAAABScKBb7+SdMGGCLFiwwO5rsxy/uuvChQtmOXTokGzcuNG0+fn5mTuC+/bta+6gBgAAAAAAAAAAAAAAAAAAAIDE4ic+TMPZTz31lNSrV09mz54toaGhJsRtWZR+XZPtomy3iYiIkBUrVpiv4Xr44YfNV3ABAAAAAAAAAAAAAAAAAAAAQGLw2UD3vHnzpEKFCiaMbQlnO4a3LQFuW662sey/fft285VYI0eOTJI5AQAAAAAAAAAAAAAAAAAAAEhdfDLQPXbsWOnSpYvcuHHDLsgdW7bh7rCwMHn//feld+/e8TpmAAAAAAAAAAAAAAAAAAAAAPD5QPfcuXPljTfecBvktlTb9nRxZKnYPXPmTHnrrbcScWYAAAAAAAAAAAAAAAAAAAAAUht/8SFnz56Vl19+2Ty2DXJbgtn58+eX5s2bS9myZaVUqVJmPSAgQLJkyWK2v3PnjlkuX74sx44dk6NHj8rGjRvl+PHjdse0hLpHjx4tLVu2lGbNmiXJfAEAAAAAAAAAAAAAAAAAAACkbD4V6H7nnXckJCTEGrzW0HW6dOmkW7du0r9/f6latWqsjqvh7u+//14mTJggt2/ftlb+1uMPHDhQ9uzZE88zAQAAAAAAAAAAAAAAAAAAAAARP/ERly5dkgULFliD1rpUq1ZNDh06JFOnTo11mFtpNe/PPvtMTp8+La1atbJW/Fb79++XdevWxdMsAAAAAAAAAAAAAAAAAAAAAMAHA90rVqyQ8PBw81hD3U2bNpU///xTihcvHm995MyZU5YtWyYvvviiXaj7559/jrc+AAAAAAAAAAAAAAAAAAAAAMDnAt1btmwxPzVonStXLpk7d65kzJgxQfr65ptvpEqVKk59AwAAAAAAAAAAAAAAAAAAAECqDHSfOHHCWp27Z8+ekjdv3gTrK126dPLGG29YA+QnT55MsL4AAAAAAAAAAAAAAAAAAAAApF4+E+i+ffu29fEzzzyT4P09/fTT4uf3v9MTEhKS4P0BAAAAAAAAAAAAAAAAAAAASH18JtCdPn166+NixYoleH+ZMmWyVgG37RsAAAAAAAAAAAAAAAAAAAAAUl2gOzAw0Po4R44cidJn9uzZzc9cuXIlSn8AAAAAAAAAAAAAAAAAAAAAUhefCXSXKlXK+vjixYuJ0ufly5clTZo0UrZs2UTpDwAAAAAAAAAAAAAAAAAAAEDq4jOB7nr16lkf79u3L8H7O3/+vFy7ds08btSoUYL3BwAAAAAAAAAAAAAAAAAAACD18ZlAd4sWLSRz5szm8YIFCxK8P0sfWqG7Q4cOCd4fAAAAAAAAAAAAAAAAAAAAgNTHZwLd2bJlk549e0pUVJTMnTtXdu/enWB9aWXuzz77zIS527VrJ6VLl06wvgAAAAAAAAAAAADAl+3du1fGjRsnK1euTOqhAAAAAADgk3wm0K0++ugjCQoKkoiICOncubP8+++/8d7HvXv35Nlnn5XLly9L7ty5Zfz48fHeBwAAAAAAAAAAAACkBAsXLpQaNWrIwIED5fHHH5fXX389qYcEAAAAAIDP8alAd86cOWXJkiWSKVMmOXbsmNStW1d+++23eL1zvHHjxrJmzRrJnj27/PLLL5IvX754Oz4AAAAAAAAAAAAApCQDBgyQ8PBw6/rXX38thw4dStIxAQAAAADga/zFx9SpU0c2bNgg7dq1k//++09atWolDRs2lF69eslTTz0lOXLk8Op49+/fN1/99eOPP8qiRYtM9e9KlSrJTz/9JKVKlUqweQAAAAAAAAAAAACAL7ty5YpcunTJri0qKkoOHDgg5cqVS7JxAUjZxo0bJzdu3LCuV61aVdq2bZukYwIAAAB8MtD94YcfxvkYzzzzjEyaNElCQ0Nl06ZNZkmTJo0ULVpUKlasKPnz55esWbNKQECAabe4e/euhISESHBwsBw8eFCOHj1qvWNc/+eCVuRu06aNCXhbvP/++3EeLwAAAAAAAAAAAACkJLly5TLL1atX7dpLly6dZGMCkDoC3WfOnLGu9+jRg0A3AAAAfF6SBLpHjBhhF7KOCz2OBrGV/jx58qScOnXKo30t+9keS+8g/+STT+zaCXQDAAAAAAAAAAAAgDj9fXXs2LHm25T1m5BV7969pXLlykk9NAAAAAAAfEqSBLrdBapjyzEc7s1xY9o3voLnAAAAAAAAAAAAAJDSdOvWzXyD8vr166VUqVLSunXrpB4SAAAAAAA+J0kD3QkVlo7LcW33ja/AOQAAAAAAAAAAAACkVNWqVTMLAAAAAABIxRW6EwKVuQEAAAAAAAAAAAAAAAAAAACk6EB3lixZpEaNGpIcbd++Xe7cuZPUwwAAAAAAAAAAAAAAAAAAAACQgiVpoLtkyZKyfv16SY70K8H27NmT1MMAAAAAAAAAAAAAAAAAAAAAkIIlaaAbAAAAAAAAAAAAAIDonD17VjZt2iT//vuvpEmTRvLlyye1atWScuXKebS/FvLatm2bBAcHS+bMmc3+DRo0kIIFCyb42ENCQmTLli1y7NgxuXHjhmTMmFEKFCggFStWlAoVKkhi2Ldvn+zevVsuXrwoYWFhkjVrVsmRI4cUKVJEihcvLoUKFTLn1RP3798389HXQs9naGio5MmTR4KCgqRmzZqSP39+SYkiIyNl69atsmvXLrl27Zr5NnI9d/Xq1ZNcuXLFuP/du3dl8+bNcujQIfNN4bpPsWLFpFGjRuaaiG/nz5+XHTt2yKVLl6zXfe7cuc01X7t2bcmUKZMkBr3udByXL1+W9OnTmzFocb1KlSp5fM0BAAAAqQWBbgAAAAAAAAAAAACARzZs2CBNmzaNdpvhw4fLiBEjnNpnzJghvXr1inbf6dOnS8+ePc3jAwcOyODBg2XVqlUSFRXltG3lypVl5MiR0qpVK5fHWrZsmbzzzjvmOK5omHb06NEmHB4f8+7Ro4eZo9Lg7gcffCCLFy82IWpXypcvL/369ZO+ffuKn59fjGOIKQDbuHFjM04VHh4u3333nYwdO1ZOnz4d7X6nTp2SokWLRrvN6tWrZfz48bJu3ToTTnanatWq0rlzZxkwYIAJEUdH+zxz5ky023gyTk+uK9trq0mTJrJx48Zot7W93vT4ej27Gmu6dOmkQ4cO8tlnn8lDDz3k9LyGt/Ua/fbbb+XWrVtOz2uwuk+fPuY9oyH7uNBwvfYzZ84cEzx3J0OGDOaGhldeeUXatWvn0bG9PWezZs2Sjz76SI4fP+5yW72J4N1335UXX3xR0qZNG2P/MV0rM2fONIun4wMAAACSo5h/KwQAAAAAAAAAAAAAIBHNmzfPVHxeuXKl2yDm3r17pXXr1jJkyBC7dt3+9ddfl6eeesptmFv98ccfpsLyTz/9FK9jnzZtmgk2z58/3xrm1uCvo4MHD0r//v3NGDSsHF+0InPDhg3ltddeizHMHZMTJ07II488Ii1btpQVK1Y4hbn9/f2dKjIPHTpUSpYsKXPnzhVfpq9dp06dTFjcXZj4wYMHZp56c4FW4LZ17tw5Uw37008/dRnmVvfu3ZNx48aZa+DChQuxHuvSpUulTJky5gYIxzC347WnVdZ///13ad++vXmP6c0H8UWP/cwzz5ibG9yFuZVWeNcbGXRb3QcAAABAElXo1l9m9O5h/YUiuUrOYwMAAAAAAAAAAACApFCiRAkZNWqUXZtjoNodDTlrJWKLGzduyFdffeW0nQaHu3btKhEREaY/rQ4cFBQkN2/elL/++suEhm1plW2t+KsBZqVVuS3HrVOnjlSvXl2yZ88u58+fl7Vr18p///1n3VcrWT/33HNSrlw5qVChglfz/uSTT+T69et2bT/88IM8//zz5rFWbtYKxA8//LDpX0O9O3bskEmTJpmwt8U///xjqn9rde3oKmXbnjtL5WjHoHFISIg0a9ZM9u/fb/27d/369SVPnjxy+fJl+fPPP+Xo0aPiib///lvatGkjV65csbbpcTQsr2HgYsWKSfr06c1xtXK3VgS3hJo1nNylSxcTFtZKza5ohWZ9TZW+XhrytfXhhx+aCtYWgYGBLo+jFdZtX5tFixbJli1bzONnn33WvP6W7ZSlSreFnndX1ae7d+8uCxYsMBWktfq5Xh8BAQFy9uxZWbNmjQnO217Leq527txpXkNdb9GihRw+fFiyZMliHhcvXlwiIyPlyJEjJlCtFbUt9Dzp9bJp06YYK7E7+vLLL02Q2/bGB0ugX+eZK1cuE07XcP7y5cvN+8Uydr0eNUz+888/R1uB3tNzpu9bPf9Kw+K65MyZU65du2beu/v27bPbfsmSJeb9OmbMmGjnaHutuHrvaT8avgcAAAB8WZoovlcG8BlaQaBixYrWdf0fMdH9jyUAAAAAAAAAQOq2v4N3oTAACafiwuj/LDtm6mD5Z/faBOl7xGtTpXypmpJQHAOoGjweMWJEjPtp9WgNBdv6/PPPzaLB14kTJ7oMaWootXPnznbVojVoq1WutWq3VpQuX768zJ492xrmta2o/MEHH8jIkSPt2h999FFZvXq1eEODu7aBag3RakBWw7NaYfzpp592u++PP/4o3bp1M6F1C61qvWfPHsmcObNH/WvA1jZUq6FjHdPMmTPN/PX86ZhsaYBdK2jbBmj1vDkGyXUeDRo0sAsd67pWM8+bN6/L8Wj0QKtRa/jWMfD/xRdfRDsXfT2GDRtm1zZ9+nQTJPaGns8iRYqY0H7GjBlNSFwDzdHRa1WvCVtTpkwxYXwNw+v51EC/Lb32Bg4cKJMnT7Zr11C2hsA1DK7Xnx5Dz3XWrFntttNQuFan3rZtm137nDlzzA0G3oS533jjDadzqSFpdzRsrtXrtUK97ftHw+ilS5f2qF9X50xvVOjTp4/5+7Wesxo1ajjt98svv5j3rt54YKGBeQ20lypVSmL73tOK4HqDA2KPfzMCvvPvxj+3r5SvZgxNkH4fa9RZendMmGMDQHJzIBlmMf2StHcAAAAAAAAAAAAAAP4/rbR8+/ZtWbZsmduKu61bt5Zx48bZtd25c0emTp0qr776qgn0asVoxzC3SpcunXz88ccmTGtLK3c7Vrv2llZX1qDv119/HW2Y21I52jFUfvz48WiDuJ4EEjRIq5XQtRK3Y5hb+fv7y2effRZteFYDvxpMtg1za3XppUuXug1zW4L9On59DRxfU309o6NVzfW1saWBdG9p2N9SgV1f45jC3O68/fbb5vpZtWqVU5hbaehex6chd1tanXrx4sUmzP3yyy+bwLdjmFsVLlzYjFWrd9vSa9hTWtldw/m2BgwYEOM1lCNHDhOs1veJ7ftHg+R6w0NsaZBfA+F6k4GrMLd68sknnV5XDeHrdQsAAACkdgS6AQAAAAAAAAAAAADJwpUrV0wQ1lUY2VavXr0kMDDQrk0D0gcPHjRVi6MLHluqRjtWl9aAbVxpmFqrFHtCKys7VigfP368HD58ONbnLn369Kb6t4Z23dFQd7169dw+r4FgrdptSytsO55vdz755BMJCgpyCmzbVlR3lC9fPlM12jGwvHv3bvGGVom28PR1cOXq1asmjK2Vq6MLsA8aNMiuTSvLa+V1nc/o0aOj7UOvUd3W1oYNG+TWrVsxjk/70erltgFsPeca1veEBskdq6ZrVfb58+dLbOn1p2HtmEL0ejNDwYIF7do0YA4AAACkdgS6AQAAAAAAAAAAAADJhmNI1l0ouWnTpnZtWmW4aNGi0r59+xj3r1WrluTMmdOubc+ePRJXGkb38/Psz/A6B62o7BjUnTBhQqz71yrLZcuWjXE7raI9ffp0s+TOndvafvHiRdNmq2TJkh6dUwsNQffv398p7DtlypRo9+vbt2+0Ae2YnD59WlavXm0e61enO1bP9kajRo3cVpm29cgjj5hgt+N1qGHy6MLgFi1atHC6sWDfvn0x7rdkyRKn4L/2qZXDPdWuXTun8LXeUBBb+p5yfE+6ou+PZs2a2bXpjRjh4eGx7hsAAABICQh0AwAAAAAAAAAAAACShcqVK5tQtidcBZfbtGnj0b4awi1durRd29GjRyWu2rZt69X2rVu3dmqbOXOmXeVlb3Tp0sWj7apXr24qPOui1ZptA9ShoaF222rlbMfQcmzOw7hx46LdR0O+jq/JnDlzJCQkxKM+taK2BuLjWp1bOVYLdydr1qxSoECBWO/v6hr25Dr8+uuvndq8Cd2rdOnSSf369e3atm7dGuv3gTf9ly9f3m49LCxMzp07F6t+AQAAgJSCQDcAAAAAAAAAAAAAIFmoVq2ax9sGBQXFaX/HIO7NmzclLvLnzy958+b1ah+tfu1qHLGpFq6Vj+vUqSNxYalwbcuxmrInKlWqZFf521JB+/jx49Hu5xjEvn37tgl1x0QD8NOmTTOPtUp1t27dJKmuQw1Ka4VwT7gKg8d0HWoF8D///NOuLTAwUKpUqSKxuYHC0ZYtWyS2Fbo9VbhwYae2uL7/AAAAAF9HoBsAAAAAAAAAAAAAkCw4VmiOTkBAQJz21+CvY3g4LlxVW/ZEqVKl4iVUW6RIEbtq2966e/eubNu2Ld7m5Wq/DRs2RLuPVgzPmDGjXZtWDY/J0qVL5dKlS+Zx586dJXv27JJU16FWmNdQd2yuQU+uw7/++kvCw8OdzrW3VdSVY+g+LoHuMmXKeLxttmzZnNri+v4DAAAAfJ1/Ug8AAAAAAAAAAAAAAAB3QU930qZNG6f9/f3t/1weEREhcZEjR45Y7Ve8eHHZuHGjXduhQ4e8Pk7OnDklLo4cOWIqXTueYw0ox0aJEiVk8+bNdm379++Pdh+tNN2xY0eZNWuWtW3Xrl2ydetWefjhh93uN3HiROvjl19+WZLyOozLNejJdejqHGrV7tGjR4u39Lw60krqCX39Z8qUyaktru8/AAAAwNcR6AYAAAAAAAAAAAAAJAuuqm4n5v5xEdvq2FmzZnVqu379eqL1b3H16lWXx/Tzi90Xf7sKNrvqw5EGsm0D3ZbAtrtA9/Hjx2XdunXmcfXq1aVWrVoSV3G5jhL6GnR1Dvfs2WOW+BCba8/bebu6GQMAAABI7WL3mxcAAAAAAAAAAAAAAPEsTZo0Sbp/XMQ2+OwqiB2bUG1c5+4qKByXcLKreXkS6K5bt65UqVLFrm3+/Ply48YNl9tPnjxZoqKizOM+ffpIfIjLuUzoa9CTcxgX7s5zcn7vAQAAACkBgW4AAAAAAAAAAAAAAJKIJYyc1OHYxOjT0z60Sretu3fvOlXtVmFhYTJjxgxrpfPnnntOUjpX57Br167mOoqP5dChQ0kyLwAAACC1I9ANAAAAAAAAAAAAAEAcRUZGxmq/O3fuOLXlzJlTEluuXLk8GpunQkJCPOrDlS5dujhV+J40aZLTdosWLZLg4GC3+6RErs6hq3MNAAAAwLcQ6AYAAAAAAAAAAAAAII5iG6q9detWsg1065xiG1R3NS9PA91abVsD2rYOHjwomzZtchvydqzqnVK5OoeuzjUAAAAA30KgGwAAAAAAAAAAAACAOLpx40as9jt58qRTW7ly5SSxlSlTRtKnT2/XFhERIadPn47V8U6cOOHUVqlSJY/379u3r1PbxIkTrY8PHz4sGzduNI9r164tVapUkdTA1Tl0da4BAAAA+BYC3QAAAAAAAAAAAAAAxJEGjGPj6NGjTm116tSRxJYpUyapVauWU7tWxo6v89G4cWOP99eAtuN5WLRokVy5ciXVVudW9erVcwrenz17Vm7fvh2r492/f1/Wrl1rXWIb4AcAAAAQNwS6AQAAAAAAAAAAAACIowsXLsilS5e82ufIkSNy8eJFu7YcOXIkWbXpli1bOrWtW7fO6+Ps3bvXGry2KFGihFm84RjU1vDx9OnTJTQ0VGbNmmU9X506dZLUQoP3DRs2tGuLioqK1euk1qxZI48++qh1OXbsmCR3adKkSeohAAAAAPGOQDcAAAAAAAAAAAAAAPFg6dKlXm2/fPlyp7aePXuKv7+/JIU+ffpIxowZneakgWFvLFmyxKnt9ddf93o8GtQODAy0a5s8ebLMnz9frl27Zta7d+9uQs6piatzuXDhwlgd64cffrA+1nPdtGlTSe4yZMhgtx4REeF223nz5lmXM2fOJMLoAAAAgNgh0A0AAAAAAAAAAAAAQDyYNGmSx+HnBw8eyDfffGPX5ufn51SVOjEFBQVJ79697dpOnjwpCxYs8PgYd+7ccZpXnjx5nI7rCQ2X9+jRw67t+PHjMmTIELsQemrz5JNPSqVKleza9DU6evSoV8c5dOiQ/PTTT9b1/v37J9nNBN7Ili2b3frdu3fdXovPPvusddm5c2cijRAAAADwHoFuAAAAAAAAAAAAAADigQZGtYK0J0aPHu1UMfjVV1+VMmXKSFL65JNPpHjx4nZtQ4cOlStXrni0/zvvvCPBwcHW9TRp0sj3338vmTNnjtV4XAW2Lcdv2LChlC9fXlIbPaczZsywq1StNwi8+OKLcv/+fY+Oce/ePVPd3FLdOnfu3LGqop4USpQoYbf+33//udxOw/+2ChYsmKDjAgAAAOKCQDcAAAAAAAAAAAAAAHHUqFEjCQgIkAEDBsiSJUui3Xbu3LkybNgwu7aSJUvKyJEjJallz57dVHvW6tgWp0+fljZt2sjFixfd7qeVyT/99FP5+uuv7doHDx4srVu3jvV4NODerFkzl88lZTXzpFa9enX56quv7Nr++OMP6dChg1y/fj3afS9dumSqfG/fvt0uIJ4zZ07xBXXr1rVb37t3r6nG7WjevHnWx3pDQdWqVRNlfAAAAEBsJP/vygEAAAAAAAAAAAAAJAvnzp2T+fPnR7vNX3/9ZapPWzz00EPSqVMnuXXrll31aleh05UrV9pVgn788celQoUKLvu2hFFtTZkyxS6UqmFi22Pv37/fun7gwAG7fXV8tuPWYLNWPPZUsWLF5KWXXpKuXbtKu3btzJxfeOEFefjhhyVbtmxy+/Zt2bFjh0ycONHpHBYuXFjWrFkTbRVr3UfPgYXtY8u67fhdnQNP1ahRQ9atW2dC3JbX4++//5ZKlSqZKs7PPPOMmW+6dOlMtez169fLt99+K5s2bbI7zrvvvisff/yxxJUGt3U8trSi9NNPP+31sfT61MV23ZHjeYzuOorpdahYsaI89thjbo/tany229SrV88s7qqX+/n5Sb9+/SQ8PNy0LV++XMqWLSv9+/eXtm3bmmrWel1p5e59+/bJ0qVL5bvvvpNr166Z7XX/8ePHm4B3Qp0zx/f+iRMnXF7ftu9pff/oZ4crnTt3ljfffNNajVyrjet7VcdQoEAB817TgPqYMWOs+/Ts2VPSp0/vdo4AAABAUksTpbfJAvAJ+j+V9Bd+C/0fBZb/gQUAAAAAAAAAgKP9HdIk9RAA/H8VF0b/Z9kxUwfLP7vXJkjfI16bKuVL1YyXY23YsEGaNm3q1T6NGzc2+2mVZw0Be2P69OkmiBnbvm3/HK7HmTlzpsf7FilSxIzZnaJFi8qZM2es6z169DAh0mnTpknfvn0lLCzM+pwGnx88eODyOBr4/vHHH6V48eLRjqdJkyayceNG8VZcIgEavNXQ8O+//+70nFZ1Tps2rTVIbCt//vwmXPvcc89JfNBzp6F32wrhGhgeNWqU18caMWKEfPDBB4l2HVmuC9vz5o3hw4ebMUdHX59XXnlFjhw54vL5DBkyWMPPtgoWLCgTJkyIsYJ6XM9ZbN77epOAXvPuaHVyvbnAk7mWLl1a/vnnH8mRI4dXY0hN+Dcj4Dv/bvxz+0r5asbQBOn3sUadpXfHhDk2ACQ3B5JhFtMvSXsHAAAAAAAAAAAAACAF6d27t+zevVs6dOhggtzKVZi7XLlypjKyVr6OKcydVLS689q1a2XVqlXSqlUruwriGth1DHNXqVJFPv30Uzl+/Hi8hbmVnsfnn3/eLhStQXP8T/PmzU0ARatgazVvDdrbcgw4ly9f3gTuDx06FGOYO7l67bXXZPbs2U5Bcdu5arhb349btmwhzA0AAIBkL1VW6NavGPrhhx/MP9r1658CAgLMnaf6S45+/VW1atWSeoiAz9wVAgAAAAAAAABIvqi2CCQfKaVCN2Ku0G0rJCTEBLaPHTsmN27cMAHTAgUKSKVKlez+7ucrNCyrf2c/d+6c+Vu7rufOnVvy5s0rNWrUMHND0rt+/bp5nbSiub5OkZGRJtBcqFAhqVmzpuTLl09SCp3b3r17ZdeuXXL16lVzTWbLlk1KliwpdevWJcjtIf7NCCQvVOgGgNSZxfQXH3P48GHZunWry+eefPJJyZUrl9t9r127Jp07d7Z+HZQlyx4aGmr+Yb9v3z4ZN26cvPTSS+ZuVA16AwAAAAAAAAAAAAAQG1myZJFHH33ULCmBBtIbN26c1MNADHLmzCmPP/64pAZ+fn5StWpVswAAAAC+zOcC3W+++ab88ssvLp/TO5vdBbrv3LljfknWr7ayBLn1a5gstM3Srl9DpF8tpF8bpb+QAgAAAAAAAAAAAAAAAAAAAEBC8BMfcvnyZVm5cqVTANvyMzqDBg0yX7FjCXLbhrlt23TR423atEmef/75BJkHAAAAAAAAAAAAAAAAAAAAAPhcoHvevHkSHh5uHtuGr/VrqipVqiTZsmVzud+OHTtk6tSp1hC3bQDcEgy3DYhbjvvjjz/Khg0bEmVuAAAAAAAAAAAAAAAAAAAAAFIff/Ehv/76q/WxBq5LlCghX3zxhbRq1UrSpUvndr9PP/3UbG8JdFsC27pPp06dpHbt2qZ9//79smDBArl+/bp1mxEjRhDqBgAAAAAAAAAAAAAAAAAAAJC6A92hoaHyxx9/WEPZNWrUkNWrV0vOnDmj3e/8+fOyZMkS635Kg9r58uWTlStXSuXKle22//DDD6V9+/by559/mvXNmzfLmTNnpEiRIgkyLwAAAAAAAAAAAAAAAAAAAACpl5/4iK1bt5pQt4axM2bMKPPmzYsxzK10u8jISOu6pVL39OnTncLcKk+ePLJs2TLJnTu3dXtdBwAAAAAAAAAAAAAAAAAAAIBUW6F7z5495qeGsTt37iwlSpTwaL8FCxY4hbmbN28uLVu2dLuPBsVfe+01ee+998z6zp074zx+AAAAAAAAAAAAAIBvOnfunMyfP9+u7datW3brBw4ckNGjR1vXH3roIenUqVOijREAAAAA4Lt8JtC9d+9e6+N27dp5tM+lS5dk27ZtJsRta9CgQTHu2759e2ug27ZvAAAAAAAAAAAAAEDqcuLECRkyZEi022zfvt0sFo0bNybQDQAAAADwiJ/4iIsXL1ofV6tWzaN9li9fbqpy2ypcuLA89thjMe5btmxZyZQpk3l8+fJlr8cLAAAAAAAAAAAAAAAAAAAAACmmQveNGzesj3Pnzu3RPkuXLrU+1mC3Vuru2LGjR/vqtvny5ZNTp07JzZs3YzFiAAAAAAAAAAAAAEBK0KRJE6diYgAAAAAApLoK3WFhYdbH/v4x59Dv3Lkjv//+uwlm2+rcubPHfQYEBJifoaGhXo0VAAAAAAAAAAAAAAAAAAAAAFJUoDtbtmzWx55UzF6+fLlTELtEiRJSrVo1j/u8d++eXbAbAAAAAAAAAAAAAAAAAAAAAFJloDtHjhzWx8eOHYtx+9mzZ1sf61dfaaXuDh06eNXntWvXzM+sWbN6tR8AAAAAAAAAAAAAAAAAAAAApKhAd5kyZayPN2zYEO22J0+elFWrVpkQt62OHTt63N9///0n169fN8cICgqKxYgBAAAAAAAAAAAAAAAAAAAAIIUEuqtXr259PG3aNImIiHC77fDhwyUyMtIpEF6lShWP+1u7dq31cdmyZb0eLwAAAAAAAAAAAAAAAAAAAACkmEB306ZNJX369Obx8ePH5Y033nC53axZs2TOnDnW6txRUVHmcZcuXbzqb8KECdbH5cuXj9PYAQAAAAAAAAAAAAAAAAAAAMAVf/EROXPmlFatWsnPP/9sAtrjx4+X7du3S8+ePaVo0aJy/fp189zChQud9tUgeO/evT3ua9y4cbJ161breq1ateJtHgAAAAAAAAAAAAAAAAAAAADgc4FuNWzYMFm6dKlERkaaytt///23WWxZKnLbPtbQd/78+WM8/v379+Wzzz6Tjz76yOyn+2fMmFEaNWqUYHMCAAAAAAAAAAAAAAAAAAAAkHr5VKC7atWq8uqrr8rYsWOtgWtHljC3Ra5cuWTkyJFuj7lt2zb5448/5MCBA/Lrr79KcHCw9bh6rMaNG0uGDBkSYDYAAAAAAAAAAAAAAAAAAAAAUjufCnSrUaNGyb///isLFy50Cm/b0lB2unTpZP78+RIYGOh2uwULFsiXX35p3UfZhsU7duwY73MAAAAAAAAAAAAAAAAAAAAAAOXna6fBz89PfvzxR/niiy8kU6ZMJnjtailcuLCsXbtWmjZtGuMxLfvYrqusWbNKp06dEnQ+AAAAAAAAAAAAAAAAAAAAAFIvn6vQbQl1Dx48WF566SVZunSp/Pnnn3LhwgUJDw+XggULSvPmzaV9+/amQndMSpQoIY0bN3b53COPPGJC4wAAAAAAAAAAAAAAAAAAAACQEHwy0G2RLVs26datm1li6+WXXzYLAAAAAAAAAAAAAAAAAAAAACQ2nw50w9nJkyfln3/+Mcvu3bslODhYrl27Zpb06dNLzpw5zZI/f36pVauW1K1bV+rUqSOBgYFJPXQAAAAAAAAAAAAAAAAAAAAg1SHQnQJocHvevHkyf/58OX36tNvtwsLCJCQkRM6dOyd79+6VVatWmfZ06dJJu3btpH///tKwYUPxBUWLFpUzZ84kaB9FihSJ9nwCAAAAAAAAAAAAAAAAAAAAceUX5yMgSWg4+/vvv5cKFSpItWrV5PPPP491+PjBgweyYMECadSokQl0nzhxIt7HCwAAAAAAAAAAAAAAAAAAAMAZFbrdGDFihJw9e1bSpEljgtPJKcj97bffypgxY+S///5zu12hQoWkSZMmUqtWLQkKCpLs2bPL7du35fLly7JlyxZZvXq1BAcHO+23efNmqVKlinz55Zfy0ksvJfBsAAAAAAAAAAAAAAAAAAAAgNSNQLcbS5culT179iS7QLeGzAcNGuT2+XLlysnHH38sbdu2FT8/1wXY+/fvb6pyT5kyxQTXHYPdd+7ckT59+si5c+fko48+ivc5AAAAAAAAAAAAAAAAAAAAAPgf14lf+KTnnntOtm/fLu3bt3cb5rZIly6d9OvXz2xfvnx5l9toMHzy5MkJNFoAAAAAAAAAAAAAAAAAAAAABLpTiJYtW8qsWbMkc+bMXu1XuHBhWb9+veTNm9fl86+//rocO3ZMkrMePXpIVFRUvC+nT59O6qkBAAAAAAAAAAAAAAAAAAAghSPQnQIEBATIjBkzJG3atLHaPygoSKZNm+byuXv37smbb74ZxxECAAAAAAAAAAAAAAAAAAAAcMVfktCDBw9k8+bNEhkZKQ0aNJAMGTJEu33v3r0TbWxnz54VX9G/f3/Jly9fnI7xxBNPSPXq1WXnzp1Ozy1ZssRU6S5VqlSc+gAAAAAAAAAAAAAAAAAAAACQTALdly5dkscff1z27Nlj1suVKyerVq2SggULut1Hq1CnSZMmEUfpG7p27Rovx+nbt6+8+OKLLp+bN2+evPfee/HSDwAAAAAAAAAAAAAAAAAAAID/8ZMkMnDgQNm9e7dERUWZ5dChQzJo0CCP9rXsk5CLryhatKhUrFgxXo7VuHFjt8+tW7cuXvoAAAAAAAAAAAAAAAAAAAAAkAwqdK9evdqu2raGqNeuXevRvlTp/j+VK1eOt2OVKlVKcubMKdevX3d6zlJJHQAAAAAAAAAAAAAAAAAAAEAKCHQHBAQ4BYezZMni0b6JUUE7uYbG9bw9+eST1vU2bdrE6/GDgoJcBrq17fbt25I1a9Z47Q8AAAAAAAAAAAAAAAAAAABIzZIs0N21a1f59NNPrcFp/fncc895tG9gYKC0bt06Qce3bNkyl8HmpJY/f35ZsWJFgh1fK3S7Q6AbAAAAAAAAAAAAAAAAAAAASCGB7uHDh0twcLDMmDHDVNzWgPeIESM82rdw4cIyffr0BB1ftWrVkmWgO6FFRka6fS5DhgyJOhYAAAAAAAAAAAAAAAAAAAAgpUuyQHf69Oll8uTJ8vXXX5tAd6ZMmZJqKLBx8+ZNl+3+/v6mMjoAAAAAAAAAAAAAAAAAAACAFBDotsiYMWNSDwE2Lly44LK9cuXKkiZNmkQfDwAAAAAAAAAAAAAAAAAAAJCS+YmP0WreSBhnzpyRW7duuXyucePGiT4eAAAAAAAAAAAAAAAAAAAAIKVL8grd3ujevbupEl24cOEE76tNmzZStWpVSU127Njh9rlOnTqJL7h586YsW7ZMtm7dKrt375azZ8+akPrt27clXbp0EhAQIAULFpRixYpJ9erVpVGjRtKgQQPx9/eptwIAAAAAAAAAAAAAAAAAAABSCJ9Ksc6YMSPR+vrggw8ktVmxYoXL9vLly0vt2rUlOTt+/Li89957snjxYrl//77LbSIiIiQ0NFSuXr0qe/fulaVLl5r2wMBAc7PAa6+9JkWLFk3kkQMAAAAAAAAAAAAAAAAAACA180vqASB5ePDggdtA99ChQyU5W716tQmdz5s3z22YOzrXrl2TcePGSZkyZWTw4MFy7969BBknAAAAAAAAAAAAAAAAAAAA4NMVupFwFi5cKMHBwU7tZcuWleeee06SswsXLtitp02bVho2bGiW0qVLS86cOU117kuXLsm///5rAuBbt26VyMhIu/3CwsJkzJgx8ssvv5jq3bpvQrp8+bLLcx5TJXLHMXsSYk+TJo2kT5/eqV33j4qK8rh/Pbf+/vYfG3oe9YYAb6RLl078/OzvJwkPDzevk6eYk3vMyT3m5Bpzco85ucecXGNO7jEn95iTa8zJPebkHnNyjTm5x5zcY06uMSf3mJN7zMk15pQ0c4pMm0Ei/TN6fIw0URGSNizEqT0ifRaJSpPW4+P4hYeKX4T9/8uOSuMnEemzijfSht2WNFH2/2+fOTEnX5qTK7afEZGRnn/meCvswQO7vymlls895sScosOc3GNOrqWmOfn6f3NT4r8jmFPqmpMj28+IB+HefVZ4Qz8/bP/NmJo+95gTc4oOc0qZcwoLC5PkhkA3DK1Q7cpXX31l3jC+QD8c+vfvL4MGDZKCBQu63e6DDz6Q8+fPy7Bhw2TGjBlOHx6HDx+WunXrmuB3jRo1Emy83333nRlLXGhAPVOmTB6dm+LFi7vc35sPpty5c5vFln5wnjp1SrxRrFgxyZAhg13bjRs35MqVKx4fgzm5x5zcY06uMSf3mJN7zMk15uQec3KPObnGnNxjTu4xJ9eYk3vMyT3m5Bpzco85ucecXGNOSTOn+7lLyb28FT0+RtrQm5L96G9O7SFFG0pExuweHyfTpf1msaUBi1tlHhdvZDvym/jfv2nXxpyYky/NyRXbz4g7d+5IQhZIyuh3KtV97jEn5hQd5uQec3ItNc3J1/+bmxL/HcGcUtecHNl+RgRf9q54pDdu3bpl91mUmj73mBNzig5zSplz+vfffyW5sY+jI1WaP3++bNu2zam9S5cu0qJFC/EFpUqVMlW3tcJ2dGFuiwIFCsi0adNkzZo1EhAQ4PT8tWvX5PHHH5cTJ04k0IgBAAAAAAAAAAAAAAAAAAAAAt2p3r179+Stt95yatdQtLuq3clNxYoVZfPmzVKlShWv923evLmsWrVKMmZ0/oqU4OBg6dSpk9el+wEAAAAAAAAAAAAAAAAAAABPpYmKioryeGukOP3795dvv/3Wri1t2rSybt06adSokSRXw4cPl6tXr0qaNGlMIL1QoUJxOt6nn34q77zzjsvnRo0aJYMHD5b4dvnyZRMa98bx48elbdu21vWdO3dK+fLlY9xPz5N+xYEj/WoDbz4C9Nrw9/e3a4uMjPQ69J4uXTrx87O/nyQ8PFwiIiI8PgZzco85ucecXGNO7jEn95iTa8zJPebkHnNyjTm5x5zcY06uMSf3mJN7zMk15uQec3KPObnGnBJvTvs7pPm/46TNIJH+zgVO3EkTFSFpw0Kc2iPSZ5GoNGk9Po5feKj4Rdy3a4tK42e+Ct0bacNuS5qoSLs25sScfGlOVebcjvYz4uuZQ2Xb3vWSEN7pN0HKlaieKj73bDEn5hQd5uQec0p9c7L9N2NK+G9uSvx3BHNKXXOqPC/U7WfE37tWy3c/vCcJ4dH6HaR7+8Gp4nPPEXNiTtFhTilzTgcPHpTq1f/v9+T9+/dLhQoVJCkR6E7FVq5cKY8//rhT+yeffCJvv/22pCb6xq1UqZIcPnzY6bkcOXLIuXPnJEuWLJLUDhw4YCqSJ6cPEQAAAAAAAABA8uUYzgGQdCoujP7PsmOmDpZ/dq9NkL5HvDZVypeqmSDHBgD4Pv7NCPjOvxv/3L5SvpoxNEH6faxRZ+ndMWGODQDJzYFkmMW0j6Mj1Th9+rR069bNqb1r166pLsyt9A6PPn36uHzuxo0bMnPmzEQfEwAAAAAAAAAAAAAAAAAAAFI+At2pUEhIiLRp00auXLli116/fn2ZOnWqpFZdunQxZfddmTdvXqKPBwAAAAAAAAAAAAAAAAAAACkfge5UJjw8XJ599lnZt2+fXbuWjl+2bJlkyJBBUqs8efLYldC39ddff8mtW7cSfUwAAAAAAAAAAAAAAAAAAABI2Qh0pyJRUVHSq1cvWbFihV17iRIlZM2aNRIYGCipXfXq1V22R0ZGyrZt2xJ9PAAAAAAAAAAAAAAAAAAAAEjZCHSnIgMGDJAffvjBrq1IkSKydu1ayZcvX5KNKzmpVKmS2+eOHDmSqGMBAAAAAAAAAAAAAAAAAABAykegO5UYPHiwfPvtt3ZtxYoVkz/++EOKFi2aZONKbqKrUn727NlEHQsAAAAAAAAAAAAAAAAAAABSPgLdqcDQoUNlzJgxdm2FCxeW9evXm5/4P9mzZ3f73K1btxJ1LAAAAAAAAAAAAAAAAAAAAEj5CHSncMOGDZPPP//crq1QoUImzF2kSJEkG1dyFRAQ4Pa5+/fvJ+pYAAAAAAAAAAAAAAAAAAAAkPIR6E7Bhg8fLiNHjrRry58/v6xbt06KFy+eZONKzkJCQtw+lzFjxkQdCwAAAAAAAAAAAAAAAAAAAFI+At0p1McffywffvihXVvevHlNmLtUqVJJNq7k7ubNm26fy5YtW6KOBQAAAAAAAAAAAAAAAAAAACkfge4U6LPPPpP33nvPri1Pnjzy+++/S9myZZNsXL7g6tWrbp8rXLhwoo4FAAAAAAAAAAAAAAAAAAAAKZ9/Ug8A8Wv06NHy9ttv27UFBgbK2rVrpUKFCl4f799//7U+zpAhgwmGJ6WiRYtaH1erVk0WL14cr8ffs2eP2+fKlCkTr30BAAAAAAAAAAAAAAAAAAAAPhPoPn78uLz00ktun//444+lXr16kpqNGzdOhgwZYteWI0cOWbNmjVSuXDlWx3zooYesjxs3biwbNmyQpHTmzBm7gHl827Fjh8t2Pz8/qVmzZrz3BwAAAAAAAAAAAAAAAAAAgNTNZwLdixYtMmHiNGnS2LVHRUWZtitXrkhqNn78eBk4cKBdW7Zs2WT16tVSvXp1SYnOnz8vERERkjZt2ng73qFDh1w+pzcL6PkEAAAAAAAAAAAAAAAAAAAA4pOf+Ijff//dLsStC/5nwoQJ8uqrr9q1ZcmSRVauXCm1atWSlCokJES2b98eb8ebOXOmCYi70qlTp3jrBwAAAAAAAAAAAAAAAAAAAPC5QPfWrVut1bktPzNlyiQ9evSQsWPHptgq1DGZMmWKvPLKK3ZtAQEB8uuvv0rdunUlOVi+fLnUqVPHvF558uSRXr16yYULF+Ll2D/99FO8HCc0NNScS1eyZ88uPXv2jJd+AAAAAAAAAAAAAAAAAAAAAFv+4gMuX74st27dMkFuS2XukiVLytq1a6Vw4cKSWk2bNk369OljV61cQ9MaoG7YsKEkB5MnTzZjtA1Oz5gxw1Rc37JlixQoUCDO1cmHDBkiQUFBcTrOhx9+KKdOnXL53DvvvGMqngMAAAAAAAAAAAAAAAAAAACpskL31atX7dY12D1p0qRUHeaeNWuWvPjii3Zh7owZM8rSpUuladOmklyC+K+//rrL586dOyeDBg2Kcx937tyRLl26SHh4eKyPsWrVKhk1apTL56pWrep2DgAAAAAAAAAAAAAAAAAAAECqCHT7+9sXEs+ePXuyCS0nhTlz5kivXr0kMjLSrl2rX7do0cIE3uNriYtffvlF7t275/b5JUuWSFhYmMSVVmrv2LGj3Lx50+t9dQxPPfWUy0B4rly5ZOHChZI+ffo4jxEAAAAAAAAAAAAAAAAAAADw2UB3YGCg3XqBAgUktfr111+lR48eTmHu5OjChQvRPn///n2n6uuxtXjxYlNNe/r06ea4MTl58qQJgbdr187l9nrNrVy5UkqWLBkv4wMAAAAAAAAAAAAAAAAAAABcsS99nUxppeSgoCAJDg4267dv35bUauvWrRIRESG+IG/evNE+r5WvHcP6MSlWrJicOnXK5XOnT5+W3r17y+DBg6Vx48ZSp04dE/7PmTOnPHjwQC5fvixnzpwxQe2dO3e67aNChQomIF6qVCmvxgYAAAAAAAAAAAAAAAAAAACkyEC3atKkiSxYsMA81mBueHi4+Psn3PA18BsSEmIeN2rUKMH6ScmefPJJyZgxo4SGhrp8/qmnnpIMGTJ4dcwTJ07IqlWrZNKkSbJixQpzHTi6du2aCWTr4o1MmTLJoEGD5L333vN6XAAAAAAAAAAAAAAAAAAAAEBs+ImP6Natm/VxWFiYrFu3LkH7e/7556Vp06bSrFmzBO0nJcuXL5+MGTPG5XMPPfSQjB071utjpkmTRh577DET1j537pxMnjzZBMMDAgJiPU4dy7Bhw+TkyZPy8ccfE+YGAAAAAAAAAAAAAAAAAABAovH3pWrPtWvXln/++cesf/fdd9KiRYsE7TMqKkqSmxEjRpjFV/Tr108KFCggn3zyiezdu1eyZMkirVu3Nuv58+ePc2D8xRdfNMv9+/dl27Ztpo99+/bJgQMH5NKlS3Lz5k2zPHjwwFQLz5EjhxQsWFBKly4t1apVM6F9/QkAAAAAAAAAAAAAAAAAAAAkBZ8JdKtZs2ZJrVq15Pbt27J8+XKZP3++dOrUKamHhRi0bdvWLAlJq2o3aNDALAAAAAAAAAAAAAAAAAAAAICv8BMfUqpUKfnll18ka9aspnp29+7dZc6cOUk9LAAAAAAAAAAAAAAAAAAAAABI+YFuVb9+fdm6datUq1ZNHjx4YELdrVq1ki1btiT10AAAAAAAAAAAAAAAAAAAAADAK/7iQ/744w/r41GjRsm0adPkxx9/lN9++80sRYsWlYYNG0rVqlWlQIECki1bNsmcOXOs+goJCYnHkQMAAAAAAAAAAAAAAAAAAACAjwe6mzRpImnSpHFqj4qKMj9PnTolp0+fltmzZyfB6AAAAAAAAAAAAAAAAAAAAAAgBQe6HQPcFrYhb8fnYstVcBwAAAAAAAAAAAAAAAAAAAAAJLUHuh3D1pYQt7YTxAYAAAAAAAAAAAAAAAAAAADgK3wy0O2IEDcAAAAAAAAAAAAAAAAAAPDUuXPn5IUXXpDVq1db29avXy9NmjSR5CwsLEy2bt0qZ8+elStXrsjt27clICBAHnroIalataqUKFEiXvuLjIyUf/75Rw4cOCDBwcGSLVs2KVy4sDRt2lSyZMkSL33s2LFDli9fbl1/5plnpGLFivFybMBX+GSg21KRGwAAAAAAAAAAAAAAAAAAJD8ajN64cWOCHT8u4espU6bI4MGD5datW+IrVqxYIePHj5dNmzbJvXv33G5XqlQpE1R/5ZVXTNA7tiIiImTSpEny0UcfycWLF52ez5Ahg3Tt2lU++eQTCQoKinU/4eHh0qNHDxMYV8WKFZOhQ4fG+niAr/ITH6R3kehdHwm5VKlSJamnCQAAAAAAAAAAAAAAAAAA4rEqd8uWLeWll17ymTD3vn37pEaNGtK6dWtTTdw2zJ0mTRrx97ev63vs2DF56623pHTp0rJmzZpY9al9aH8aCrcNc9v2df/+ffn++++lUqVKsmfPHomtr7/+2hrmVl999ZVkzJgx1scDfJVPBroBAAAAAAAAAAAAAAAAAEDqlT17dq+rclesWNGEon3FggULpE6dOrJz505rW4ECBWTMmDEmuK2hal3Onz8v06ZNsytkq22PPfaYmbe3tPL2b7/9Zl1v27atCZaHhYXJ9evXTf9aoVtdvnxZmjVrJv/995/X/Vy4cEFGjBhhXW/VqpUJkgOpEYFuAAAAAAAAAAAAAAAAAADgMzSYXa1atVhX5S5evLjUrFlTkrOVK1fKs88+K3fv3rW26TwOHjwogwYNkpIlS0q6dOnEz89P8ufPL7169ZJt27aZqtoWkZGR0qdPH1myZInH/c6bN09+/vln63r79u3Nup5zrQieI0cO0/+PP/5o3ebatWvm/Hpr8ODBcvv2bfNYq3JrdW4gtSLQDQAAAAAAAAAAAAAAAAAAEsypU6ckKioq1su6devsjte3b99YVeXWQPKAAQNk7969UqFCBUmuTp48KZ07dzaBbIvKlSubYHV0lck14D1+/HgTwrbQ89ezZ08JDg72qO9PPvnE+lhD1t988405b47atWsnTz75pHX9119/ld27d4un/vjjD5k7d651/c033zRBeyC18rlAt364JAa9+6Zx48bSqFGjROkPAAAAAAAAAAAAAAAAAAA4mzBhgvVxlixZpFu3bh7tZ1uVWytab9y4Ub7++msJCAiQ5GzYsGFy8+ZNuzYdd+bMmWPcV8PXWunadls91vDhw2Pcd8eOHbJv3z7reuvWrU31b3e0+ret6dOniyfCw8PtKokXLVpUhg4d6tG+QErlU4Hu69evm0XvzEhoemfO+vXrzQIAAAAAAAAAAAAAAAAAABLfxYsXZcmSJdb1rl27StasWT3e38/PT1577TXZs2ePNGzYUJK7w4cPy/z58+3atDq3Fqj1VKFChUyFb1uTJ082x47O5s2b7dabN28e7fbNmjWzq97tuL87WvV7//791nUNoGfKlMmjfYGUyqcC3fpVAbroHTYAAAAAAAAAAAAAAAAAACBl0+KsDx48sK7369fP431Lly5tCsiOGzfOo+rWycGiRYskMjLSru3pp5/2+jjPPPOM3XpERESMFbT37t1rt16uXLlot9dK54ULF7bbPyoqKsaAvm218CeeeELatGkT7T5AauCf1AMAAAAAAAAAAAAAAAAAAAApiwaKq1atah5ny5YtVsfQELIGui3q168vlSpV8nj/3bt3+1zl5zVr1ji11axZ0+vj1KlTx6lt4cKF8vnnn7vd5+rVq3bruXPnjrEf3ebMmTPmcXh4uNy6dcsU7nXnzTffNNuoDBkymOrcAAh0AwAAAAAAAAAAAAAAAACAeNa/f/84H2PFihVy7ty5WFXnVr4W5lY7d+50aoupUrYrOXPmlAIFCsj58+etbadOnZJt27ZJrVq1XO4TEhJit54xY0avz7Eew12ge/PmzTJ79mzr+pAhQ6RkyZIx9gGkBn5JPQAAAAAAAAAAAAAAAAAAAABH3333nfVxUFCQqfqdkt2/f19u377t1B4YGBir4+XJk8epbcuWLW63z5o1q936vXv3Yuzj7t270R7Dttr6K6+8Yl0vUqSIvPPOOzEeH0gtUlyF7rNnz5o7SHQ5efKk3LhxQ27evCmFChWSRYsWOW3/+++/m69h8OROEgAAAAAAAAAAAAAAAAAAkPCOHz8ua9assa4///zzkj59eknJrl275rI9S5YssTqeq3D1vn373G6fK1cuu/VLly5JhQoVou0jODjY+jhdunRuA93ffvut7N2717o+btw4n6ygDiSUFBHo1g+N77//XqZOnSpnzpxxej4qKkoePHjgct8XXnhBrly5Iu3bt5d3331XSpcunQgjBgAAAAAAAAAAAAAAAAAA7kyaNMlk/5Sfn5/06dNHUjp3gfWwsLBYhZ/TpEnj1LZ//36321epUsVu/cCBA9KsWTO322s18X///dduf1d9Xr58Wd5//33r+mOPPSZt27b1aA5AauEnPkzL+ffv318KFy4s7733npw+fdp8gDsuMblz54788MMP5k6Sfv36ma8tAAAAAAAAAAAAAAAAAAAAiS80NFSmT59uXX/iiSekSJEiktLlzJnTZSBag9OxcfPmTae2o0ePut2+YcOGduu2FdJdWbt2rV1Gs0GDBi63e/PNN61jyZAhg4wfPz7GsQOpjc8Guv/++2+pXLmyTJgwwVTf1g8F/SBztcTEsk1ERIS5q6d27dpy9uzZRJgFAAAAAAAAAAAAAAAAAACwtWDBArl69ap1XQu1pgZaiTxv3rxO7bHNM166dMmp7datW263r1q1ql2V7t9++01OnTrldnvNb9rq3bu30zZ//fWXzJo1y7o+ePBgKVmypEfjB1ITnwx064dE8+bN5eTJk3ZBbldiqtBdqFAhayVvPYb+3Lt3rynpf+3atQSaAQAAAAAAAAAAAAAAAAAAiCkoXLx4cWnZsqWkFvXq1XNq27dvn9fHuXz5sstAtxbQvX//vtv93n33Xevj8PBweemll0yxXEdz5861q+Ddpk0bqVSpkt02ut8rr7xizXEWLlxY3nnnHa/nAqQGPhfo3r9/v3Ts2NF8pYKyDXJbgtm2S0w2bdpkAtx6Z4i/v7/1eEeOHJFu3bol4EwAAAAAAAAAAAAAAAAAAICtXbt2yZYtW6zrffr0MZWrU4smTZo4tdkGpz21YcMGt8/dvn3b7XMdOnSQTp06WdfXrl0rjzzyiKm0fe/ePfn333/lo48+kh49eli3yZMnj0yaNMllMH/37t3W9bFjx0rmzJm9nguQGviLD4mMjJTu3bvLnTt3nILcGTNmlIYNG0rlypWlbNmykiNHDsmSJYs8/vjjbqt3W1SsWFGmTp0qAwYMkM6dO5swtx5z5cqVsnTpUnnqqacSYXYAAAAAAAAAAAAAAAAAAKRuttW5M2TIYIq1pibPPvusvPXWWyY8bbF8+XITws6aNavHx5k9e7bb52yP7crMmTNN0V3NT1rC4fXr13e5bYECBUzgPF++fHbtwcHB8t5771nXtcp6+/bt3fapmc3//vvPVBXX112PGxgYGO04gZTEp25b0Q8YvVvDEtDWN/BDDz0k33//vXkTr1q1SkaNGiXPP/+8PP30015/zUKVKlXkjz/+MIFw7UOPP3LkyASaDQAAAAAAAAAAAAAAAAAAsLh165bMnTvXrlp07ty5JTXR+TqG2ENCQrzKMv7555+yYsUKt8+nT58+2v01UL148WKTzSxatKjLbbTgbv/+/WXfvn1Svnx5p+c1lH7jxg1rf19//bXL45w7d05efvllyZ8/v8mD1qxZUypVqiS5cuWSatWqybhx4+TBgwcxzBjwfT5VoVvfmEqD1qpXr17yzTffSKZMmeKtDy39v2DBAvOhEBYWJjt27JBDhw5JuXLl4q0PAAAAAAAAAAAAAAAAAADgXBn6zp071vV+/fpJavThhx+aQPaZM2esbWPGjJFGjRrJE088Ee2+Fy5ckO7du8cY2I6JFsXVYLkue/bskYMHD8qVK1dMkLtw4cLSoEEDt8fZsmWLzJgxw7r+xhtvSOnSpZ22mzNnjrz00kty9+5dl8fRAsC6TJkyRX777TfTL5BS+UyF7qNHj5oPBf2Q0EWrcOvdH/EZ5raoWLGidOvWzboe3Z0qAAAAAAAAAAAAAAAAAAAg7iZOnGh9XLVqValbt66kRoGBgbJw4ULJnDmztS08PFyefvppGT16tNuK1WvXrjVB65MnT5r1pk2bOm2TNm1ayZ49u1fjqVKlijz77LMyYMAAU4i3efPmbsPckZGR8sorr1gL92rV7WHDhrkMc2tO0xLm1mrca9askdu3b8vFixdNRe+AgADznIbJdV6XLl3yatyAL/GZCt0bNmywPi5UqJB89dVXCdqffuhoYFxplW4AAAAAAAAAAAAAAAAAAJAwNm7caIK7Fn379pXUrFatWrJp0yZp3bq1nD9/3rSFhobKkCFD5NNPPzWVuosWLWqC1VqVe926dXL48GHr/m3btjWVrfPkyWN33Ny5c5uiugkZyt+5c6d1fezYsXbBdHX69Gnz+lpC35UqVZLNmzdbt9Mq4Boe1/ZHHnlEIiIi5Ny5c6aa99KlSxNs7EBS8plAt5bNt3jhhRec3uAJ8WGYPn16cyfLvn37ErQvAAAAAAAAAAAAAAAAAABSs++++876WCtId+nSRVK76tWry65du+TDDz804eywsDDTfu3aNfnhhx9c7pMvXz4T+O7Ro4dcvnzZ6fkiRYok2HivXLliV4370UcfNVXFHY0cOdJU4raYMGGCy0xokyZNpGfPntbivMuWLZMtW7ZInTp1EmwOQFLxEx+hd2RYPPnkkwneX7p06SR//vzmDhD98AMAAAAAAAAAAAAAAAAAAPHv0qVLsnjxYut69+7dJSAgIEnHlFwEBQXJN998I6dOnTLB5/bt20uZMmUkV65cJueoz2slaz1nCxculOPHj5sQtFbhvnPnjtPxdN+EMnToULl+/bp5rGMbP3680zYhISEyd+5c67qOvX79+m6P6VipXc8BkBL5TIXuGzduJModIra0bL+6efNmovQHAAAAAICn9ndIuK/CA+C9igv/97WQAAAAAAAAAADvTZ06VR48eOA2xAuRAgUKyMsvv2wWT/37779ObVWrVpWE8M8//8i0adOs64MGDXIZHtcK23fv3rWut2jRIsYq5Rpev3r1qllfv359vI4bSC58pkK37Yd1tmzZEqVPywdAREREovQHAAAAAAAAAAAAAAAAAEBqovm8yZMnW9ebNGki5cqVS9IxpRTHjh1zaqtbt2689xMZGSmvvPKKREX9r/hJoUKF5L333nO57Y4dO+zWy5cvH+2xtdK47fVw7tw5uXz5cryMG0hOfCbQnTlzZuvjixcvJkqAPDg42K5SNwAAAAAAAAAAAAAAAAAAiD+//PKLnD171rrer1+/JB1PSnL8+HG79Zw5c0qtWrXivZ8pU6bYBbW//PJLCQgIcLmtYxg7b968MR4/KCjIbt2S7QRSEp8JdOfLl8/6eNu2bQne37p168ydP3p3h94tAgAAAAAAAAAAAAAAAAAA4teECROsj/Pnzy/t2rVL0vGkJJs2bbJbb9Wqlfj7+8drH1evXpV33nnHut68eXPp0KGD2+1v3bplt54pUyavCgKrmzdvxmqsQHLmM4HuMmXKWB/Pnj07wfubM2eOxyX9AQAAAAAAAAAAAAAAAACAd06ePCmrVq2yrr/wwgvxHjhOrW7cuCFbtmyxa+vdu3e89/P222/LtWvXzON06dLJN998E+32jgHu0NDQGPu4d+9etAFvICXwmUB3gwYNzM+oqChZtmyZ/PbbbwnW1z///GMC3VqdWzVs2DDB+gIAAAAAAAAAAAAAAAAAIDWaOHGiyQSqtGnTSp8+fZJ6SMnCgwcP5OLFi2a5c+dOrI6xdOlSiYiIsK5XqVJFmjRpEo+jFNm2bZt8//331vWBAwdK2bJlo90nMDDQbj04ODjGfq5cuRLtMYCUwGcC3fpBkiNHDhOy1g/wZ599VjZv3hzv/Zw4cUI6duxoHms/fn5+fIUDAAAAAAAAAAAAAAAAAADx6P79+zJ9+nTreps2baRgwYKSkmiF7M8//1ymTZsmISEhXhWlzZ8/v1k0JO2tyMhI+eyzz+zaPv74Y6+PE1Mfr7zyivmp9LV77733YtyvYsWKdusHDx6McZ/Dhw9bH2fPnl0eeuihWI0ZSM58JtCdPn166d69uwlZa6j71q1b0qJFC/nggw/MB3t80DtSGjVqJOfOnbP206pVK/OhCAAAAAAAAAAAAAAAAAAA4seCBQvsKi/37dtXUpKPPvpI6tatK0OHDpXnn39eqlatKpcuXfL6OKtWrbKGpj2lVbNtQ9BPPfWUyULGJ+1DK3RbjBkzRrJkyRLjfvXq1TPZTIu1a9dGu/3+/fvtzpvj/kBK4TOBbvXmm29KQECAeaxvyNDQUPnwww+laNGi8vbbb8vevXu9PqZ+HcHcuXOlWbNm0r59e7lw4YL1za7Vud9///14nwcAAAAAAAAAAAAAAAAAAKnZhAkTrI9Lly4tjzzyiKQUhw4dMsVqbZ04ccJkIL119uxZmTVrlldVwQcMGGBd12rWkyZNkvh07do1k9m00Pxlp06dPNq3QIECdq/1zp07Zfv27W63nzx5st16z549YzVmILnzFx+ib+SRI0fK66+/bkLXumglbb374osvvjBLjhw5pE6dOlKqVCkpXry43QeI3hFy+/ZtU9371KlTsm/fPjlw4ICEhYWZbSxVuS0/X331ValWrVoSzhgAAAAAAAAAAAAAAAAAgJRlz5498vfff1vXX3755RRVdXn9+vUSERHhstp2bGhAW/OQjRo1ina72bNnS//+/eX+/ftmPTAwUJYvXy558+aV+PTuu+/K1atXzeN06dLJN99849X+b731lqxZs8auOvvmzZslQ4YMdttp0Ns2jF6mTBlTuBdIiXwq0K00ZP3XX3+Zr1uwhLqVhrDV9evXZeXKlWax0OfOnTsnL730kt2xLPtY2P4HoX79+vL5558n8GwAAAAAAAAAAAAAAAAAAEhdvvvuO+vjTJkyJWjV5dGjR7ts12KwjubPn++yWnTFihXlscce87hPx2xiTO0xCQkJMVWwe/fuLd27d5caNWqY83b37l05f/68/P777ybM/eeff1r3KVq0qAlz69jj044dO+yqZr/22mtSrlw5r47RvHlzE+KfOHGiWddzrvP78ssvpVatWhIaGiqLFi0yx7YU7NXguM7R39/nYq+AR3zyytY3pd5BsnTpUmsI2zaM7epDz90HoeNdPbqdVvhesWIFb3wAAAAAAAAAAAAAAAAAAOLRrVu3ZM6cOdb1zp07S86cOROsvyFDhni8rSVg7KhHjx5eBbobN24sfn5+EhkZadfeokULiS2t+D1lyhSzKK1mbanEbUv71YC8hqOzZ88u8Unzla+88op1XgUKFJDhw4fH6ljjx4+X27dvW68FLfSr2c20adM6VTfX8PpPP/1kwt5ASuUnPkjvtFi8eLGMHDnSvHkdWSp324a1bdtcPa8fNLro1w1s3LhRsmXLlmjzAQAAAAAAAAAAAAAAAAAgNdCCrnfu3LGu9+vXT1IarYr9wQcf2GUUtWL2qFGjPNq/bt26snnzZnn77bdNyFnD244cw9z58uUzYet9+/bJ999/H+9hbjVt2jT5559/rOtjxoyRLFmyxOpYWnBXr4VZs2ZJsWLFrO22YW4Np7dq1Up27dolTzzxRBxHDyRvPl2CWj+s9E6WgQMHyrZt21xW3HZcd1e5u2zZsvLpp5/KU089lYAjBgAAAAAAAAAAAAAAAAAg9dLQsS6JxZIRTGzDhg0zVb03bdpkCsx27NhRsmbN6tG+Wui2fv36ZlFhYWFy5MgROXHihPz333+msrUGnzVMXahQISlfvryUK1cugWckEhoaaq3IrXPS6upxofnObt26mWX37t2yc+dOCQ4OlvTp00vBggWlSZMmEhQUFE+jB5I3nw50q3r16pk7PrSq9tixY+X333+3u3snOnrXin7gDRgwgCA3AAAAAAAAAAAAAAAAAACINzVr1jRLXGnAuVKlSmZJSgkZxK9atapZgNTK5wPdFlqpWxe962T79u3m6wbOnTsn165dk6tXr0pkZKTkypVLAgMDJX/+/OYrCXRx9VUEAAAAAAAAAAAAAAAAAAAAAJAYUkyg2/arBmrXrm0WAAAAAAAAAAAAAAAAAAAAAEjO/JJ6AAAAAAAAAAAAAAAAAAAAAACQWhHoBgAAAAAAAAAAAAAAAAAAAIAk4i8+ZMSIEXL27FlJkyaNfP/990k9HAAAAAAAAAAAAAAAAAAAAABIPYHupUuXyp49ewh0AwAAAAAAAAAAAAAAAAAAAEgR/JJ6AAAAAAAAAAAAAAAAAAAAAACQWvlUhW5bV65ckdy5cyf1MAAAAAAAAACP3Qu9IxeCz8bpGGkkjRTIW0QypM8Ub+MCAAAAAAAAAABA0vHJQHdUVJTkz59fGjVqJB06dJCnn35a8uTJk9TDAgAAAAAAAFwKD38g380ZLn/vWC0RkeFxPp6/fzppVret9O4wVPz80sbLGAEAAAAAAAAAAJA0/MQHpUmTRiIiImTDhg3yyiuvSIECBaR58+YyadIkCQ4OTurhAQAAAAAAAHZmL/lSNm/7NV7C3JaA+OpNC+XnVd/Hy/EAAAAAAAAAAACQdHwy0G1bqVsXS7i7X79+UrBgQXnkkUdMuPvKlStJPUQAAAAAAABAtu5ZnyDH3bZnXYIcFwAAAAAAAAAAAInHZwPdx48fl5EjR0rVqlXtwt3h4eGyfv16E+7Wyt0a7p48eTLhbgAAAAAAACSZq9cvJshxr9y4lCDHBQAAAAAAAAAAQOLx2UB38eLF5e2335adO3fK0aNHTbi7SpUqLsPdffv2NeHuRx991IS7r169mtTDBwAAAAAAAAAAAAAAAAAAAADfDXTbKlmypAl379q1S44cOSIfffSRVKpUySncvW7dOhPuzp8/vwl3T5kyhXA3AAAAAAAAAAAAAAAAAAAAgCSTIgLdtkqVKiXvvvuu7NmzRw4fPiwffvihVKxY0WW4++WXXzbh7hYtWhDuBgAAAAAAAAAAAAAAAAAAAJDofCrQreHs6dOny7Rp0zzavnTp0jJs2DDZu3evHDx4UEaMGCEVKlRwCnf//vvvduHuqVOnyrVr1xJ4NgAAAAAAAAAAAAAAAAAAAABSO58KdLdu3Vp69OhhFm+VLVtW3n//fdm3b58cOHBAhg8fLuXLl3cZ7u7Tp4/ky5dPWrZsKd9//30CzAQAAAAAAAAAAAAAAAAAgNRBi7KOGzdOVq5cmdRDAYBkyacC3fGlXLlyJtC9f/9+E/B+7733JHv27JImTRq7cPeaNWvkpZdeSurhAgAAAAAAAAAAAAAAAADgkxYuXCg1atSQgQMHyuOPPy6vv/56Ug8JAJKdVBnotti+fbvMmjVLZs+eLbdu3TJtGuq2LAAAAAAAAAAAAAAAAAAAIPYGDBhgCqxafP3113Lo0KEkHRMAJDf+kgpD3AsWLJCffvpJzpw5Y9q0IrcjAt0AAAAAAAAAAAAAAAAAAMTelStX5NKlS3Ztmtc7cOCAlCtXLsnGBSBlGzdunNy4ccO6XrVqVWnbtq0kZ6ki0L1t2zbztQ3RhbgJcAMAAAAAAAAAAAAAAAAAEH9y5cpllqtXr9q1ly5dOsnGBCB1BLrP/P+8sOrRoweB7uQa4nYX4LZso/8RadeunXTo0CGRRgwAAAAAAAAAAAAAAAAAQMqhOb2xY8dKr169JCIiwrT17t1bKleunNRDA4BkxT+lhbgXLFhgQtxnz571OsSdO3dua4i7adOmkjZt2kQaOQAAAAAAAAAAAAAAAAAAKU+3bt2kYsWKsn79eilVqpS0bt06qYcEAMmOzwe6t27daq3EHZsQd548eawh7iZNmhDiBgAAAAAAAAAAAAAAAAAgHlWrVs0sAIAUFOiOjxB3+/btrSFuPz+/RBo5AAAAAAAAAAAAAAAAAAAAAPhooPvTTz+VSZMmyblz57wOcQcFBVlD3I0bNybEDQAAAAAAAAAAAAAAAAAAACDJ+VSge8GCBdaK3J6EuPPmzWsNcTdq1IgQNwAAAAAAAAAAAAAAAAAAAIBkxacC3a5C3Bre1jbbEPfTTz9tDXG7C30DAAAAAAAAAAAAAAAAAADfp4ViN23aJP/++6/JDObLl09q1aol5cqV82j/PXv2yLZt2yQ4OFgyZ85s9m/QoIEULFgwwcceEhIiW7ZskWPHjsmNGzckY8aMUqBAAalYsaJUqFBBEsO+fftk9+7dcvHiRQkLC5OsWbNKjhw5pEiRIlK8eHEpVKiQx1nM+/fvm/noa6HnMzQ0VPLkySNBQUFSs2ZNyZ8/v6REkZGRsnXrVtm1a5dcu3ZNsmTJYs5dvXr1JFeuXDHuf/fuXdm8ebMcOnRI7ty5Y/YpVqyYycHqNRHfzp8/Lzt27JBLly5Zr/vcuXOba7527dqSKVMmSQx63ek4Ll++LOnTpzdjqFatmlSqVCnV5X99LtBtCXBbWMLcgYGBMm7cOOnatWuSjg8A4sP+DqnrP0ZAcldx4f9uHAMAAAAAAAAAAAAAANHbsGGDNG3aNNpthg8fLiNGjHBqnzFjhvTq1SvafadPny49e/Y0jw8cOCCDBw+WVatWWYvC2qpcubKMHDlSWrVq5fJYy5Ytk3feecccxxUN044ePdqEw+Nj3j169DBzVBrc/eCDD2Tx4sUmRO1K+fLlpV+/ftK3b1/x8/OLcQwxBWAbN25sxqnCw8Plu+++k7Fjx8rp06ej3e/UqVNStGjRaLdZvXq1jB8/XtatW2fCye5UrVpVOnfuLAMGDDAh4uhon2fOnIl2G0/G6cl1ZXttNWnSRDZu3BjttrbXmx5fr2dXY02XLp0pUPzZZ5/JQw895PS8hrf1Gv3222/l1q1bTs9rsLpPnz7mPaMh+7jQcL32M2fOHBM8dydDhgzmhoZXXnlF2rVr59GxvT1ns2bNko8++kiOHz/ucttChQrJu+++Ky+++KKkTZs2xv5julZmzpxpFnf0BgpXnyGJKeZ3eDLy6aefSseOHc2bWE+c5eTph9D169fNh53ezaAfMOfOnUvq4QIAAAAAAAAAAAAAAAAAgAQwb948U/F55cqVboOYe/fuldatW8uQIUPs2nX7119/XZ566im3YW71xx9/mEziTz/9FK9jnzZtmgk2z58/3xrm1uCvo4MHD0r//v3NGDSsHF+0InPDhg3ltddeizHMHZMTJ07II488Ii1btpQVK1Y4hbn9/f2dKjIPHTpUSpYsKXPnzhVfpq9dp06dTFjcXZj4wYMHZp56c4FW4LalOVethq3ZWFdhbnXv3j1T7FivgQsXLsR6rEuXLpUyZcqYGyAcw9yO155WWf/999+lffv25j2mNx/EFz32M888Y/K+7sLcSiu8640Muq3ukxr4VIXuxx57zCx6gf7666+yYMEC+eWXX+w+AP755x+z6EX38MMPmzsb9AUtXLhwko4dAAAAAAAAAAAAAAAAAICUrkSJEjJq1Ci7NsdAtTsactZKxBY3btyQr776ymk7DQ537dpVIiIiTH9aHTgoKEhu3rwpf/31lwkN29Iq21rxVwPMSqtyW45bp04dqV69umTPnl3Onz8va9eulf/++8+6r1ayfu6556RcuXJSoUIFr+b9ySefmGK1tn744Qd5/vnnzWPNN2oFYs06av8a6t2xY4dMmjTJhL0tNBOp1b+1unZ0lbJtz52lcrRj0DgkJESaNWsm+/fvN+sa8q1fv77kyZNHLl++LH/++accPXpUPPH3339LmzZt5MqVK9Y2PY6G5TUMXKxYMUmfPr05rlbu1orgllCzhpO7dOliwsJaqdkVrdCsr6nS10tDvrY+/PBDU8HaIjAw0OVxtMK67WuzaNEi2bJli3n87LPPmtffsp2yVOm20PPuqvp09+7dTY5VK0hr9XO9PgICAuTs2bOyZs0aE5y3vZb1XO3cudO8hrreokULOXz4sGTJksU8Ll68uERGRsqRI0dMoForalvoedLrZdOmTTFWYnf05Zdfmkyt7Y0PlkC/zjNXrlwmnK7h/OXLl5v3i2Xsej1qmPznn3+OtgK9p+dM37d6/pWGxXXJmTOnXLt2zbx39+3bZ7f9kiVLzPt1zJgx0c7R9lpx9d7TfjR8b3Hx4kW7Y77xxhuS1NJEJXWN8DjScLd+OC9cuNCEvG3D3bYXrb7R9GLWhXA3fJXeDaal/S30P6rR/SMBvmt/B+/+owsgYVVc6NP/XAIApFD8mxHwrX8zduxfNUH6zZolp3z/2foEOTYAIGXg342A7/ybcczUwfLP7rUJ0veI16ZK+VI1E+TYAADfx78ZAd/5d+Of21fKVzOGJki/jzXqLL07JsyxlWMAVYPHI0aMiHE/rR6toWBbn3/+uVk0+Dpx4kS7kKaFhlI7d+5slyfUoK1Wudaq3VpRunz58jJ79mxrmNe2ovIHH3wgI0eOtGt/9NFHZfXq1eINDe7aBqo1RKsBWQ3PaoXxp59+2u2+P/74o3Tr1s2E1i20qvWePXskc+bMHvWvAVvbUK2GjnVMM2fONPPX86djsqUBdq2gbRt21fPmGCTXeTRo0MAudKzrWs08b968LsejcVWtRq3hW8fA/xdffBHtXPT1GDZsmF3b9OnTTZDYG3o+ixQpYkL7GTNmNCFxDTRHR69VvSZsTZkyxYTxNQyv51MD/bb02hs4cKBMnjzZrl0zrBoC1zC4Xn96DD3XWbNmtdtOQ+FazHjbtm127XPmzDE3GHgT5nYMK+u51JC0Oxo21+r1WqHe9v2jYfTSpUt71K+rc6Y3KvTp08dkHvWc1ahRw2m/X375xbx39cYDCw3Ma6C9VKlSEtv3nlYE1xscknMW0098nN5dYbnA9S4O/ZDTOzu0Xd/8lkUv6jfffNN8uOsdNfoGcFfiHgAAAAAAAAAAAAAAAAAAJD9aafn27duybNkyl2Fu1bp1axk3bpxd2507d2Tq1Kny6quvmkCvVox2DHOrdOnSyccff2zCtLa0cndcM4daXVmDvl9//XW0YW5L5WjHUPnx48ejDeLGREOsGqTVSuhaidsxzK38/f3ls88+izY8q4FfzW3ahrm1uvTSpUvdhrktwX4dv74Gjq+pvp7R0arm+trY0kC6tzTsb6nArq9xTGFud95++21z/axatcopzK00dK/j05C7La1OvXjxYhPmfvnll03g2zHMrbRwsY5Vq3fb0mvYU1rZXcP5tgYMGBDjNZQjRw4TrNb3ie37R4PkesNDbGmQXwPhepOBqzC3evLJJ51eVw3h63Wb0vl8oNvxDdCxY0dzh4eWe9c7VNq1a2fuonAV7tYPkNq1a5vy8IS7AQAAAAAAAAAAAAAAAABI3q5cuWKCsK7CyLZ69eolgYGBdm0akD548KCpWhxd8NhSNdqWZg81YBtXGqbWKsWe0MrKjhXKx48fL4cPH471uUufPr3JVmpo1x0NdderV8/t8xoI1qrdtrTCtuP5dueTTz6RoKAgp8C2bUV1R/ny5TNVox0Dy7t37xZvaJVoC09fB1euXr1qwthauTq6APugQYPs2rSyvFZe1/lodjU6eo3qtrY2bNggt27dinF82o9WL7cNYOs517C+JzRI7lg1Xauyz58/X2JLrz8Na8cUon/22WelYMGCdm0aME/pUlSg2zHcrXff6N0MGu6eO3eutG3b1incvX37dnnrrbcIdwMAAAAAAAAAAAAAAAAA4AMcQ7LuQslNmza1a9Mqw0WLFpX27dvHuH+tWrUkZ86cdm179uyRuNIwup+fZ9FNnYNWVHYM6k6YMCHW/WuV5bJly8a4nVbRnj59ully585tbb948aJps1WyZEmPzqmFhqD79+/vFPadMmVKtPv17ds32oB2TE6fPi2rV682jytWrOhUPdsbjRo1cltl2tYjjzxigt2O16GGyaMLg1u0aNHCbl1zr/v27YtxvyVLljgF/7VPzdZ6SgsqO4av9YaC2NL3lON70hU/Pz9p1qyZXZveiBEeHi4pWYoNdNvSi75z587y888/y+XLl2XOnDnmTo3owt0PP/ywKeOvb2AAAAAAAAAAAAAAAAAAAJD0KleubELZnnAVXG7Tpo1H+2oIt3Tp0nZtR48elbjSwrTeaN26tVPbzJkz7Sove6NLly4ebVe9enVT4VkXrdZsG6AODQ2121bzmI6h5dich3HjxkW7j4Z8HV8TzYOGhIR41KdW1NZAfFyrcyvHauHuZM2aVQoUKBDr/V1dw55ch19//bVTmzehe5UuXTqpX7++XdvWrVtj/T7wpv/y5cvbrYeFhcm5c+ckJUsVgW5b+sGi5dgXL15swt2zZ882H9AZMmQwz1vC3VoafujQoebOEQAAAAAAAAAAAAAAAAAAkPSqVavm8bZBQUFx2t8xiHvz5k2Ji/z580vevHm92kczjK7GEZtq4Vr5uE6dOhIXlgrXthyrKXuiUqVKdpW/lRbgPX78eLT7OQaxb9++bULdMdEA/LRp08xjrVLdrVs3SarrUIPSWiHcE67C4DFdh1oB/M8//7RrCwwMlCpVqkhsbqBwtGXLFolthW5PFS5c2Kktru+/5C7VBbodOX51gd4lYrlTxBLuBgAAAAAAAAAAAAAAAAAASc+xQnN0AgIC4rS/Bn8dw8Nx4arasidKlSoVL6HaIkWK2FXb9tbdu3dl27Zt8TYvV/tt2LAh2n20YnjGjBnt2rRqeEyWLl0qly5dMo87d+4s2bNnl6S6DrXCvIa6Y3MNenId/vXXXxIeHu50rr2toq4cQ/dxCXSXKVPG422zZcvm1BbX919y5y8+5OLFi6Zsurv0vSe0tP6yZctk4cKFsmrVKrl//771OdvwdmwuXAAAAAAAAAAAAAAAAAAAkHBcBT3dSZs2bZz29/e3j1hGRERIXOTIkSNW+xUvXlw2btxo13bo0CGvj5MzZ06JiyNHjphK147nWAPKsVGiRAnZvHmzXdv+/fuj3UcrTXfs2FFmzZplbdu1a5ds3bpVHn74Ybf7TZw40fr45ZdflqS8DuNyDXpyHbo6h1q1e/To0eItPa+OtJJ6Ql//mTJlcmqL6/svufOpQPfjjz8ue/fuNWFrx7sHYhPidqy+TYgbAAAAAAAAAAAAAAAAAIDky1XV7cTcPy5iWx07a9asTm3Xr19PtP4trl696vKYfn5+sTqeq2Czqz4caSDbNtBtCWy7C3QfP35c1q1bZx5Xr15datWqJXEVl+sooa9BV+dwz549ZokPsbn2vJ13Whc3Y6R0sXsXJSENYTsGsd2FuOfMmSNt27aVoKAg6datmwl1h4aGWo+hAW7bxfb4ejG0bNlSpkyZkgizAgAAAAAAAAAAAAAAAAAAMYlr4dakLPwa2+CzqyB2bEK1cZ27q6BwXMLJrublSaC7bt26UqVKFbu2+fPny40bN1xuP3nyZGvutE+fPhIf4nIuE/oa9OQcxoW78xwTii6noArdMbl9+7a1Evfq1atdVuJ2dUFYntfS9M2bN5cOHTpIu3bt4vz1AgAAAAAAAAAAAAAAAAAAAHHhqghuUoRjE6NPT/vQKt19+/a1rt+9e9dU7X711VfttgsLC5MZM2ZYK50/99xzktK5Ooddu3aV2bNnJ8l4kEoC3ZYQ94IFC0yIW998ihA3AAAAAAAAAAAAAAAAAABILiIjI2O13507d5zakiLrmCtXLo/G5qmQkBCP+nClS5cuMmTIELtjTJo0ySnQvWjRIgkODrbu46oqeErj6hy6OtdIXnw20P3DDz9YK3ET4gYAAAAAAAAAAAAAAAAAAMlZbEO1t27dSraBbp2TBtX9/PziZV6eBrq12rYGtDXEbXHw4EHZtGmTNGzY0Npm+7xW9U4NXJ1DV+cayYvPBrp79OhhfhLiBgAAAAAAAAAAAAAAAAAAyd2NGzditd/Jkyed2sqVKyeJrUyZMpI+fXprEV4VEREhp0+fluLFi3t9vBMnTji1VapUyeP9+/btaxfYVhMnTrQGug8fPiwbN240j2vXri1VqlSR1MDVOXR1rpG8eH9LRDKhQW1dNMRtWRyfS5s2rbRs2VKmTp0qly5dkt9++0169+5NmBsAAAAAAAAAAAAAAAAAACQqDRjHxtGjR53a6tSpI4ktU6ZMUqtWLad2rYwdX+ejcePGHu+vAW3H87Bo0SK5cuVKqq3OrerVq2eC97bOnj0rt2/fjtXx7t+/L2vXrrUuGuBH/PPZQDchbgAAAAAAAAAAAAAAAAAA4CsuXLhgco3eOHLkiFy8eNGuLUeOHElWbVrzmY7WrVvn9XH27t1rDV5blChRwizecAxqa/h4+vTpEhoaKrNmzbKer06dOklqocF7S5VyC83XxuZ1UmvWrJFHH33Uuhw7dkySuzQ2+WJf4bOBbkWIGwAAAAAAAAAAAAAAAAAA+IqlS5d6tf3y5cud2nr27Cn+/v6SFPr06SMZM2Z0mpNmOb2xZMkSp7bXX3/d6/FoUDswMNCubfLkyTJ//ny5du2aWe/evbsJOacmrs7lwoULY3WsH374wfpYz3XTpk0lucuQIYPdekRERLTb//rrrzJv3jw5c+aMJBWfDHQT4gYAAAAAAPh/7N0HlFXV+Tfgd4YuRUQQULFgQwR714i9967R2LH3GmNsUdTEmsTee0ssUWOJYu8dO2IHRYoiTaTNt/b5r7nfdGaGGe7cmedZ66w5Z99zzt773MJl5nffCwAAAAAAFJprr7221uHnGTNmxD//+c9ybcXFxZWqUs9LCy20UJbVLOvLL7+M++67r9bnmDJlSqV59ejRo9J5ayOFy/fbb79ybSNGjIiTTz65XAi9pdlmm21i4MCB5drSfTR8+PA6neeTTz6Jf/3rX7nto446Km8fJqiLLl26lNueOnVqjfufcsopsddee8U777wT+VKQge5UCl2IGwAAAAAAAAAAACgkKTCaKkjXxsUXX1ypYvAxxxwTyy23XOTTkCFDom/fvuXaTjvttBg3blytjj/99NNj7Nix5TKhN954Y8w333z1Gk9Vge3S8//ud7+L/v37R0uTruktt9xSrlJ1+oDAIYccEr/99lutzvHrr79m1c1Lq1t37969XlXU82GppZYqtz1q1KhaHbfIIotEvjT9mHw1hLgBgKbkpwlj4sPhb8TEST/P1Xnatm0X/ZZaNfr0Xip7cw0AAAAAAAAANA8bbLBBvP3223H00UdHz549Y8cdd6x237vuuivOOOOMcm1LL710nH/++ZFv888/f1btef31149p06ZlbV9//XVsv/328cADD0SvXr2qPC5VJr/wwgvj73//e7n2k046Kbbbbrt6jycF3DfeeOMYOnRopdvyWc0831ZdddW44ooryl2DF154IXbbbbe49dZba8zhpqLLqWL1W2+9VS4gXijZ3XXWWSfuueee3PawYcOyyvAdO3as9pj0gYKVV1458qVgA90AAE3FG+8PjctuPCVmzZ7ZYOfceqPfx347nyTUDQAAAAAAAEBB+e677+Lee++tcZ9XXnklqz5dqk+fPrHHHnvExIkTy1Wv/vnnykXVnnjiiXKVoLfaaqtYYYUVquy7NIxa1vXXX18ulJrCxGXP/eGHH+a2P/roo3LHpvGVHXcKNqeKx7W15JJLxuDBg2OfffaJnXbaKZvzwQcfHGuuuWZ06dIlJk2alAW+r7nmmkrXcLHFFov//e9/NVaxTseka1Cq7HrpdtnxV3UNamu11VbLAtQpxF16f7z66qsxcODArIrzrrvums23TZs2WbXsZ599Nq688sp48cUXy53nT3/6U5x33nkxt1JouWKgO1WU3mWXXep8rvT4TEvZ7YoqXseaHkdzuh8GDBgQW265ZbXnrmp8ZfdZd911s6W66uXFxcVxxBFHxMyZ/5dreeSRR6Jfv35x1FFHZR8qSNWs0+MqVe7+4IMP4uGHH46rrroqfvrpp2z/dPw//vGP2GabbRrtmk2s8Nz/4osvqnx8l31Op+dPeu2oyp577hmnnHJKrhp5qjaenqtpDAsvvHAW7q5o//33j7Zt20a+FJWkjzwUiLPOOiu+/fbbbP3mm2/O93BgnktvENKLd6n0ol/6ZoTm5cPdBDihKRlwf/Vvl36b/msceMqgmDFzeoP3+6cjr4qVlq/6DTcAeM8IhfOeMdn9qMap6NC50wJx44XPNsq5AWgevG+EwnnPeMkNJ8Xr7z3dKH2ffewN0X+Z1Rvl3AAUPu8ZoXDeN7781hNxxS2nNUq/W26wZxy4e8Oc+7nnnouNNtqoTscMGjQoOy5VeU4h4LpIWcIUxKxv32UjlOk8qXJxbS2++OLZmKuzxBJLxDfffJPb3m+//bIqxzfddFMcfvjhMX36/88apODzjBkzqjxPCnzffffd0bdv3xrHs+GGG8bzzz8fdTU3MdIUvE2h4WeeeabSbamIXatWrXJB4rJ69+6dhWv33nvvaAjp2qXQ++jRo8sFhv/2t7/V+Vxnn312nHPOOfPscVT6uChV1+J/KVubxlyTdP8ceeSR8dlnn1V5e7t27XLh57IWWWSRuPrqq+dYQX1ur9nX9Xjupw8JpMd8dVJ18vThgtrMNT1X33333ejatWvkS3EUkHRnpxdfYW4AoKn47Mv3GyXMnXzw2RuNcl4AAAAAAAAAIH8OPPDAeO+992K33XbLgtxJVWHu5ZdfPquMnCpfzynMnS+puvPTTz8dTz75ZGy77bblKoinwG7FMPdKK60UF1xwQYwYMaLBwtxJuo4HHXRQuVB0CprzfzbZZJOsgGyqgp2qeaegfVkVA879+/fPAveffPLJHMPcTdWxxx4bt99+e6WgeFXB9TvvvDOvYe6Cq9ANLZ0K3S2HT0BD4XwC+rV3n45Lb6z7Vw/Vxqbr7RqD9zqjUc4NQOHznhGaFhW6AWiqvG+EpkOFbgCaKu8ZoWlpDhW6mXOF7rImT56cBbY///zzmDBhQlY9eOGFF46BAweWy4oVihSWfe211+K7776LsWPHZtvdu3ePnj17xmqrrZbNjfz7+eefs/spVTRP99Ps2bOzQPOiiy4aq6++evTq1Suai9mzZ8ewYcOyCtzjx4/PHpNTpkzJPljQlLKYrfPaOwAAAAAAAAAAAEAL1alTp9hss82ypTlIgfRBgwblexjMwQILLBBbbbVVtATFxcWx8sorZ0vZ4rplA91NQbMNdP/0008xatSoLE2flpSwX3DBBaNbt27ZJwea06cHAAAAAAAAAAAAAIDC1KwC3f/5z3/iiSeeiOeffz4+/fTTGvddfPHFY4MNNohNN900dt9992jbtu08GycAAAAAAAAAAAAAQLMIdM+YMSNuvvnmuPjii+OLL77I2kpKSuZ43Ndffx3ffPNN3H777XHyySfHMcccE0ceeWR06dJlHowaAAAAAAAAAAAAACCiOApYCnCvs846cfjhh8eIESOyIHdpmLuoqGiOS+n+P/74Y5xxxhmx4oorxquvvprvaQEAAAAAAAAAAAAALUTBBroffPDBWG211eLdd9/NQtkVw9q1UTHc/e2338agQYPi0ksvbfTxAwAAAAAAAAAAAAC0jgL0wAMPxJ577hkzZ87MtmsKcJdW7C5V3b6l7emcJ598ckybNi1OP/30Bh03AAAAAAAAAAAAAEBBB7o//PDD+MMf/pAFryuGs8uGtzt06BC9e/eOjh07ZktxcXFMnjw5pkyZEmPGjIlJkyaVO7b0XKXVuv/85z9H//79Y8cdd5xHMwMAAAAAAAAAAAAK0XfffRf33ntvubaJEyeW2/7oo4/i4osvzm336dMn9thjj3k2RqDpKqhAdwpaH3DAATF16tRcALs0xN23b9/YbrvtYosttsiC2IsttliN5xo9enR8+umnMXTo0HjkkUfi/fffz9rTeUtD3YceemgMGjQoFlhggXkwOwAAAAAAAAAAAKAQffHFF3HyySfXuM9bb72VLaVSPlGgG0iKC+ky3H333fH222+XC3OvvPLK8Z///CdGjBgRl112WWy55ZZzDHMnvXr1ig033DDOPffcePfdd+O1116LzTffvFyV73HjxsVFF13UqHMCAAAAAAAAAAAAAFquggp0p8B2kkLXaTniiCPizTffjG233Xauz73mmmvGE088EVdccUW0atUqV6X7mmuuiWnTpjXA6AEAAAAAAAAAAIDmKBWYLc021nZ57rnn8j1soIkoLqSvIyitzp2WFOb+5z//mYWvG9LRRx8dV155Za5S96RJk+Kxxx5r0D4AAAAAAAAAAAAAAAoq0D106NDc+pJLLhmXXHJJo/V1yCGHxBZbbJHbfuaZZxqtLwAAAAAAAAAAAACg5SqYQPe7776bWz/ggAOiXbt2jdrfkUcemVt/7733GrUvAAAAAAAAAAAAAKBlKphA99dff51b33bbbRu9vy233DJat26drX/11VeN3h8AAAAAAAAAAAAA0PIUTKB7woQJufU+ffo0en8pzN2rV68oKSmJX375pdH7AwAAAAAAAAAAAABanoIJdM+YMSO33qlTp3nSZ8eOHSv1DQAAAAAAAAAAAADQ4gLdpeHq5Pvvv58nff7www/Zz/nmm2+e9AcAAAAAAAAAAAAAtCwFE+ju3bt3bn3YsGGN3t/XX38dEydOjKKionJ9AwAAAAAAAAAAAAC0uED3csstl1u/6667Gr2/O++8M7fer1+/Ru8PAAAAAAAAAAAAAGh5CibQve6662Y/S0pK4oEHHohXX3210foaOXJkXHLJJZX6BgAAAAAAAAAAAABokYHuDTfcMLp06RJFRUUxc+bM+P3vfx9ff/11g/czYcKE2H333bOfpXbccccG7wcAAAAAAAAAAAAAoGAC3W3bto299947q9CdQt0pzL3GGmvEo48+2mB9vPHGG7H22mvH66+/nvWRlvXWWy+WXXbZBusDAAAAAAAAAAAAAKDgAt3JqaeeGu3atcvWU9h6/PjxscMOO8S6664b9957b/zyyy91Pue0adOyUPj2228f66yzTgwfPjxrT8Hx5Mwzz2zgWQAAAAAAAAAAAAAA/J/WUUAWX3zxOP300+Oss87KVdBOwetUUTtV727dunWsvvrq0b9//1huueVi4YUXjo4dO0anTp2yfadMmZItP/74Y3z22Wfx6aefZsemUHdSWv279Ofuu+8em266ab6nDQAAAAAAAAAAAAA0UwUV6E7+9Kc/xQsvvBDPPPNMuVB3MmPGjHjttdeypbZKj03SuUotu+yycd111zXw6AEAAAAAAAAAAAAA/r/iKDDFxcXx4IMPxlprrZULY5cGu0vD3XVZyh6bpLYll1wynnrqqejcuXOeZwsAAAAAAAAAAAAANGcFF+hOOnXqFM8991wccsgh5SpsJ2UD2rVZykrn2nrrrePNN9+MPn36zONZAQAAAAAAAAAAAAAtTUEGupN27drFtddeG/fcc08sssgilYLddZGO7dq1a1xyySXx6KOPRrdu3Rp0rAAAAAAAAAAAAAAAzSrQXWr33XePL7/8Mm666aYYOHBgFs6uuJSq6rYlllgiLr300vj222/j+OOPz+tcAAAAAAAAAAAAAICWpXU0A61bt479998/W0aPHh3PP/98vPTSSzFy5MgYP358tsyePTsWXHDBrPp27969Y5111olBgwbFkksume/hAwAAAAAAAAAAAAAtVLMIdJfVq1ev2GOPPbIFAAAAAAAAAAAAAKApK873AAAAAAAAAAAAAAAAWiqBbgAAAAAAAAAAAACAPBHoBgAAAAAAAAAAAADIk9b56ripu+6662L06NHZ+plnnpnv4QAAAAAAAAAAAAAAzZBAdzWuvvrqGDZsWLYu0A0AAAAAAAAAAAAANIbiRjlrM1FSUpLvIQAAAAAAAAAAAAAAzZhANwAAAAAAAAAAAABAngh0AwAAAAAAAAAAAADkSet8dHruuedGUzd69Oh8DwEAAAAAAAAAAAAAaObyEug+++yzo6ioKB9dAwAAAAAAAAAAAAC07EB3qZKSknx2XyOBcwAAAAAAAAAAAACgWQe6haYBAAAAAAAAAAAAgJZMhe5qCJsDAAAAAAAAAAAAAM060N2pU6dYbbXVoil66623YsqUKfkeBgAAAAAAAAAAAADQjOU10L300kvHs88+G03RKqusEu+//36+hwEAAAAAAAAAAAAANGPF+R4AAAAAAAAAAAAAAEBLJdANAAAAAAAAAAAAAJAnAt0AAAAAAAAAAAAAAHnSOh+dbrDBBlFUVBRLL710NFWrr756dO3aNd/DAAAAAAAAAAAAAACasbwEup977rlo6q6//vp8DwEAAAAAAAAAAAAAaOaK8z0AAAAAAAAAAAAAAICWSqAbAAAAAAAAAAAAACBPBLoBAAAAAAAAAAAAAPJEoBsAAAAAAAAAAAAAIE9az+sOZ8yYEX/9619j1qxZ0bt37zjkkEOiEJWUlMTll18ekyZNio4dO8aJJ56Y7yEBAAAAAAAAAAAAAAVmnge627RpEyNGjIjbbrst207B7sMOOywKzZ///OcYMmRIFBUVxemnn57v4QAAAAAAAAAAAAAABag4H52ee+650a5du6zK9XHHHRfPPPNMFJIURk9h7qRHjx5x6qmn5ntIAAAAAAAAAAAAAEABykugu0+fPlmQO5k+fXpss802cffdd0chuOSSS+LAAw/M1lN17rPOOis6deqU72EBAAAAAAAAAAAAAAUoL4Hu5Jxzzok111wzC0WnUPe+++6bVbqeNm1aNEUTJkyIAw44IE455ZSYPXt2Nu7tt98+Dj/88HwPDQAAAAAAAAAAAAAoUHkLdLdp0yb+/e9/R48ePbJwdApJX3zxxbHCCivEo48+Gk1FGtftt98e/fr1i9tuuy1KSkqy8S6zzDLZNgAAAAAAAAAAAABAwQW6k0UWWSTuu+++6NChQxaSTmHpr776KnbYYYfo379/XHPNNTFp0qS8jG3s2LFx0UUXxZJLLhn7779/jBkzJhtf0q1bt3jwwQejc+fOeRkbAAAAAAAAAAAAANA8tM73ADbYYIN4+umnY7vttouffvopa0vB6U8//TSOPPLIOO6442LQoEGx8cYbx7rrrhurrLJKdOrUqcHH8fPPP8c777wTL7/8cjzzzDPxyiuvZNW5S0PcpYHzxRZbLJ588slYbrnlGnwMAAAAAAAAAAAAAEDLkvdAd7L22mvHiy++GFtuuWV8++23WXg6SQHq6dOnZ4HvtJRaeOGFY+mll87C1anKd/fu3bOq2SnoPd9880W7du2iVatWUVxcnIWyZ82aFdOmTYupU6dmFb9TcDxV4B41alR88803MWLEiKwCd1llg9yl2yuuuGI8/vjj0bt373l6fQAAAAAAAAAAAACA5qlJBLqTfv36xXvvvRcnnnhi3HLLLZXC1GWlIPb333/fYH1XPH/FvlM4PFUKP/fcc6NDhw4N1i8AAAAAAAAAAAAA0LIVRxPStWvXuPHGG+OZZ57JKnCnMHVaUri64lJ6W0MsNZ1/tdVWizfffDP+9re/CXMDAAAAAAAAAAAAAM030F1qww03jI8//jjuvvvuWGeddXLh6rKqCmHXdymrtK/NNtssHn300SzMvfLKK8/jKwAAAAAAAAAAAAAAtARNMtCdtGrVKvbYY494+eWX44033ohDDz00Fl988UrVtedGxXMtu+yycdxxx8VHH30UTz75ZGy99dYNNh8AAAAAAAAAAAAAgIpaRwFYffXVsyX5/PPPs7D1Sy+9FJ999lm2PXXq1Dqfs3PnzlmAu1+/fjFo0KDYYostok+fPo0wegAAAAAAAAAAAACAAg50l7XMMstky1FHHZVrGzlyZIwYMSJ++umnmDJlSkyePDn7OW3atOjQoUN07NgxOnXqlC0LLrhgdnyvXr3yOg8AAAAAAAAAAAAAgIILdFdl0UUXzRYAAAAAAAAAAAAAgEJSnO8BAAAAAAAAAAAAAAC0VALdAAAAAAAAAAAAAAB5ItANAAAAAAAAAAAAAJAnAt0AAAAAAAAAAAAAAHki0A0AAAAAAAAAAAAAkCcC3QAAAAAAAAAAAAAAeSLQDQAAAAAAAAAAAACQJwLdAAAAAAAAAAAAAAB5ItANAAAAAAAAAAAAAJAnAt0AAAAAAAAAAAAAAHki0A0AAAAAAAAAAAAAkCcC3QAAAAAAAAAAAAAAeSLQDQAAAAAAAAAAAACQJwLdAAAAAAAAAAAAAAB5ItANAAAAAAAAAAAAAJAnAt0AAAAAAAAAAAAAAHki0A0AAAAAAAAAAAAAkCcC3QAAAAAAAAAAAAAAeSLQDQAAAAAAAAAAAACQJwLdAAAAAAAAAAAAAAB5ItANAAAAAAAAAAAAAJAnraNATJ48OR544IFqb994441j0UUXnadjAgAAAAAAAAAAAABoEYHuRx55JPbff/8oKiqq8vYHH3xQoBsAAAAAAAAAAAAAKCgFE+j+73//m/0sKSmpdFt1IW8AAAAAAAAAAAAAgKasYALdL730UqXgdgp3t2/fPvr16xfdu3fP29gAAAAAAAAAAAAAAJptoHvKlCnxzTff5ALdKcjdpk2bGDJkSBx55JFZqBsAAAAAAAAAAAAAoNAURKD7hx9+yK2nMHcKdp977rlx4okn5nVcAAAAAAAAAAAAAABzozgKwPTp08ttt27dOo466qi8jQcAAAAAAAAAAAAAoMUEurt06VJue4klloiOHTvmbTwAAAAAAAAAAAAAAC0m0N27d+9o3759bru4uCCGDQAAAAAAAAAAAABQo4JIRrdq1SpWXXXVKCkpybZHjx6d7yEBAAAAAAAAAAAAALSMQHeyyy675NYnTpwYn332WaP2t+2220bfvn1jqaWWatR+AAAAAAAAAAAAAICWq2AC3QcddFAsuOCCUVRUlG3fc889jdrfqFGj4uuvv84WAAAAAAAAAAAAAIAWHeju0qVLXHzxxVFSUpJtp/WRI0fme1gAAAAAAAAAAAAAAM0/0J3st99+cfzxx2eh7qlTp8ZWW20VY8aMyfewAAAAAAAAAAAAAADqpXUUmEsuuSR69OgRZ555Znz00Uex2mqrxWWXXRa77rprvofWJHz55Zfx+uuvZ8t7770XY8eOjZ9++ilb2rZtGwsssEC29O7dO9ZYY41YZ511Yu21145u3bpFc5IeGw8//HC89tpr8fHHH2fXYcqUKTHffPNlc+3Xr182/+222y77WVRUlO8hAwAAAAAAAAAAANACFVygOznttNNi0003jeOOOy5eeeWV2GOPPWKZZZaJ3//+9/G73/0uVl555ejatWu0FCm4fc8998S9994bX3/9dbX7TZ8+PSZPnhzfffddDBs2LJ588smsvU2bNrHTTjvFUUcdlV2/QvbYY4/FkCFDssdFVSZNmpQt33zzTTb/8847L1ZYYYU49dRTY5999hHsBgAAAAAAAAAAAGCeKqhAd9++fSu1pQBuSUlJDB8+PM4+++xce7t27aJLly5ZReb6+P7776MpS+Hs22+/PS699NKsAvXcmDFjRtx3333Zsv7668ctt9wSSy21VBSS8ePHxyGHHBIPPvhgvap5/+EPf4jrr78+br311lhyySUbZYwAAAAAAAAAAAAAUNCB7lR9ujTAnZRWUy79WdqeTJs2LVvqq6lWak5B7iuvvDIuueSSGDVqVLX7LbroorHhhhvGGmusEQsttFDMP//8WWXqMWPGxGuvvRZPPfVUjB07ttJxL730Uqy00kpZUHzw4MFRCD755JPYdttt48svv6x028ILL5xVcF999dVjgQUWiIkTJ8b7778f999/f4wYMaLcvi+++GKsueaaWSg8BdsBAAAAAAAAAAAAoLEVVKC7VFUB7tTWVEPYDenbb7+NE044odrbl19++TjvvPNixx13jOLi4ir3Oeqoo7Kq3KkidapqXjHYPWXKlDj00EPju+++i7/85S/RlKVQ9sYbbxyjR48u1966dets7CeeeGK0adOm3G0p4H3++efHjTfeGMcdd1w231Ljxo2LLbfcMp555plYa6215tk8AAAAAAAAAAAAAGiZqk78FojSEHdLCHLXxt577x1vvfVW7LzzztWGuUulkPMRRxyR7d+/f/8q90nB8Ouuuy6aqlRxfKuttqoyzP3www/HaaedVinMXSo9Zg4++OB49tlno1OnTuVuSwHvbbbZJkaOHNmo4wcAAAAAAAAAAACAggx0p8rc82IpJFtssUXcdtttMd9889XpuMUWWywLNffs2bPK21MF688//zyaosMOOyyr0F3RpZdeGltvvXWtzrHGGmtk162i8ePHZwH52bNnN8hYAQAAAAAAAAAAAKAqraMApRDyOeec06h9nHnmmfHdd99FIejYsWPccsst0apVq3odv9BCC8VNN92UVaWu6Ndff41TTjklHnzwwWhKnnrqqbjrrrsqta+33npx1FFH1elcO+20U+y2225x//33l2t/8cUX48Ybb4xDDjlkrscLAAAAAAAAAAAAAM0m0N2tW7fYb7/9GrWPyy+/vGAC3SnA3KtXr7k6R6poveqqq8Y777xT6baHHnooq9K9zDLLRFOQqmanyuFVGTJkSBQVFdX5nOedd178+9//rlSR+/TTT4+99torOnXqVO/xAgAAAAAAAAAAAEB1iqu9hYKxzz77NMh5Dj/88Gpvu+eee6KpeOCBB+KTTz6p1L7SSivFBhtsUK9zLrvssrHFFltUah83blxce+219TonAAAAAAAAAAAAAMyJQHeBW2KJJWLAgAENcq5BgwZVe9vQoUOjqbj00kurbJ/bqu37779/tdXaK1buBgAAAAAAAAAAAIAWGeguKSlpVv3MrRVXXLHBzrXMMsvEAgssUOVt77//fjQFw4cPj1dffbXK27bffvu5OvdWW20Vbdu2rdQ+cuTIeOaZZ+bq3AAAAAAAAAAAAABQldZRQC677LLsZ48ePRq9r8cffzymT58eTU3Hjh1jm222abAQc0ULLbRQ/Pzzz5XaU9ukSZOic+fOkU933nlnle2LL754LLXUUnN17jS3NdZYI15++eVKt911112x2WabzdX5AQAAAAAAAAAAAKCgA93HHnvsPOurd+/e0RSlcT366KONdv7qKnQnTSHQ/d///rfK9vXXX79Bzp/OU1WgOwX8U9X2oqKiBukHAAAAAAAAAAAAAJJil4GyZs+eXe1t7dq1i3z66aef4p133qnytpVXXrlB+lhllVWqbP/xxx9j2LBhDdIHAAAAAAAAAAAAAJQS6KacX375pcr21q1bR7du3SKfXn/99WoD5wMGDGiQPgYOHFjtba+++mqD9AEAAAAAAAAAAAAApQS6KeeHH36osn3FFVeMoqKiyKe333672tuWWmqpBumjb9++1c6zpv4BAAAAAAAAAAAAoD5aRzOUqji///77MXLkyBg/fny2pLYFF1wwqzLdq1evWHXVVaNt27b5HmqT8s0338TEiROrvG3QoEGRb8OGDauyPQWwF1tssQbpo3379tGzZ88YPXp0rfsHAAAAAAAAAAAAgGjpge7JkyfHTTfdFE888US8/PLL2XZN2rVrF2uuuWZsuummMXjw4FhooYWipaupAvUee+wR+fbFF19U2Z6C+un+bCi9e/euMtD95ZdfNlgfAAAAAAAAAAAAAJAUF/plmDBhQpx++ulZhebjjz8+nnzyyZg0aVKUlJTUuEybNi1efPHFOOuss2KJJZaIQw89NKtQ3ZI9+uijVbb3798/1lprrci3r776qsr2Hj16NGg/1Z1v3Lhx1VYwBwAAAAAAAAAAAIAWF+h+9dVXY6WVVoqLLrooC3aXhrWLiopqtZQNd99www3Zue6///5oiWbMmFFtoPu0006LfPvtt9/i559/rvK2bt26NWhf3bt3r/a2H3/8sUH7AgAAAAAAAAAAAKBlK9hA9z/+8Y/YcMMN47vvvqsU4q6tiuHuVH15zz33jGOOOSZamhRkHzt2bKX2fv36xd577x35Nn78+Gpv69SpU4P21bFjx3qNAwAAAAAAAAAAAADqqnUUoCuvvDKOPfbYbL2mAHcKaVen4nGl2+mYdP5Usfrqq6+OluLyyy+vsv2KK66IVq1aRb799NNP8yzQXdP5ahpHXY0ZM6bKEH1NRowYUW57+vTpWfXyOUmP77Zt21ZqT8fX9DypKD0WWrcu/7Ixe/bs7PlSF23atIni4vKfJ5k5c2bMmjWr1udoSXOa1bZTlBTV/nlYPHNaFM8q/7goKSqOWW07R120mj4pikpml2ub3apdzG7dvtbnKCqZFa2mT67Ubk7mVMhzqvgaMXNm3V4v6mL27Fm51/mW9LpnTuZUE3OqmjlVr6XMqTn+m2tO/8ecCmdOFdX1NaLeSkrK/W6gpbzumdP/MaeqmVP1zKl6LWVOzeHf3Ob4PsKcWs6cqlL2NSL9PrCxTJ8xo9z7xpbyumdO5lQTc6qeOVWtJc2p0P/NbY7vI8ypZc2porKvETMa8e/T6fWj7HvGlvS6Z07mVBNzap5zmj59ejQ1BRfofvHFF+O4446rFMgu+wBId0zfvn1jkUUWyaotpyXdUZMnT44pU6ZkQdrPP/88pk37//8Alp6vtFr3ddddFyuvvHIceuih0dzde++98eabb1Zq//3vfx+bb755NAW//vprtbdV9aIwN2o6X03jqKurrroqzjnnnLk6x8iRI6NDhw61mlN6TlR1fF1emLp3754tZaUXzq+++irqYskll4x27dqVa5swYUKMGzeu1udoSXOavMTvYlb7+Wt9ng4/fpgtZaX/rExcbquoiy6fPR6tf/ulXNtv3ZeJX3sOqPU5Wk37JeYf/nildnMyp0KeU8XXiB9/HBONZeLESbm+WtLrnjmZU03MqWrmVL2WMqfm+G+uOf0fcyqcOVVU19eI+kq/iCz7mtZSXvfM6f+YU9XMqXrmVL2WMqfm8G9uc3wfYU4tZ05VKfsakf6O2Vh++OGHaF/8VYt73TMnc6qJOVXPnKrWkuZU6P/mNsf3EebUsuZUUdnXiLFj6lY8si4mTpxY7rWoJb3umZM51cScmuecRo4cGU1NQQW605188MEHZ3+oKltRO61vvPHGsd1228UWW2wRyyyzzByrSqfjvvnmmxg6dGg88sgj8cQTT2SfMErnKg11n3zyybHNNtvEoosuGs1VCiifeuqpldpTGL66qt35UNMTvKEriFf8tEdtxwEAAAAAAAAAAAAAdVW+vngTd+ONN2aVtcuGubfddtt499134+mnn45jjz02+vXrV6uAbzrHEkssEQceeGA8+OCD2XkHDx5crvJ3+hT8+eefH81ZCq2nYHtZ6frdddddlT71kE+p9P28CnTXdL66lvEHAAAAAAAAAAAAgJoUlaRUdIFYfvnlY/jw4VmQO4VuL7jggjjppJMatI///Oc/sc8++2Rh7tRP+/btY/To0dGlS5doblJV8q22qvwVI0OGDIk//vGP0ZS8/PLLsf7661d527777hu33XZbg/V1xhlnVBvkT0H3vfbaq0H6GTNmTIwdW7evQRkxYkTsuOOOue133nkn+vfvP8fj0gcV0lccVFVxvC4vAel5V7GC+ezZs+scdG/Tpk0UFxdXCu2n6vu11dzn9OFu///DJbPadoqSotp/cKF45rQonvVbubaSouLsa4XqotX0SVFUMrtc2+xW7WJ26/a1PkdRyaxoNX1ypXZzMqdCm9OA+0uqfY14c9jQ+PutjfPv5sbr7BQH7Hpai3jdK8uczKkm5lQ1c2qZcyr7nrG5/Jtbljn9H3MqnDkNvG9Wja8R+564VjSGzh27xlXnPtkiXvfKMqf/Y05VM6fqmVPLnFPZ943N4d/c5vg+wpxazpxWunNSja8Rf7/1tHhz2LPRGE4/4upYfqlVW8TrXlnmZE41MafqmVPLm1PZ94zN4d/c5vg+wpxa1pxWvGdata8Rr777VFx1x5+jMWy23m7xh51PahGvexWZkznVxJya55w+/vjjWHXV////5A8//DBWWGGFyKfyV6EJSxfrs88+yx4EaTnzzDMbPMydbL/99nHHHXfkQrO//fZbPPzww1louDn5+uuvq5xTCrM3tTB3UtUTvzbVu+ujpvPVNI66WmihhbJlbqTxtGvXbq6On1vpRXBuxlAqvShXfGGuj+Y4p6re8NdV+o9H699+mevzpP8IVfzPUH2YU9XMqTDmVPE1onXrNnN9zur7ajXH16Pm+LpnTlUzp+qZU/XMqeXMqTn+m2tO1TOnwphTQ71GzFFR0Rxf05rj6545Vc+cqmdOVTOnljWn5vhvrjlVz5wKY05lXyPS7wMbS9s2beb4OtIcX/fMqXrmVDVzqp45taw5Ncd/c82pauZUGHMq+xrRphH/Pp1ClbV5LWqOr3vmVDVzqp45Ff6c2jZgFrShlI+jN2HPPfdcbn3AgAFZFeXGkkLde+65Z277+eefj+Zk8uTJ2RzHjRtXrn299daLG264IZqiDh06VHtb+kRHQ6rpkx2pYjsAAAAAAAAAAAAAtLhA97Bhw3LrBxxwQFaluzENHjw4t/7+++9Hc5GqT++1117xwQcflGtPIfn//Oc/DfKphsawwAIL1BhQb0iTJlX+urtS3bp1a9C+AAAAAAAAAAAAAGjZCibQ/c033+TWN99880bvb9CgQdGmTZsoKSkp13chS3NJYfhHH320XPtSSy0V//vf/5p0WLl79+7zLNA9ZcqUam9bcMEFG7QvAAAAAAAAAAAAAFq2ggl0T5w4Mbe+yCKLNHp/qQL4wgsvXKnvQnb00UfHHXfcUa5t8cUXj6effjp69eoVTVmqHN61a9cqb/vpp58atK/x48dXe1tTv04AAAAAAAAAAAAAFJaCCXTPmjWrXLh3Xmjbtm32c/bs2VHoTjrppLjyyivLtS255JLxwgsvxBJLLBGFII23KmPHjm3QfsaMGVNtde4uXbo0aF8AAAAAAAAAAAAAtGwFE+ju1KlTbn3UqFHzpM/vv/++Ut+F6LTTTotLLrmkXNtiiy0Wzz77bPazUCy11FLVVtSePn16g/Xzww8/1Kl/AAAAAAAAAAAAAGj2ge5FFlkkt/7mm282en8ff/xxTJkyJYqKisr1XWjOOOOMuOiii8q1LbroolmYe/HFF49CsuKKK1bZXlJSEt98802D9DFt2rT48ccfq7xt4MCBDdIHAAAAAAAAAAAAABRcoHv55ZfPrd9+++2N3t9tt92WW+/fv38UorPOOivOP//8cm29e/eOoUOHRt++faPQrLbaatXe9sUXXzRIH19++WUWEK/K6quv3iB9AAAAAAAAAAAAAEDBBbrXX3/97GcK2z7xxBPx6KOPNmp17r///e+V+i4k5513Xpx77rnl2nr27JmFuZdZZpkoRGuvvXYUF1f9kP3www8bpI8PPvigxv4BAAAAAAAAAAAAoMUGunv06BFFRUVZqPvAAw+Mt956q8H7+eabb2LnnXeOadOmZdupv5122ikKyYUXXhh//vOfy7Wla/fMM89Ev379olB169YtVl111Spve++99xqkj+rOk8LwK620UoP0AQAAAAAAAAAAAAAFF+hOlZkPOOCALMydQtbjxo2LDTbYIP75z3/GzJkzG6SP+++/P9Zcc834/PPPs+3Uz1ZbbRWLLrpoFIqLL744/vjHP1YKQj/99NOxwgor1Pl8I0eOzC1jx46NfNt6662rbH/55Zcb5PwvvfRSle1bbrll9ngAAAAAAAAAAAAAgBYZ6E5OPPHE6NKlS7aewrWpivaxxx4bSy+9dFx00UXx0Ucf1fmcX3/9dRYKT9WX99xzz3Kh5dTHWWedFYXi8ssvj5NPPrlcW9euXeN///tfrLjiivU6Z58+fXLLbrvtFvm29957V3s/fvnll3N17kmTJsUbb7xRp34BAAAAAAAAAAAAYG60jgLSo0ePLLh9+OGHZ2HrtKSK3d9++22cfvrp2dKrV6/o379/LLfccrHwwgtHx44do1OnTtm+U6ZMyZYff/wxPvvss/j000/jm2++yc6dzpOUnjP9PPLII2P11VePQvCPf/wjjj/++HJtKfz+1FNPxaqrrhrNRbpf11577Xjttdcq3fbwww9XugZ18fjjj8f06dMrtacK7Ztuumm9zwsAAAAAAAAAAAAAzSLQnRx66KHx0ksvxZ133pkLdZcNZP/www8xevToGDp06BzPVXpMqdJzJSk0fPHFF0chuPrqq+OYY44p15ZC7E888USsscYa0dyccMIJsfvuu1dqv+222+Yq0H3rrbdW2X7cccdFcXFBFbMHAAAAAAAAAAAAoEAUZEr15ptvjh133LFcILs03F1aYbs2S9ljygbDUwj6scceizZt2kRTd/3112eVxMtKVcn/+9//xjrrrBNNwSOPPJIF5Dt06JBVWT/ggAOy4H197bLLLtGvX79K7e+991688MIL9Trn8OHDswB8Rd27d4/BgwfX65wAAAAAAAAAAAAA0CwD3a1bt44HHnggzj///GjVqlWl2ysGtatbyioNeR9++OHx4osvRteuXaOpu+mmm7KK5WWD7Sk0nQLUv/vd76IpuO6662L77beP119/PaZNmxbjxo2LW265JdZaa634/vvv63XOVC378ssvr/K2P/3pT5Uqr9fGn//855g9e3al9iFDhkTnzp3rNU4AAAAAAAAAAAAAaJaB7lJ//OMf47nnnovVV189F8iuq9Ljll122fj3v/8dV155ZbRt2zaauttuuy0OOeSQcnNu3759PPzww7HRRhtFUzBmzJg47rjjqrztu+++ixNOOKHe595iiy1i7733rtT+0ksvZfdhXaRrdt9991VqT6H4gw46qN5jBAAAAAAAAAAAAIBmHehO1l133az689ChQ2O77baL+eabLxfSntPSpk2bLPycqn1/8sknsdNOO0UhuPPOO+OAAw6oVFE6VcDefPPNa12hvK5VzOvqsccei19//bXa2x966KGYPn16vc9/9dVXx9JLL12pPQXFn3jiiVqd46233op99923UvuCCy4Yd911V1YNHAAAAAAAAAAAAAAaS+toJjbccMNsmTVrVhbSTZWaR44cGePHj8+WFH5OId1u3bpF7969Y5111om11147q2pdSP773//GfvvtVynM3RT98MMPNd7+22+/ZfdNuj/qo0uXLvH444/H+uuvHz/++GOufcaMGVm4/7zzzosTTzwxWreu/DBPgf6bbropjj322JgyZUq52zp27BiPPvpoLLroovUaFwAAAAAAAAAAAAC0uEB3qVatWsVaa62VLc3RG2+8kYXWC0HPnj1rvL1t27ZZwH5upArdqTr7tttuG1999VWufebMmXHaaafFP//5z9hjjz1itdVWiwUWWCAmTpwYw4YNi/vuuy8+//zzSudLof8HH3wwC/sDAAAAAAAAAAAAQGNrdoFumo5tttkmq4A+bdq0Km/fYYcdol27dnPdT//+/eP111+PwYMHx0MPPVTutlSl/ZJLLqnVedZbb7247bbbom/fvnM9JgAAAAAAAAAAAABoVoHuyZMnxwMPPFDt7RtvvHEsuuii83RM1KxXr15ZmPrII4+sdFufPn3isssua7C+evTokVXWfuyxx+K8886L1157rdbHLr/88lk173333TeKiooabEwAAAAAAAAAAAAA0GwC3Y888kjsv//+1QZuU5i3JQS6zz777GwpFEcccUQsvPDCMWTIkBg2bFh06tQptttuu2y7d+/ejVIVPC2pr//85z9ZsPvjjz+OcePGxdSpU7OK4d26dYt+/frFGmusEdtuu22svfbagtwAAAAAAAAAAAAA5EXBBLr/+9//Zj9LSkoq3SaM27TtuOOO2TIvrbjiitkCAAAAAAAAAAAAAE1ZwQS6X3rppUrB7RTuThWXU7Xl7t27521sAAAAAAAAAAAAAADNNtA9ZcqU+Oabb3KB7hTkbtOmTQwZMiSOPPLILNQNAAAAAAAAAAAAAFBoCiLQ/cMPP+TWU5g7BbvPPffcOPHEE/M6LgAAAAAAAAAAAACAuVEcBWD69Onltlu3bh1HHXVU3sYDAAAAAAAAAAAAANBiAt1dunQpt73EEktEx44d8zYeAAAAAAAAAAAAAIAWE+ju3bt3tG/fPrddXFwQwwYAAAAAAAAAAAAAqFFBJKNbtWoVq666apSUlGTbo0ePzveQAAAAAAAAAAAAAABaRqA72WWXXXLrEydOjM8++6xR+9t2222jb9++sdRSSzVqPwAAAAAAAAAAAABAy1Uwge6DDjooFlxwwSgqKsq277nnnkbtb9SoUfH1119nCwAAAAAAAAAAAABAiw50d+nSJS6++OIoKSnJttP6yJEj8z0sAAAAAAAAAAAAAIDmH+hO9ttvvzj++OOzUPfUqVNjq622ijFjxuR7WAAAAAAAAAAAAAAAzT/QnVxyySUxZMiQaNWqVXz00Uex2mqrxb/+9a98DwsAAAAAAAAAAAAAoPkHupPTTjstXnnllVh33XVj1KhRsccee0S/fv3iL3/5Szz33HMxYcKEfA8RAAAAAAAAAAAAAGCOWkcB6du3b6W2oqKiKCkpieHDh8fZZ5+da2/Xrl106dIl5ptvvnr19f3338/VWAEAAAAAAAAAAAAAmlWg++uvv84FuJO0XvZnaXsybdq0bKmv0nMCAAAAAAAAAAAAADSWggp0l6oqwJ3ahLABAAAAAAAAAAAAgEJSkIHuUgLcAAAAAAAAAAAAAEAhK8hAd9nK3AAAAAAAAAAAAAAAhaogA92LLbZYnHPOOY3ax5lnnhnfffddo/YBAAAAAAAAAAAAALRsBRno7tatW+y3336N2sfll18u0A0AAAAAAAAAAAAANKrixj09AAAAAAAAAAAAAADVEegGAAAAAAAAAAAAAMiTggt0l5SUNKt+AAAAAAAAAAAAAICWq3UUkMsuuyz72aNHj0bv6/HHH4/p06c3ej8AAAAAAAAAAAAAQMtVUIHuY489dp711bt373nWFwAAAAAAAAAAAADQMhVMoHv8+PFx5ZVXVnv7nnvuGcsuu+w8HRMAAAAAAAAAAAAAQIsIdD/00ENx9tlnR1FRUZW3r7zyygLdAAAAAAAAAAAAAEBBKY4C8cQTT2Q/S0pKckvpNgAAAAAAAAAAAABAISqYCt2vvPJKpercKcy9+OKLxyqrrBKLLbZY3sYGAAAAAAAAAAAAANBsA92//PJL/PDDD7lAdwpyd+rUKW6++ebYZZdd8j08AAAAAAAAAAAAAIB6KY4CMGbMmNx6CnOnYPell14qzA0AAAAAAAAAAAAAFLSCCHTPnDmz3Ha7du1iv/32y9t4AAAAAAAAAAAAAABaTKB7/vnnL7e9xBJLRJs2bfI2HgAAAAAAAAAAAACAFhPo7t27d3Tq1Cm3PWPGjLyOBwAAAAAAAAAAAACgIbSOAlBUVBRrrbVWPPPMM9n2jz/+mO8hAQC0GNOmTYuBAwfGiBEjyrU/++yzseGGGzZIH+kDe59//nl8/PHHMW7cuJgwYULum1q6deuW9b/ccstFq1atGqS/5iBds5deeimGDx8eP/30U3Tt2jWWWmqpGDRoULRr165B+kjvv1988cXc9sEHHxyLLrpog5wbAAAAAAAAAIACCnQnu+++ey7QPWXKlHjnnXdi1VVXbbT+VllllRg2bFgWJp85c2aj9QMA0NSdd955lcLcc2vWrFnxwgsvxNNPPx1Dhw6Nt99+e47fwjLffPPF9ttvHwcccEBsvvnmMa8999xzsdFGGzXa+VMQO/VRm4D93/72t7j00ktzwfey0jfbHH744fHnP/85OnfuXO/x/PLLL7H33nvHmDFjsu111103zjrrrHqfDwAAAAAAAACAqhVHgfjDH/4Qffr0yQLWyR133NHofZaUlGQLAEBL9cknn2Th4YaSqkkfccQR0bt379h4441jyJAh8dprr1UKc1dVYXrq1Klxzz33xBZbbJEtX375ZbQ048ePj9/97ndx5plnlgtzt279/z+nOXny5Ow+W3311ePbb7+td18pEF4a5k6V0a+88srce3EAAAAAAAAAAFpgoDuFeq655prcdgqUfPjhh3kdEwBAc5Y+2HbooYfG9OnTG+yc//3vf+Pqq6+OsWPHlmsfOHBgXHXVVfHxxx9n/aUq1OnnZ599lrX369ev3P5PPfVUrLPOOvHBBx9EczH//PPXeHv61pitt9463nrrrVzbwQcfHF988UUWiP/hhx/i9NNPj+Li4lx4PoXmU8C7rt5///3supdKFb9XXnnlOp8HAAAAAAAAAIBmFOhOttpqq+yr5VO4KAVattxyyyzkAwBAw7vxxhvjxRdfbPR+UiXod999NwsNL7/88tGmTZusPf1cdtlls/Zhw4ZlP8tK1aPT+8FffvklmoN99923xtsvvvjieOONN3Lbxx9/fFx//fXRt2/fbLtXr15x/vnnxyWXXJLbJ4W9Tz311DqNI73XPvLII2PWrFnZdo8ePeIvf/lLHWcDAAAAAAAAAECzDHQnxx57bNx5553RuXPn+P7772ONNdbIwi2//fZbvocGANBspAraZYPAXbt2bZR+jjrqqDj33HOjVatWNe6Xwt3pG1pSheqy0vvBs88+O+a1/fbbLws+z82ywQYb5M638MILx4477lhtf+m9bvpgY6nS8HZ175cHDBiQ277hhhuy8Htt3X777fHyyy/nti+66KJGu/8BAAAAAAAAACjAQHey1157xQcffBB77713TJkyJQsb9ezZMw466KC45ZZb4r333stCK9OmTcv3UAEACtIJJ5wQP/30U7a+zjrrxPbbb9/gfcw///zVhpKrUlRUFBdeeGGl9jvuuCNXTbpQfPzxx/HCCy/ktg8++OBo3bp1tfs/+uijWci+bDXvDh06VHudBg8enNuePn169oHI2kjVzk855ZTcdrrv999//1odCwAAAAAAAABACwh0p8qNpcsSSywRd999d9aeKhxOnDgxC3OnUPdqq60WvXv3jo4dO5Y7pi7LsGHD8j1dAIC8eOaZZ7KQdJJCxtdee20WEm5oO+20U3Tp0qVOxwwcODD69u1brm3cuHHZB/oKydVXX51bT9e4bAC7Ki+99FK57U022aTG/TfddNMaj6/OWWedFT/++GO2XlxcnFVFb4z7HgAAAAAAAACAAg10V/VV9UkKmaRlbr/2vqpzAwC0JOkbTg4//PBylbpTiLoxrL322vU6rn///pXaRo0aFYUifcPMbbfdlttO1c8XWWSRGo+p+GHD5Zdfvsb9l1566XIVv99///05jit9A04KcJc67LDDYpVVVpnjcQAAAAAAAAAAzJ3qv9e9iaqqQmAKX5eGuhuSUDcA0NKcf/758fnnn2friy++eFaxuSEdfPDBseuuu2br3bt3r9c5qqrqPXXq1Ghsiy66aBx77LHZ+pprrlnv89x5553Zt8uUKhugr8748ePLbc/p2rVp0ybmn3/+3HEVj6/KUUcdFTNnzsyd/7zzzpvjMQAAAAAAAAAAtMBAd1V8DTwAwNz75JNP4q9//WtuO1Vrnm+++Rq0j06dOmXL3JgwYUKltp49e0ZjS1WvL7/88rk+z9VXX51bX3bZZWOTTTaZ4zGTJ08ut92+ffs5HtOhQ4dqj6/ojjvuiBdeeCG3feGFF8YCCywwxz4AAAAAAAAAAGihgW6VswEAGv791WGHHRbTp0/PtnfZZZfYZpttoin67LPPym23bt06Vl999SgEr776arz33nvlqnPX5sOJnTt3Lrf966+/RseOHWs8pmzV8orHl5WqhZ988sm57bXXXjsOPPDAOY4JAAAAAAAAAIAWHOhO1RGvv/76Ru3j4IMPji+++KJR+wAAaCpuvvnmXIXmFP694ooroqlWEa/4Hm3bbbetMbDclJStzp2qn++///61Om7BBRcst/3jjz9G3759q90/BfN/+eWXao8v6+yzz47Ro0dn68XFxfHPf/7TN+AAAAAAAAAAAMxDBRno7tSpUwwaNKjR+wAAaAnGjRsXp5xySm77vPPOi0UWWSSaoj//+c/lttu2bRvnnntuFILx48fH/fffn9vec889o2vXrrU6dqWVVopnnnkmt/3RRx/VGOgePnx4zJo1K7e98sorV7lfOs8//vGP3PbgwYNjtdVWq9WYAAAAAAAAAABoGMUNdB4AAArUCSeckIWNkxTmPfLII6MpOuecc+Lf//53bjtVkb7qqqti4MCBUQhuuummmDZtWm77iCOOqPWxv/vd78pt/+9//6tx/4q3r7/++lXud9RRR8XMmTOz9e7du8eQIUNqPSYAAAAAAAAAABqGQDcAQAs2dOjQuP3227P14uLiuPbaa6NVq1bRlLz44oux2Wabxdlnn51r69y5c9x1111x0EEHRSEoKSnJrm2pNddcs06VsLfeeutYaKGFctt33HFHTJw4scp9Z8+eHdddd11uu127dvH73/++0n533313PPfcc7ntCy64IBZYYIFajwkAAAAAAAAAgIbROgpMCsM0p34AAPLlt99+i8MOO6xctea6hIwb2gcffBBPPvlk/Prrr1lY+fPPP4+33norRo0aldunY8eOsc8++8SZZ54ZCy+8cBSKp556Kr744ovc9uGHH16n49u2bRsnnXRSnHLKKdn2zz//HMcee2zcfPPNlfa98MIL49NPP81tH3rooVn17bImTZqUna9swLxQwvEAAAAAAAAAAM1NQQW6H3zwwezn/PPP3+h9vffee43eBwBAPp1//vlZaDpZZJFF4rzzzsvreF5//fU4+eSTq7ytV69ecfzxx2fh5HnxXrChXXXVVbn1bt26xZ577lnnc5xwwgnx8MMPx8svv5xt33LLLVkw+89//nP069cvRo4cGX//+9+zpdRyyy2XVd6u6Jxzzonvv/8+V5n9yiuvjKKionrODgAAAAAAAACAFhPo3mGHHfI9BACAZiFVcL7oooty21dccUV07tw5mqrRo0fHqaeemo0zVegePHhwLLXUUlEIvvvuu3jsscdy2wcccEC0b9++zudp1apVPProo7HVVlvFa6+9lrX9+9//zpaqLL/88vH000/HfPPNV679448/Lhf6PuSQQ2L11Vevtt+ZM2dmVdLHjh2bVUhP4f8uXbrUefwAAAAAAAAAAFStuJp2AACascMOOyymT5+erW+zzTaxyy675HtIcfDBB0dJSUm2TJ48OQtCP/HEE3HaaafFQgstlO2Tqkr/9a9/jf79+8fpp58eM2bMiKbuuuuui1mzZmXrqQp2uvb11bVr13jhhReya9CzZ88q90kVwFPV7rfeeisWXnjhSrcfffTRueu24IILZpXaq5KC3yk836NHj1hiiSVijTXWyK77AgssEOuuu27ceuut2X0FAAAAAAAAAEALqtANAMDcu/nmm+P555/P1lP15n/+85/R1KRK0GlZdNFFY4sttogzzzwzzjnnnFxV8RRGv+CCC+KNN96IBx98sMlWF0/B6RtuuCG3vdlmm8XSSy89V+ds06ZNnHzyyXHiiSfGm2++GZ9//nn89NNPWdi7b9++sc4662TVvKty7733xtChQ3PbQ4YMyULdFf3tb3/LAvOpOndFs2fPjldffTVbbrzxxnj44YezkDcAAAAAAAAAAPWjQjcAQAsybty4LAxc6qyzzsqqLzd1HTp0iAsvvLBS+PyZZ56Jvffeu8lWik5h89GjR+e2jzjiiAY7d3Fxcay11lpZFe1jjjkm/vCHP8T6669fbZg7VT1PIfBSq6++elYVvaIU8j7llFNyYe5NNtkkXnnllZgyZUp8++23cfbZZ0fr1v/3udAXX3wxNt544/j1118bbF4AAAAAAAAAAC2NQDcAQAuSAr3jx4/P1gcOHBgnnHBCFJIjjzwydt1113Jtjz76aFx33XXRFF199dW59T59+sS2226bt7Gce+65MWrUqFwY/Kqrrsp+lvX6669n1dDLVhR/8skns6rfqZp7mkP6EMBNN92U2+e9996L0047bR7OBAAAAAAAAACgeRHoBgBoIZ599tm47bbbsvWioqK45pprcpWWC0nZwHHZqtIzZsyIpuSTTz6J5557Lrc9ePDgaqtnN7ZPP/00Lr/88tz2QQcdFGussUal/c4444yYNWtWtp4eGykoX9WY991336xyd6kUDh85cmSjjR8AAAAAAAAAoDnLW4LnwAMPnOM+KWh04403NtoYttpqq3jqqafmOIbSr5sHAChUv/32Wxx22GG57UMOOSTWXXfdKESpsviKK64Yw4YNy7V9++238cILL5QLGTel6txt2rTJrnm+HHXUUbnAe7du3eKCCy6otM8XX3wRzzzzTLn3yksssUS15zz88MNz+6f3y9dff32cc845jTJ+AAAAAAAAAIDmLG+B7ltuuSULS1enpKSk0QPdpf0AADR3KWw7fPjw3Hb37t3j4osvrvXxH330UaW2e++9N956660q999jjz2iT58+0VhSGL1soDtJ1bCbSqB76tSpuWroyc477xw9e/bMy1juv//+ckHt888/PxZccMEqK7iXfW+8+eab13jezTbbLHu/XnpMOl6gGwAAAAAAAACggALdNQWqawp6zwul/Qt7AwDNxbhx48ptDxkyZK7Pec0111R72+qrr96oge7FF1+8UtuXX34ZTcWdd94Zv/zyS7lq1vkwZcqUOPHEE3Pbq622WgwePLjKfd9+++1y2/3796/x3F26dIlFFlkkRo4cWeXxAAAAAAAAAAAUSKC7NEC92GKLzfN+99lnn1h77bUrtafKgvkOlQMAUL1u3bpVahs/fnw0FVdffXVufYUVVohBgwblZRznnXdefPfdd9l6en975ZVXRnFxcZX7jhkzptx2bSqKL7TQQrlAd6pKngLkHTt2bJCxAwAAAAAAAAC0FE0i0J189dVX87zP3//+91W2+6p4AKC5Ofvss7Olvvbff/+49dZby7U9++yzseGGG9b6HKNHj86WJH2Yr6pQdm399ttvldrat28fTcHrr78e7777bt6rcw8fPjwuvfTS3PaBBx4Ya621VrX7T5w4sdx2hw4d5tjHfPPNV247VSUX6AYAAAAAAAAAqJuqy/MBAEADu+aaa2KVVVbJljvvvHOuzlWxmnRtK0rPC1dddVVuvVOnTrHvvvvmZRxHH310TJ8+PVtfYIEF4sILL6xx/4oB7mnTps2xj19//bXGgDcAAAAAAAAAAAVUobs2WrVqVedj0lfLz5w5s1HGAwBA/YwaNWquq2BXtOyyy0a+/fTTT3HfffeV+0aYLl26zPNx/Pvf/46nnnoqt33++edH9+7dazymYsX0sWPHzrGfcePGlXuvPv/889drvAAAAAAAAAAALVlBVeguKSnJ/azLAgBA0/Lcc8/V+9gUIn7ppZcqtW+zzTa1On727Nnx4IMPxgUXXBAPP/xwg75fvPnmm8tVtj7iiCNiXps6dWqccMIJue1VV101Dj300DkeN2DAgHLbH3/88Rz7+fbbb3Pb/fv3zz5MCQAAAAAAAABAMw50J1UFblJwpHQBAKDpe+ONN+Ktt96q17F/+9vf4tdffy3XlkLL/fr1q1WYe8stt4ydd945Tj/99Nhxxx1jhx12aJBQdzrHNddck9ted911Y8UVV4x5LVXjLg1ap/fHV155ZRQXz/lt//rrr19u++mnn65x/6FDh5a7bhWPBwAAAAAAAACgdlpHATnrrLOqbD/nnHNygZXq9gEAoOlIQeDBgwfHCy+8EJ06dar1cU8++WRcfPHFldovu+yyWh1/4403xv/+979ybY888kjccsstccABB8TcSOcdMWJEXqtzf/755+WuT5rT2muvXatj11prrVh22WVj+PDh2fZjjz0Wo0aNikUWWaTK/a+77rpy2/vvv/9cjR0AAAAAAAAAoKVqVoHumvYBAKBpeffdd2PTTTeNO+64I5Zeeuk57n/zzTdnIelUZbusU045JTbYYINa9VkxzF02KD63ge6rr746t96jR4/YbbfdYl475phjYvr06dl6165d48ILL6z1senDkaeeemocdNBB2fZvv/2WXe+HHnqo0jfh/Oc//8mC8KU22WSTWHPNNRtsHgAAAAAAAAAALUlBBboBAGhcr7zySrZU9NFHH1Vqu/fee+Ott96q1J4qb3fp0qVW/b3++usxYMCA2GWXXWLXXXeN1VdfPXr37h2tW7fOAsXffPNNPPfcc3HttdfGO++8U+n4E044IS666KI6VQavS3ttjRw5slzA+cADD4y2bdvGvPTggw/GE088kds+77zzsmB5XaRx/+tf/4rHH388F9zeYYcdYsiQIbHCCivExIkTs2rmp512Wu6Y+eefP6t8DgAAAAAAAABA/Qh0AwCQ89RTT5X79pOaXHPNNVW2p2B2VYHuE088MXr27JlVsv7ggw9y7Sm4fdddd2VLqXbt2mXt1enbt292ns033zzqYuONN84CyxXV9TwVXXfddTFr1qxsvbi4OA477LCYl3799dc4/vjjc9urrLJKvcdw3333ZSHuoUOHZtspqJ6WFLKfOXNmuX0XXHDBLES++OKLz+UMAAAAAAAAAABaruJ8DwAAgJahc+fOcfjhh8ewYcOyatt/+ctfYt11162yknVVYe6uXbvGzjvvHA8//HB89tln9Qphp+rhW265Zbm2rbfeOg444ICorxRyvuGGG3LbW221VSyxxBIxL6UK2qmaeVJUVBRXXnlltGrVql7n6tSpUzz55JNx+eWXZwH8UmXD3G3atIl999033n///ayqOgAAAAAAAAAA9adCNwAAOWeffXa2NLZUQTotZ5xxRhYUHjFiRHzyyScxbty4mDhxYlZxumPHjlml7+7du8fAgQOzqtxzK4Wc//vf/8bjjz8eH374YSy77LKx/fbbZ1W16ytVrv7+++8jX2bPnp1dq7POOivb7tOnT6yzzjpzdc40p2OPPTaOPvroeP3117Nrle6b1E86f6p0Pv/88zfQDAAAAAAAAAAAWjaBbgAA8iqFh/v165ct80KqYJ2qcqelOUhh9NNOO63Rzp3C4XMbEAcAAAAAAAAAoHr1L0UIAAAAAAAAAAAAAMBcEegGAAAAAAAAAAAAAMgTgW4AAAAAAAAAAAAAgDwR6AYAAAAAAAAAAAAAyBOBbgAAAAAAAAAAAACAPBHoBgAAAAAAAAAAAADIE4FuAAAAAAAAAAAAAIA8aR1NxLnnntukzgMAAAAAAAAAAAAA0CIC3SUlJXHOOec0mfMAAAAAAAAAAAAAALSYQHdpGLspnQcAAAAAAAAAAAAAoMUEuouKihokxD0356nqfAAAAAAAAAAAAAAAzTrQPbch7IYIcTfm+QAAAAAAAAAAAAAAmmyguylWxBbqBgAAAAAAAAAAAACafaA7Bad33333OOyww6KpuPHGG+OOO+7I9zAAAAAAAAAAAAAAgGYu74HupE+fPjFo0KBoKp5++ul8DwEAAAAAAAAAAAAAaAGK8z0AAAAAAAAAAAAAAICWSqAbAAAAAAAAAAAAACBPWkcelZSURFFRUT6HAADQYg0bNiyGDh0a/fr1iy233DLfwwEAAAAAAAAAgBYpb4Hu7bffPhfmHjBgQDQl/fv3jx122CHfwwAAaDT3339/7L333jFz5sxs+9hjj43LL78838MCAAAAAAAAAIAWJ2+B7oceeiiaqr322itbAACaq6OPPjoX5k7+/ve/x6GHHhrLL798XscFAAAAAAAAAAAtTd4C3QAA5Me4cePixx9/LNdWUlISH330kUA30GjStwBMmDAht73yyivHjjvumNcxAQAAAAAAAEBTINANANDCLLjggtkyfvz4cu3LLrts3sYEtIxA9zfffJPb3m+//QS6AQAAAAAAACAiivM9AAAA5q2ioqK47LLLolWrVrm2Aw88MFZcccW8jgsAAAAAAAAAAFoiFboBAFqgfffdNwYMGBDPPvtsLLPMMrHddtvle0gAAAAAAAAAANAiCXQDALRQq6yySrYAAAAAAAAAAAD5I9ANAAAAAAAA0EBmRcTMouJK7b9Om1LzcbNmNtqYfps+bY79V1RUVBTt283XaGMCAAAA/j+BbgAAAAAAAIC5NKWodTwy/6LxVdtOMaOoVeUdTlov8uWCq4+q13ELLbhIbDloz9h2430bfEwAAADA/yfQDQAAAAAAADAXSiLi9gX6xpg2HaI5GTN+VNz2wCXRrm2H2Gz9XfM9HAAAAGi2BLoBAGgU3377bbz44osxcuTI7OtZe/XqFWussUYsv/zytTr+/fffjzfffDPGjh0b8803X3b8+uuvH4ssskijj33y5Mnx2muvxeeffx4TJkyI9u3bx8ILLxwDBgyIFVZYIeaFDz74IN57770YPXp0TJ8+PTp37hxdu3aNxRdfPPr27RuLLrpodl1r47fffsvmk+6LdD2nTZsWPXr0iIUWWihWX3316N27dzRHs2fPjjfeeCPefffd+Omnn6JTp07ZtVt33XVjwQUXnOPxU6dOjZdeeik++eSTmDJlSnbMkksuGRtssEH2mGho33//fbz99tvx448/5h733bt3zx7za621VnToMG/+IJwed2kcY8aMibZt22ZjWGWVVWLgwIG1fswBAABAS/ND6w7NLsxd1nOv/UegGwAAABqRQDcAQAvw3HPPxUYbbVTjPmeddVacffbZldpvueWWOOCAA2o89uabb479998/W//oo4/ipJNOiieffDJKSlJtovJWXHHFOP/882Pbbbet8lz/+c9/4vTTT8/OU5UUpr344ouzcHhDzHu//fbL5pik4O4555wTDz74YBairkr//v3jiCOOiMMPPzyKi4vnOIY5BWAHDRqUjTOZOXNmXHXVVXHZZZfF119/XeNxX331VSyxxBI17vPUU0/FP/7xjxg6dGgWTq7OyiuvHHvuuWccffTRWYi4JqnPb775psZ9ajPO2jyuyj62Ntxww3j++edr3Lfs4y2dPz2eqxprmzZtYrfddosLL7ww+vTpU+n2FN5Oj9Err7wyJk6cWOn2FKw+9NBDs+dMCtnPjRSuT/3ceeedWfC8Ou3atcs+0HDkkUfGTjvtVKtz1/Wa3XbbbfGXv/wlRowYUeW+6UMEf/rTn+KQQw6JVq2q+NroOj5Wbr311myp7fgAAACgKZvdzD8EPbtkVr6HAAAAAM3anBMoAABQS/fcc09W8fmJJ56oNog5bNiw2G677eLkk08u1572P+6442KHHXaoNsydvPDCC1mF5X/9618NOvabbropCzbfe++9uTB3Cv5W9PHHH8dRRx2VjSGFlRtKqsj8u9/9Lo499tg5hrnn5IsvvohNN900tthii3j00Ucrhblbt25dqSLzaaedFksvvXTcddddUcjSfbfHHntkYfHqwsQzZszI5pk+XJAqcJf13XffZdWwL7jggirD3Mmvv/4al19+efYY+OGHH+o91ocffjiWW2657AMQFcPcFR97qcr6M888EzvvvHP2HEsfPmgo6dy77rpr9uGG6sLcSarwnj7IkPZNxwAAAAD/X68Zv0b72TOjuVphmTkXVwAAAADqT4VuAIAWYKmlloq//e1v5doqBqqrk0LOqRJxqQkTJsQVV1xRab8UHN5nn31i1qxZWX+pOvBCCy0Uv/zyS7zyyitZaLisVGU7VfxNAeYkVeUuPe/aa68dq666asw///zx/fffx9NPPx2jRo3KHZsqWe+9996x/PLLxworrFCneQ8ZMiR+/vnncm133HFHHHTQQdl6qtycKhCvueaaWf8p1Pv222/Htddem4W9S73++utZ9e9UXbumStllr11p5eiKQePJkyfHxhtvHB9++GG2nUK+6623XvTo0SPGjBkTL7/8cgwfPjxq49VXX43tt98+xo0bl2tL50lh+RQGXnLJJaNt27bZeVPl7lQRvDTUnMLJv//977OwcKrUXJVUoTndp0m6v1LIt6xzzz03q2Bdqlu3blWeJ1VYL3vf/Pvf/47XXnstW99rr72y+790v6S0SnepdN2rqj79hz/8Ie67776sgnSqfp4eHx07doxvv/02/ve//2XB+bKP5XSt3nnnnew+TNubb755fPrpp9GpU6dsvW/fvjF79uz47LPPskB1qqhdKl2n9Hh58cUX51iJvaJLL700C3KX/eBDaaA/zXPBBRfMwukpnP/II49kz5fSsafHYwqTP/DAAzVWoK/tNUvP23T9kxQWT8sCCywQP/30U/bc/eCDD8rt/9BDD2XP10suuaTGOZZ9rFT13Ev9pPA9AAAANAetoyTWmjounu/UK5qbNq3bxtYb7p3vYQAAAECzVlTiO6yhYKRqpQMGDMhtp9BXTSE2CteHuzXvr2aEQjPg/urfLr327tNx6Y0nNUq/m663awze64xoLBUDqCl4fPbZZ8/xuFQ9OoWCy7rooouyJQVfr7nmmipDmimUuueee5arFp2CtqnKdaranSpK9+/fP26//fZcmLdsReVzzjknzj///HLtm222WTz11FNRFym4WzZQnUK0KSCbwrOpwvguu+xS7bF333137LvvvllovVSqav3+++/HfPPNV6v+U8C2bKg2hY7TmG699dZs/un6pTGVlQLsqYJ22QBtum4Vg+RpHuuvv3650HHaTtXMe/bsWeV40n8HUjXqFL6tGPj/61//WuNc0v1xxhnlH6M333xzFiSui3Q9F1988Sy03759+ywkngLNNUmP1fSYKOv666/PwvgpDJ+uZwr0l5Uee8cff3xcd9115dpTKDuFwFMYPD3+0jnSte7cuXO5/VIoPFWnfvPNN8u133nnndkHDOoS5j7xxBMrXcsUkq5OCpun6vWpQn3Z508Koy+77LK16reqa5Y+qHDooYdm7ynTNVtttdUqHffYY49lz930wYNSKTCfAu3LLLNM1Pe5lyqCpw84UH/eM0LhvGdMdj9q5Ubpt3OnBeLGC59tlHMD0Dx43wjzzrSi4vh79+VjWnHzqqm11YZ7xQG7nprvYQDQiLxnhML5XePLbz0RV9xyWqP0u+UGe8aBuzfOuQGamo+aYBazOK+9AwDQLKRKy5MmTYr//Oc/1Vbc3W677eLyyy8v1zZlypS44YYb4phjjskCvalidMUwd9KmTZs477zzsjBtWalyd8Vq13WVqiunoO/f//73GsPcpZWjK4bKR4wYUWMQtzb/SUhB2lQJPVXirhjmTlq3bh0XXnhhjeHZFPhNweSyYe5UXfrhhx+uNsxdGuxP40/3QcX7NN2fNUlVzdN9U1YKpNdVCvuXVmBP9/GcwtzV+eMf/5g9fp588slKYe4khe7T+FLIvaxUnfrBBx/MwtyHHXZYFviuGOZOFltssWysqXp3WekxXFupsnsK55d19NFHz/Ex1LVr1yxYnZ4nZZ8/KUiePvBQXynInwLh6UMGVYW5k2222abS/ZpC+OlxCwAAAPx/7UtmZ1W6m1t17h02PSDfwwAAAIBmT6AbAIC5Nm7cuCwIW1UYuawDDjggunXrVq4tBaQ//vjjrGpxTcHj0qrRFatLp4Dt3Eph6lSluDZSZeWKFcr/8Y9/xKefflrva9e2bdus+ncK7VYnhbrXXXfdam9PgeBUtbusVGG74vWuzpAhQ2KhhRaqFNguW1G9ol69emVVoysGlt97772oi1QlulRt74eqjB8/Pgtjp8rVNQXYTzjhhHJtqbJ8qrye5nPxxRfX2Ed6jKZ9y3ruuedi4sSJcxxf6idVLy8bwE7XPIX1ayMFyStWTU9V2e+9996or/T4S2HtOYXo04cZFllkkXJtKWAOAAAAlLfW1LHRfvbMaC42XX+X6Na1/O+MAAAAgIYn0A0AQIOoGJKtLpS80UYblWtLVYaXWGKJ2Hnnned4/BprrBELLLBAubb3338/5lYKoxcX1+6tcZpDqqhcMah79dVX17v/VGW5X79+c9wvVdG++eabs6V79+659tGjR2dtZS299NK1uqalUgj6qKOOqhT2vf7662s87vDDD68xoD0nX3/9dTz11FPZevo6o4rVs+tigw02qLbKdFmbbrppFuyu+DhMYfKawuClNt9880ofLPjggw/meNxDDz1UKfif+kyVw2trp512qhS+Th8oqK/0nKr4nKxKen5svPHG5drSBzFmzmw+f6AGAACAhtCcqnSrzg0AAADzjkA3AABzbcUVV8xC2bVRVXB5++23r9WxKYS77LLLlmsbPnx4zK0dd9yxTvtvt912ldpuvfXWcpWX6+L3v/99rfZbddVVswrPaUnVmssGqKdNm1Zu31Q5u2JouT7X4fLLL6/xmBTyrXif3HnnnTF58uRa9ZkqaqdA/NxW504qVguvTufOnWPhhReu9/FVPYZr8zj8+9//XqmtLqH7pE2bNrHeeuuVa3vjjTfq/TyoS//9+/cvtz19+vT47rvv6tUvAAAANGfNpUq36twAAAAw7wh0AwAw11ZZZZVa77vQQgvN1fEVg7i//PJLzI3evXtHz54963RMqn5d1TjqUy08VT5ee+21Y26UVrguq2I15doYOHBgucrfpRW0R4wYUeNxFYPYkyZNykLdc5IC8DfddFO2nqpU77vvvpGvx2EKSqcK4bVRVRh8To/DVAH85ZdfLtfWrVu3WGmllaI+H6Co6LXXXov6VuiurcUWW6xS29w+/wAAAKA5ag5VulXnBgAAgHlLoBsAgLlWsUJzTTp27DhXx6fgb8Xw8NyoqtpybSyzzDINEqpdfPHFy1XbrqupU6fGm2++2WDzquq45557rsZjUsXw9u3bl2tLVcPn5OGHH44ff/wxW99zzz1j/vnnj3w9DlOF+RTqrs9jsDaPw1deeSVmzpxZ6VrXtYp6UjF0PzeB7uWWW67W+3bp0qVS29w+/wAAAKC5KvQq3apzAwAAwLzVeh73BwBAM1RV0LM6rVq1mqvjW7cu/xZ21qxZMTe6du1ar+P69u0bzz//fLm2Tz75pM7nWWCBBWJufPbZZ1ml64rXOAWU62OppZaKl156qVzbhx9+WOMxqdL07rvvHrfddluu7d1334033ngj1lxzzWqPu+aaa3Lrhx12WOTzcTg3j8HaPA6ruoapavfFF18cdZWua0WpknpjP/47dOhQqW1un38AAADQ3Kt0P9+pVxQa1bkBAABg3hPoBgBgrlVVdXteHj836lsdu3PnzpXafv7553nWf6nx48dXec7i4vp9GU9Vweaq+qgoBbLLBrpLA9vVBbpHjBgRQ4cOzdZXXXXVWGONNWJuzc3jqLEfg1Vdw/fffz9bGkJ9Hnt1nXdVH8YAAAAAaq7S/fp83WNacWH9SVZ1bgAAAJj36pfyAACAMoqKivJ6/Nyob/C5qiB2fUK1czv3qoLCcxNOrmpetQl0r7POOrHSSiuVa7v33ntjwoQJVe5/3XXXRUlJSbZ+6KGHRkOYm2vZ2I/B2lzDuVHddW7Kzz0AAABoKVW6C4nq3AAAAJAfAt3V2HbbbaNv377ZV84DAEBFpWHkfIdj50Wfte0jVekua+rUqZWqdifTp0+PW265JVfpfO+9947mrqpruM8++2SPo4ZYPvnkk7zMCwAAAJhzle72s2dGoVCdGwAAAPJDoLsao0aNiq+//jpbAABovmbPnl2v46ZMmVKpbYEFFoh5bcEFF6zV2Gpr8uTJteqjKr///e8rVfi+9tprK+3373//O8aOHVvtMc1RVdewqmsNAAAANC+FVKVbdW4AAADIH4FuAABatPqGaidOnNhkA91pTvUNqlc1r9oGulO17RTQLuvjjz+OF198sdqQd8Wq3s1VVdewqmsNAAAAND+FUqVbdW4AAADIH4FuAABatAkTJtTruC+//LJS2/LLLx/z2nLLLRdt27Yt1zZr1qx6f9PMF198Ualt4MCBtT7+8MMPr9R2zTXX5NY//fTTeP7557P1tdZaK1ZaaaVoCaq6hlVdawAAAKD5KYQq3apzAwAAQH4JdAMA0KKlgHF9DB8+vFLb2muvHfNahw4dYo011qjUnipjN9T1GDRoUK2PTwHtitfh3//+d4wbN67FVudO1l133UrB+2+//TYmTZpUr/P99ttv8fTTT+eW+gb4AQAAgHlXpbtjh87RVKnODQAAAPnVOh+d9u3bN5q677//Pt9DAABgHvjhhx/ixx9/jJ49e9b6mM8++yxGjx5drq1r1655qza9xRZbxMsvv1yubejQobHtttvW6TzDhg3LBa9LLbXUUtlSFymo/dprr5ULH998881x9NFHx2233Za7XnvssUe0FCl4/7vf/S6eeeaZXFtJSUl2P+2www51Pt///ve/2G677XLbTz31VCyxxBLRlBUVFeV7CAAAAJDXKt3bbLxP3PfY1dHUqM4NAAAALTTQnarHpT/mpwBDUyVsAADQcjz88MMxePDgWu//yCOPVGrbf//9o3XrvLy9jkMPPTSGDBkS06ZNKzenSy65pE7vax966KFKbccdd1ydx5OC2ieccEL89NNPubbrrrsuFlpooVzbH/7whyzk3JKka1k20J3cf//99Qp033HHHbn1bt26xUYbbRRNXbt27cptz5o1q9p977nnntz6OuusE4svvnijjg0AAADmha0G7RWPDb0jpvxav2/saiyqcwMAAED+Feez8xQuaaoLAAAtx7XXXlvrDxvOmDEj/vnPf5ZrKy4uzqpS50sKSh944IHl2r788su47777an2OKVOmVJpXjx49Kp23Ntq3bx/77bdfubYRI0bEySefXC6E3tJss802MXDgwHJt6T4aPnx4nc7zySefxL/+9a/c9lFHHZW3DxPURZcuXcptT506tdrH4l577ZVb3nnnnXk0QgAAAGhcHefrklXpbkpU5wYAAICmIa+BbgAAaApSYDRVkK6Niy++OL755ptybcccc0wst9xykU+pQnffvn3LtZ122mkxbty4Wh1/+umnx9ixY3Pb6UOON954Y8w333z1Gk9Vge3S8//ud7+L/v37R0uTruktt9xSrlJ1+oDAIYccEr/99lutzvHrr79m1c1Lq1t37969XlXU82GppZYqtz1q1Kgq90vh/7IWWWSRRh0XAAAAzOsq3R07dI6mQnVuAAAAaBryGuhOVRDndpnbc9Z0PAAAzd8GG2wQHTt2jKOPPjoeeuihGve966674owzzijXtvTSS8f5558f+Tb//PNn1Z5TdexSX3/9dWy//fYxevToao9L73svuOCC+Pvf/16u/aSTTortttuu3uNJAfeNN964ytvyWc0831ZdddW44ooryrW98MILsdtuu8XPP/9c47E//vhjVuX7rbfeKhcQX2CBBaIQrLPOOuW2hw0bllXjruiee+7JracPFKy88srzZHwAAADQ0qp0q84NAAAATUdev5d7scUWi3POOadex6ZwykUXXZRVsisNX6cgw0orrRQrrLBCLLzwwtGpU6dsKS7+/7n1FBiYNGlSVqnw448/zkIEpZXhUiCiV69eceqpp0bXrl0baJYAAPn33Xffxb333lvjPq+88kpWfbpUnz59Yo899oiJEyeWq15dVej0iSeeKFcJequttsrek1XVd2kYtazrr7++XCg1hYnLnvvDDz/MbX/00Ufljk3jKzvuFGxOFY9ra8kll4zBgwfHPvvsEzvttFM254MPPjjWXHPN6NKlS/be8e23345rrrmm0jVM72f/97//1VjFOh2TrkGpsuul22XHX9U1qK3VVlsthg4dmoW4S++PV199NQYOHJhVcd51112z+bZp0yarlv3ss8/GlVdeGS+++GK58/zpT3+K8847L+ZWCm6n8ZSVKkrvsssudT5Xenympex2RRWvY02PozndDwMGDIgtt9yy2nNXNb6y+6y77rrZUl318vR/lCOOOCJmzpyZtT3yyCPRr1+/OOqoo2LHHXfMqlmnx1X6/84HH3wQDz/8cFx11VXx008/Zfun4//xj39kAe/GumYVn/tffPFFlY/vss/p9PxJrx1V2XPPPeOUU07JVSNP1cbTczWNIf3/LT3XUkD9kksuyR2z//77R9u2baudIwAAABRqle7Hht4RU36dlNdxqM4NAAAATUdRSR5KUafwQQpPp/B1+nr7ukphhxS4mTx5cnaevffeO/tD/4YbblguvF1b7777blZt8eqrr46pU6dmoe77778/1ltvvTqfCxpTCrClcFGpFEoqDcvRvHy4W1G+hwCUMeD+6t8uvfbu03HpjXUP3tbGpuvtGoP3Kl8Nur6ee+652Gijjep0zKBBg7Lj0gfpUgi4Lm6++ebs/Vl9+y77FjWd59Zbb631sYsvvng25uosscQS8c033+S299tvvyxEetNNN8Xhhx8e06dPz92Wgs8zZsyo8jwp8H333XdH3759axxPeo/6/PPPR13Nzdv0FLxNoeFnnnmm0m3p/XOrVq1yQeKyevfunYVr0/vrhpCuXQq9l60QngLDf/vb3+p8rrPPPrvOHwadm8dR6eOi7HWri7POOisbc03S/XPkkUfGZ599VuXt7dq1y4Wfy1pkkUWy/7vMqYL63F6z+jz304cE0mO+Oqk6efpwQW3muuyyy8brr7/uw7Y18J4RCuc9Y7L7UY3zjQOdOy0QN174bKOcG4DmwftGaJrvGf/1+LVx32NX57U69z/OflSgG4CM94xQOL9rfPmtJ+KKW05rlH633GDPOHD3xjk3QFPzURPMYtY9/Zxnjz32WOy8885Z9bZ08VIg/Lbbbsu+zr0+Ye5klVVWyUIln3/+eWy99dZZ4GSTTTbJqvgBANAyHHjggfHee+/FbrvtlgW5k6rC3Msvv3xWGTlVvp5TmDtfUnXnp59+Op588snYdttty1UQT4HdimHu9EHLCy64IEaMGNFgYe4kXceDDjqoXCg6Bc35P+n/HOk/hakKdqrmnYL2ZVUMOPfv3z8L3H/yySdzDHM3Vccee2zcfvvtlYLiZeeawt3p+fjaa68JcwMAANCsq3R37NA5b/2rzg0AAABNS0FV6B45cmQWYkiVuVNFxBRQmX/++Rt0bLNnz86q8d15551Z8CVV715mmWUatA9oTp8KoXH4BDQ0Lc2hQjdzrtBdVnq/mQLb6QN/EyZMyAKmCy+8cAwcOLDcv8WFIoVlUzj2u+++i7Fjx2bb3bt3j549e8Zqq62WzY38+/nnn7P7KX3ANN1P6f8mKdC86KKLxuqrr559k1BzkeY2bNiw7P9b48ePzx6TXbp0iaWXXjrWWWcdQe5a8p4RmhYVugFoqrxvhKb7njFfVbpV5wagIu8ZoWlRoRugZWYxW+er4/rkyE8//fQsXNOxY8e46667GjzMXRo2TxXyUoDnyy+/zL4C/amnnmrwfgAAaLo6deoUm222WbY0BymQPmjQoHwPgzlYYIEFYquttoqWIP2/a+WVV84WAAAAaMlVuh8bekdM+XXSPO1XdW4AAABoeorz0endd9+dLRdeeGGtj0kV6u69996ssneqotiYX2/foUOHOPXUU7P1Z555JqsaBwAAAAAAANBQOs7XJbbZeJ95Xp17h00PmKd9AgAAAE000L3HHntky+abb17rYx5//PGYMWNGtr7LLrtEY0t9pPB48q9//avR+wMAAAAAAABaXpXujh06z7P+VOcGAACApikvge76ePPNN3Pr/fr1a/T+unXrFt27d8/WX3nllUbvDwAAAAAAAGhZ5mWVbtW5AQAAoOkqmED3iBEjcuulQevGlvopKSmJ4cOHz5P+AAAAAAAAgJZlXlXpVp0bAAAAmq6CCXT/8ssvufVx48bNkz7Hjx+f/fz555/nSX8AAAAAAABAyzIvqnSrzg0AAABNW8EEumfOnJlb//zzz+dJgHzs2LHZ+qxZsxq9PwAAAAAAAKBlauwq3apzAwAAQNPWOgpE165dc+v33ntvbLDBBo3a3z333BMlJSXZ+vzzz9+ofQEA0Di+++677L1jWRMnTiy3/dFHH8XFF1+c2+7Tp0/sscce82yMAAAAAFBapfu+x65u8HOrzg0AAABNX8EEupdeeul4+umns5D17bffHieccEIstdRSjdLXpEmT4m9/+1sUFRXl+gYAoPB88cUXcfLJJ9e4z1tvvZUtpQYNGiTQDQAAAEBeqnQ/NvSOmPLrpAY9r+rcAAAA0PQVR4FYf/31s58pZD158uTYfffd46effmrwfmbMmBH77bdffPnll7kK3eutt16D9wMAAAAAAABQsUp3Q1KdGwAAAApDwQS6t9566+jQoUMu1P3uu+/GGmusEa+//nqDVnBMFRkffvjhXHXuJIXHAQAoPBtuuGH2Ib26LM8991y+hw0AAABAC67S3bFD5wY7n+rcAAAAUBgKJtDdtWvX2H///XNVs5Ovvvoq1l133dhmm23i0UcfjV9//bXO5505c2YW2tl3331j+eWXzwLipWGeFOpO1blTcBwAAAAAAACgUKp0q84NAAAAhaN1FJBzzjkn/vWvf8W4ceNyFbRT8PqJJ57Ilnbt2sVaa60V/fr1i2WWWSZ69+4dHTt2zJa0/5QpU2Lq1KkxZsyY+Pzzz+Ozzz6L1157LSZPnpw7V5L2TeutW7eOyy+/PK9zBgAAAAAAAFpWle7Hht4RU36dNFfnUZ0bAAAACkdBBbq7d+8et912W2y33XYxa9ascuHrZNq0afHCCy9kS22VrfhdNiSe1v/617/Gqquu2uDzAAAAAAAAAKipSvd9j11d73Oozg0AAACFpTgKzBZbbBG33HJLtGrVKteWwtelSwpj12Upe2xZZ5xxRhx77LF5mCEAAAAAAADQ0qt0d+zQud7Hq84NAAAAhaXgAt3J3nvvHU888UQsuuii5SpsJ2UD2rVZykrn6tKlSxYYP+ecc+bxrAAAAAAAAAD+f5Xu+lCdGwAAAApP6yhQG2+8cXz00Udx8sknx/XXXx+zZ8+uFNCurdJQ+Pbbbx9XXXVVLLzwwg08WgAAAABofiZNmhTvvvtufP755/Hzzz/HtGnTYr755otu3bpF3759Y9lll41evXrle5hNzksvvRQHHHBAjBgxItdWsXBFQ0i/M3399dez36OOHTs2K2ax2GKLxUYbbRSdOnVqkD7efvvteOSRR3Lbu+66awwYMKBBzg0ALV2q0v3Y0Dtiyq+T6nSc6twAAABQeAo20J2kPzpcffXV8cc//jGuvfbauOmmm+LHH3+stF9p0LuqP4qkP2Lss88+cdhhh/lDAwAAAADMwfTp0+Oee+7Jfhf38ssvx8yZM2vcv0+fPrHuuuvGlltuGVtttVX07Nmzxv3rW7ShthojOF1bv/76a5x++unx97//PQtbN5ZZs2Zlvy/9y1/+EqNHj650e7t27bLfiQ4ZMiQWWqj+Ya903++3335ZYDxZcskl47TTTpursQMAlat03/fY1bU+RnVuAAAAKEwFHegularKnH/++XHeeefFhx9+GK+99lq8+eabMWrUqJgwYUJWHSj9oaZr167Zkv5otNpqq8Xaa68dq6yySrRu3SwuAwAAAAA0qv/+979xzDHHxBdffFHptvQ7thRSrhhU/u677+Lee+/Nlk022SSefvrpaImqqsrdWKHxXXbZJR5//PFK909p+P63336LG2+8Maus/dRTT8VKK61Ur75SML00zJ1cccUV0b59+7mcAQAwN1W6VecGAACAwtSsksypes/AgQOz5ZBDDsn3cAAAAACgWUjFElLl5b/+9a+5th49esTee+8du+66ayyzzDK5Ss8jR46MoUOHxmWXXRbvv/9+NCXp2/qaa1XuUqnydtkw94477phV6l5hhRXil19+ySqrp/GkUPeYMWNi4403jmHDhsUiiyxSp35++OGHOPvss3Pb2267bWy33XYNOhcAoG5VulXnBgAAgMJVnO8BAAAAAABN26GHHlouzL3HHnvEp59+Gpdffnmsv/762TfipWILaenTp0/st99+8dZbb8XgwYOjKdl3333neVXuFVdcMbtOpWHuFKDu3r17o/R3zz33xAMPPJDb3nnnnbPtAQMGZPdN+vbCE044Ie6+++7cPj/99FO97qeTTjopJk36v0qhqSp3qs4NADRele6OHTrPcT/VuQEAAKBwCXQDAAAAANW66KKL4vrrr89tH3zwwVlwuFu3bjUe17p167jqqqtijTXWqFe/iy++eFYZfG6WM888s9w5Dz/88JhXVbmPP/74GDRoUIwYMSJr69y5c1x99dXx9NNPR8eOHRul3yFDhuTWU8j6n//8ZxbkrminnXaKbbbZJrf93//+N957771a9/PCCy/EXXfdlds+5ZRTom/fvnM1dgBgzlW6a6I6NwAAABQ2gW4AAAAAoEpvv/12nHHGGbnttdZaKwsl11arVq2ySs75MHPmzLjhhhty2xtssEGssMIK86Tv119/vVxV7s022yw+/PDDOOyww6oMWDfUffXBBx/ktrfbbrvo3bt3jVXXy7r55ptrfV2PPPLI3PYSSywRp512Wr3GDAA0XJVu1bkBAACgsLXO9wAAAAAAgKYpBZBTgLfUBRdckFXerouNNtooF6RecsklY155+OGH4/vvv89tH3HEETGvdenSJS6++OI45JBDGr2vl156qdz2JptsUuP+G2+8cRYuT5XMqzq+Oqnqdwqnl7riiiuiQ4cO9RozALQ0P5y11Fwdv0G0jcejXaX21lES67x/ffzwwXVzdX5oaXqf80W+hwAAANAyAt1Tp06NCRMmxC+//BKzZs2KAQMG5HtIAAAAAFAQHnzwwXjrrbdy22uvvXYWzq6rHj16lAsAzytXXXVVbr1nz56x8847z9P+t9hii7j++uujT58+86S/YcOGldtefvnla9y/Y8eOsdhii8U333yTOz6Fu2uqID569Og466yzcttbb711bL/99nM9dgCgdjZoPz1e+61N/Dy7/Jcwb9Jhesxf/H8f0gIAAAAKU7MKdL/wwgvx3HPPxZtvvpktY8eOzd228sorZ187WtHhhx8e6667buywww5ZxRwAAAAAIOLyyy8vt73TTjvNk36PPfbY7Ge3bt3qfY7PPvsshg4dmts++OCDo02bNjGvpPD7E088EfPS+PHjy2137959jsekfUoD3akS+8SJE2P++eevdv9TTjkl2ydp165dVp0bAJh35iuOOLzL1Lh5Uof4YVaraBMlsXa7GbFlh+n5HhoAAADQ0gPd6Q8IN9xwQ1x33XXx+eef59pLvyq0uu1S6Q8r6dhOnTrFMcccEyeeeGJ07dq10ccNAAAAAE3ViBEjsuIJZW277bZ5CZLXxzXXXJNbb9WqVRx66KExL7Vv3z7mtcmTJ9d5DB06dKh0juoC3S+99FLcfvvtue2TTz45ll566XqPFwCon56tSuK0rlNjwqyimK+4JNpW/+UaAAAAQAEp/31cBebJJ5+MFVZYIfvjwfDhw7PQdumSvhq07FKTtP+kSZNiyJAhMWDAgHjxxRfn2RwAAAAAoKl56KGHym137Ngx+vfvH4Xg119/jVtuuaVcEL1Pnz7R3HXu3LnSdZiTqVOn1niOUrNmzYojjzwyt7344ovH6aefXu+xAgBzr2srYW4AAABoTgo20J2+enXrrbeOUaNG1RjgLg1416T0mLTf999/H5tssknceeed82AWAAAAAND0PPXUU+W2l1lmmSgUd999d0yYMCG3fcQRR0RLsOCCC5bb/vHHH+d4zNixY3Prbdq0qTbQfeWVV8awYcPKVVGvWN0bAAAAAACov9ZRgAYPHhw33nj7v6TsAAEAAElEQVRjLsg9t8HwO+64I955551sO51v5syZceCBB0avXr2ycDcAAAAAtBSzZ8+O119/fY6B7lS1Of1O7bvvvsvCw8XFxdGjR49YZJFFYrXVVovWrfPzq8err746t7700kvHZpttFi3BSiutVG77o48+io033rja/dM3Fo4cObLc8VX9rnXMmDFx5pln5ra33HLL2HHHHRts3AAAAAAAQAFW6L7mmmvihhtuyNarqsRdusw333zRs2fPOZ7vuOOOi7feeiteeOGFWHvttXMh8RkzZsQBBxwQU6ZMadT5AAAAAEBT8uWXX8bEiRPLtaWgdqnPP/88DjrooKwYwpprrhm77LJLVgX7sMMOy9bT79i6desWO+20U7z00kvzdOxvvvlm9ru+UmlMc1sQolD87ne/K7f9v//9r8b9n3766XLfbLj++utXud8pp5wSv/zyS7berl27+Mc//tEg4wUAAAAAAAo00P3DDz/EqaeeWinIncLbe+21V9x5553xwQcfxPTp07MKM99//322T23+aJP+YJH+wHTCCSfkQt2jRo2KCy64oFHnBAAAAABNyWeffVaprWPHjllF7j/+8Y8xYMCAuOmmm2LcuHHVniP9bu6hhx7KQsbbbbddjfs2VnXu9u3bZwUbWoqVV165XJXuxx9/PL766qtaXaskfWNhRa+88krcdtttue2TTjopq3oOAAAAAAC04ED3JZdckv0xqFT62tYTTzwxC26nMHcKda+wwgr1/jrX9LWwF198cQwePDhX6fv666/P/lgF8P/YuxMwu+b7f+CfmUwiOwmyWULsQglaat9KLbXvO63Yaakqre3XUi1qqdprLUqpSqktlEhJkZTYSQQRsokssicz/+d7+sz9z3InmTUzd+b1ep7z5Nxzz/ku9965c3Puez4nAAAAoA349NNPq21r165dVn37iiuuyIopdOjQIU477bR48cUXY9KkSTFv3rz47LPP4sEHH4xdd9210rGPP/54bLnllvH+++836bi//vrr+Mtf/pK7feihh2aVwtuSX/ziF7n1RYsWZec5853bvP/++ytV8N5nn31i4403rrRPOi49x+VVvFdfffW44IILmnT8AAAAAADQVhVMoDt9AXHXXXdllbPLq3L/85//jCuvvDK6devWqH1dc8010b9//2w9VQ966qmnGrV9AAAAAGipJk+eXG3btddeG4899li2vtpqq8WoUaPihhtuiO233z569eoVyy23XLb9kEMOyYLCN998c6Wr5n388cexxx57ZKHrppLOHc6dOzd3+9RTT4225uCDD86C7OWGDh2aBexTpe302Hz++efxq1/9Ko499tjcPiuvvHLccssteSt4v/HGG5XOmaZzsgAAAAAAQBsOdL/00ksxbdq0LMydvgxKX9DssssuTdJXp06d4vTTT8/dHjZsWJP0AwAAAAAtTb7QdarAXX7e7JlnnsmukrckJ510Uvz2t7+ttO2TTz6JH/7wh9EU0jnDFCIvt/nmm8d3vvOdaIvuvvvu2HfffXO3X3jhhdhmm22yMHYK3V900UVZ8YykX79+2f19+vSp1MaUKVPiwgsvzN3efffd44ADDlji45/C4iNHjoy33347O48LAAAAAAC0wkB3qiKTpDB3CnKnS7w2pf333z+3nioOAQAAAEBbUB7ezudnP/tZrL/++rVq5+yzz46NNtqo0rZHH300Xn311Whszz//fHz44Ye526ecckq0Valaenqc//SnP8Uaa6yRd5+uXbtmBS3eeuut2HDDDavdf95558X06dOz9Q4dOsT111+ft53x48fHySefHH379s3C4ltssUVsvPHGseKKK8agQYOyyu4LFy5s5BkCAAAAAEDrUxIF4v3338+tH3XUUU3e34ABA2KFFVaIGTNmxNixY5u8PwAAAABoCWoK4JaUlMRpp51W63batWuXhbpPOOGEStt/97vfxcMPPxyN6cYbb8ytp3N6hx9+eLRlqShGetzT8uabb8a7774bU6dOzYLcq6++emy77bZZ8DufESNGZFdHLHfOOefEuuuuW22/++67LwYPHhxz5szJ284bb7yRLbfddls8+eSTWb8AAAAAAECBV+ieMGFCbj194bAs9OrVK7tcaAp1AwAAAEBb0KlTp7zbt99++1h55ZXr1NZ+++2XBcEreuKJJ2LBggXRWL744osYMmRI7vZxxx0XnTt3brT2C90mm2ySBdzPOOOMOP7447OrH9YU5i4tLc1C++mcaJKqbv/yl7/MG+Y++uijc2HuVI372WefjVmzZsXEiROzit5dunTJ7kth8nQ+d9KkSU06TwAAAAAAKGQFE+iePXt2bj1dwnNZVbJJ0hcRAAAAANAWdOvWLe/2LbbYos5t9ejRo1p153nz5sWrr74ajSVVgF60aFHufN7JJ5/caG23NTfffHOMGjUqd/uaa66pFo7/5JNP4pRTTsmFvjfeeOMYPnx47LrrrlkF8N69e2fh8ccffzyr0p6MHz8+q+YNAAAAAAAUeKC7PFydlH9B09TKq8ZUrSIEAAAAAG0t0L3BBhvUq72BAwdW2zZy5MhoDOk8YQp0l9t5551jvfXWa5S225qpU6dWqsb9ve99Lw488MBq+1122WWVCmDcdNNNeSui77jjjlm19HKpivqIESOaZOwAAAAAAFDoigvxi6TPP/98mYS5p0+fngXJUyUhAAAAAGgLll9++bzb63uOrFevXnnDw40hhYQnTJiQu33qqac2Srtt0c9//vP4+uuvs/X27dvHH/7wh2r7fPPNN3H//ffnbqfq3Ntss02NbaZK3hWl8DcAAAAAAFDAge7+/fvn1p988skm7y99GVRuzTXXbPL+AAAAAKAlWHvttfNu79q1a6NV/J42bVo0hooB4VVWWSX22WefRmm3rfnPf/4Td9xxR+722WefnbfSeaqwPWfOnNzt3XbbbYntbrbZZrHiiivmbv/rX/9qtDEDAAAAAEBrUjCB7k022SS3fvPNN8f8+fObrK/FixfHDTfckLu9+eabN1lfAAAAANCSpKrL+SxYsKBe7aUr4FVVVlYWDfXhhx/Gc889l7t94oknRklJSYPbbWtKS0vjtNNOyz0nq666alx44YV59x05cmSl2xtuuOFSn/sNNtggd3v8+PExefLkRhk3AAAAAAC0JgUT6N55551z62PHjo1zzz23yfq68sor46233srd3mWXXZqsLwAAAABoSVKl6x49elTb/s0339SrvVmzZlXbtvLKK0dDpaIP5SHkFOQePHhwg9tsi2677bZKQe3f//730aVLl7z7Vg1j9+7de6nt9+rVq9LtKVOm1HusAAAAAADQWhVMoHvgwIHZkqQvav74xz9mlWMWLlzYqP2kyty/+MUvcpWD0pdXe+yxR6P2AQAAAAAt2Xe/+91q27744ot6tZUvwFs15FtXc+fOjbvuuit3e7/99ou+ffs2qM226KuvvooLLrigUmGLgw8+uMb9Z86cWel2p06dltpH586dK92eMWNGvcYKAAAAAACtWcEEupOf/OQnWZg7ha3Tv6kKz0YbbRSPPfZYgy/TOm7cuNh///3jrLPOytoq7+fUU0+N9u3bN9ocAAAAAKClO+igg6pte+edd+rVVr7jttpqq2iIv/zlL/H111/nbp9yyikNaq+tOv/882PatGnZejoHmopdLEnVAPe8efNqFb5fUsAbAAAAAAAosED3scceGxtvvHG2Xh7q/uijj+KAAw6I1VZbLc4+++ws3F2by3YuWrQo3n777bj99ttjt912i3XXXTeGDBmSC3Inffr0iXPPPbfJ5wUAAAAALUkqfNChQ4dK215++eU6t5MqOn/wwQeVtvXs2TMGDRrUoPHddNNNufX1118/dt555wa11xa99tpr8ac//alSMY30WC5Jeu4qqs152KlTpy6xDQAAAAAAIKIkCki7du2yS6lus802MX/+/FzwOoWw0yVfr7vuumxJunfvHmuuuWbu2DFjxsR2220Xs2bNyr5ImjBhQhbqLlde4bs8KF5cXJyFvbt167bM5wkAAAAAzWmFFVbIqnTff//9lSptp3D2euutV+t2UvGFhQsXVtp22GGHZefe6mvkyJFZGLmpq3M/88wzWV9rrLFG9li0pqv4lZaWxmmnnZb9m6yyyipx4YUXLvW4dLXEit59992lHvP+++/n1pdffvmsMAcAAAAAAFDAFbqTVL0nhborfumTQtjlQezyZcaMGfHGG29k96fbs2fPzqoIjR49Oj755JPsi6SK+5e3Ue63v/1t7LHHHs0yRwAAAABobpdffnl07Nix0rbf/e53tT4+nXO7/vrrK21bbrnl4vzzz2/QuG688cbceufOnbOr+jW2H/3oR7H77rvHBRdcEEcccURsvfXWMXfu3GgtUmXuiqH4q6++Orp27brU49LjUPEc6tChQ5e4f7pC4qRJk2o8HgAAAAAAKNBAd3LIIYfEAw88EF26dKm0vTyUXTWcnZQHt/PtV7HSd0lJSVbl+5xzzlmGMwIAAACAlqV///7xk5/8pNK2u+++O4YPH16r42+++eZ4/fXXK21LAelVV1213mOaPn16/OUvf8ndTmHrVPW5MT377LNZ4LmiNI8rrrgiWoNp06ZVCtXvvPPOceihh9bq2H79+sWuu+6auz1q1Khqz3FFt956a6Xbxx13XL3GDAAAAAAArV1BBrqTdJnTdMnTTTbZJBfUrqpiqLumoHe51Mbqq68ew4YNizPOOKPJxg0AAAAAheLiiy+OLbfcMnd78eLFse+++8arr766xONS6PrMM8+stG2fffaJCy+8sEHjSYHyOXPm5G6feuqp0dhSoDufp59+OlqDX/ziF/HVV19l6+3bt48bbrihTsefd955lW6fcsopMX/+/Gr7paD3Lbfckru93nrrxQEHHFDvcQMAAAAAQGtWEgVsnXXWiREjRsTtt98e11xzTYwdOzZvkLsm5UHw3r17x1lnnRWnnXZadOvWbRmMHAAAAABavuWWWy4ef/zx+N73vhdvvPFGrsLzNttsE4MHD45jjjkmNt544+jcuXNWPTudq0tVmR999NFK7Rx44IFZGHtJ5+pqW/W7XAqaDxo0KBpbTcUjatpek/Hjx8eDDz6Y976ZM2dW23bVVVfl3XePPfaIgQMHRmNIBTIqVs1O50Q32GCDOrWxyy67xMknn5x7LlJwO1X5/v3vfx/f/va3Y968efHII49kbS9YsCAXHL/33nuzqyMCAAAAAADVFfwZ9A4dOmSVeFIlmCFDhsSTTz4ZL774YnzwwQdLvWTsDjvskF0i9JBDDsnaAQAAAAAqW2mllWL48OFZMYQUyk4WLVoUN954Y7aUB3YXLlxY7diuXbvGL3/5y/jZz37W4DD3888/H++//36TVudOUjg5X7h6t912q1M7qfjEueeeW+v9a9o3Pf6NEehOgfT0HJaWlma3+/Xrl1Vgr48//OEPMWvWrLjvvvuy2y+//HJstdVW0a5du6yKe0WdOnWKhx9+OAt7AwAAAAAArTTQXS59IZQu95qWJF02NFXBSRWD0nr6omLFFVeMnj17Rt++fbMFAAAAAFi6Ll26xF133ZVVZr766qvjH//4R8yfPz93f9Uwd7qy3sEHHxxnn312dk6uMdx000259dRmKtLQFFJF7BNPPDFuu+223LbNNtsszj///Chkd9xxR/znP//J3U7PYwrc10eqtJ0qbu++++5ZKHzcuHHZ9oph7uLi4thzzz2zcPx6663XCDMAAAAAAIDWq9UEuqtKX+o01pdFAAAAAEBkVZj/+te/xty5c+PVV1/NrpKXiikk6Vxcr169YvPNN4/VVlut0ftO/S4rt956axx55JHx2muvZZWsDzrooDpf4W/HHXfMqmK3FPPmzctV5O7evXscdthhDS6wcfTRR2fLG2+8EaNGjYopU6Zkj9Mqq6ySzT+9HgAAAAAAgDYc6AYAAAAAmkanTp1ihx12yJbWqrXN77TTTmuytjfddNNsAQAAAAAAWnmge8yYMTF48OAa7//1r38dW2+99TIdEwAAAAAAAAAAAABAmwh0P/LII/HCCy9kl/KsKF22NG2bOnVqs40NAAAAAAAAAAAAAKA+iqNAPPfcc5VC3GkBAAAAAAAAAAAAAChkBVOh+9VXX81V507/pkB3p06d4uCDD45BgwbFZptt1txDBAAAAAAAAAAAAABofYHuyZMnx8yZM3NB7mTttdeOoUOHxuqrr97cwwMAAAAAAAAAAAAAqJfiKABfffVVpdsp2H3LLbcIcwMAAAAAAAAAAAAABa0gAt0lJZULiS+//PKx0047Ndt4AAAAAAAAAAAAAADaTKC7Z8+elW7369ev2cYCAAAAAAAAAAAAANCmAt0rrrhi9OrVK3d71qxZzToeAAAAAAAAAAAAAIA2E+hOdtxxxygrK8vWJ0+eHIsWLWrS/kaNGhXDhg3LFgAAAAAAAAAAAACANh3oPvroo3PrCxYsiOeff75J+/vhD38YO+20U+y8885N2g8AAAAAAAAAAAAA0HYVTKB7r732ii233DJ3+8Ybb2zyPlNF8PKq4AAAAAAAAAAAAAAAbTbQndxzzz3RvXv3bP0f//hHPPjgg809JAAAAAAAAAAAAACAthHoXmeddeKJJ56Ibt26ZZWzjznmmLjvvvuae1gAAAAAAAAAAAAAAK0/0J1ss8028eqrr8agQYNi4cKFWah77733jhEjRjT30AAAAAAAAAAAAAAA6qQkCsiwYcNy61deeWXccccd8cADD8STTz6ZLWussUZst912semmm0a/fv2ie/fu0blz53r19c033zTiyAEAAAAAAAAAAAAACjzQveOOO0ZRUVG17WVlZdm/48aNi08++STuvffeZhgdAAAAAAAAAAAAAEArDnRXDXCXqxjyrnpffeULjgMAAABAWzJ69Oh4/vnnY/3114/vf//7zT0cAAAAAACAVqkgA91Vw9blIe60XRAbAAAAABrur3/9axxxxBGxaNGi7PZZZ50V1157bXMPCwAAAAAAoNUpjlZAkBsAAAAAGtcZZ5yRC3Mn119/fbz33nvNOiYAAAAAAIDWqCArdJdX5AYAAAAAGt/UqVNj0qRJ1c7JvfPOO7HBBhs027iA1i1dBWD69Om525tuumnst99+zTomAAAAAIBloSAD3ekk7qhRo5q0j0GDBsWbb77ZpH0AAAAAQEu04oorZstXX31Vafu6667bbGMC2kag+9NPP83dPvbYYwW6AQAAAIA2obi5BwAAAAAAtCxFRUVxzTXXRLt27XLbTjjhhPjWt77VrOMCAAAAAABojQS628glco844ojsi7iKyyWXXNLcQwMAAACghTr66KPjtddei6uvvjqGDBkSf/rTn5p7SAAAAAAAAK1SSXMPgKb117/+NU4//fSYPHlytBbHHXdc3H333U3ez7hx42KNNdZo8n4AAAAAWqpBgwZlCwAAAAAAAE2n4ALdZWVly6SfLbbYIlZYYYUoVJMmTYrTTjstHnnkkeYeCgAAAAAAAAAAAADQGgLdX3/9dfZvu3btmryv2267LQrVfffdF2eeeWZMmzatuYcCAAAAAAAAAAAAALSWQPfyyy/f3ENo0b744os46aST4vHHH2/uoQAAAAAAAAAAAAAArS3QTc3uuOOOOOecc2L69OmVtq+44orx1VdfNdu4AAAAAKAmn332Wbz00kvx+eefR1FRUfTp0ye+/e1vxwYbbFCr499888147bXXYsqUKdG5c+fs+G233TZWWWWVJh/7N998EyNGjIiPPvooOyfXsWPH6NevX2y00UYxcODAWBbeeuuteOONN2LixImxYMGC6NatW6ywwgrRv3//GDBgQKy66qrZ41ob8+fPz+aTnov0eM6bNy9WXnnl6NWrV2yxxRbRt2/faI1KS0vj1Vdfjf/+97/ZFQ+7du2aPXZbb711dm51aebMmRPDhw+P9957L2bPnp0ds+aaa8b222+fvSaaoqjHyJEjY9KkSbnX/UorrZS95rfccsvo1KlTLAvpdZfGMXny5OjQoUM2hkGDBsXGG29c69ccAAAAAAD/n0B3DUaNGpV9KZOkk+8t+UuvE088MZ555plq9x144IHxxz/+Mfsiq7VKX0598sknzT0MAAAAgIL3wgsvxE477bTEfS6++OK45JJLqm2/66674vjjj1/isXfeeWccd9xx2fo777wTP/3pT+Ppp5+OsrKyavt+61vfissuuyz23nvvvG0NGTIkLrjggqydfNL5vKuuuioLhzfGvI899thsjkkK7l566aXx6KOPZiHqfDbccMM49dRT45RTToni4uKljmFpAdgddtghG2eyaNGiuPHGG+Oaa65Z6nmxcePGxRprrLHEfdJ5xT/84Q/x/PPPZ+Hkmmy66aZx2GGHxRlnnJGFiJck9fnpp58ucZ/ajLM2r6uKr60dd9wxXnzxxSXuW/H1ltpPr+d8Y23fvn0cfPDBccUVV8Rqq61W7f4U3k6v0XT+debMmdXuT8HqdDXF9DOTQvYNkcL1qZ/77rsvC57XZLnllsv+oOG0006L/fffv1Zt1/Uxu+eee+JXv/pVjBkzJu++6Y8IfvGLX2TnrNu1a7fU/pf2Wrn77ruzpbbjAwAAAAAoVEv/NqGN+uEPf5h9kbPzzjtHS5W+qEgVf6qGuXv37h0PP/xwtqR1AAAAAGgp/vKXv2QVn5966qkag5ijR4+OH/zgB3HuuedW2p72//GPfxz77rtvjWHuZNiwYVmF5XR+rLGvkpeCzQ8++GAuzJ2Cv1W9++67cfrpp2djSGHlxpIqMm+33XZx1llnNbjIwdixY2PXXXeN3XffPR5//PFqYe6SkpJqFZl//vOfx9prrx33339/FLL03B166KFZWLymMPHChQuzeaY/LkgVuCsaP358Vg37N7/5Td4wdzJ37ty49tprs9fAl19+We+xPvbYY7HeeutlfwBRNcxd9bWXqqw/99xzccABB2Q/Y+mPDxpLavuggw7K/rihpjB3kiq8pz9kSPumYwAAAAAAqB0Vugu4ssd1110Xs2bNqrTtyCOPjOuvvz569uzZbOMCAAAAoPCstdZaceWVV1baVjVQXZMUck6ViMtNnz49O3dVVQoOH3XUUbF48eKsv1QduFevXjFjxox4+eWXs9BwRanKdqr4mwLMSarKXd7uVlttFZtttlksv/zy8cUXX8TQoUNjwoQJuWNTJesjjjgiNthggxg4cGCd5n355ZfH119/XWnbn//856wIRJIqN6cKxN/5zney/lOod+TIkXHLLbdkYe9y//nPf7KiEam69pIqZVd87MorR1cNGqerCabiE2+//XZ2O4V8t9lmm1h55ZVj8uTJ8e9//zs+/PDDqI1XXnkl9tlnn5g6dWpuW2onheVTGHjNNdeMDh06ZO2myt2pInh5qDmFk9M5yBQWTpWa80kVmtNzmqTnK4V8K/q///u/rIJ1uZrOZaYK6xWfm0ceeSRGjBiRrR9++OHZ81++X1JepbtcetzzVZ8+5phj4qGHHsoqSKfq5+n10aVLl+xqiM8++2wWnK/4Wk6PVbqiY3oO0+3ddtst3n///ejatWu2PmDAgCgtLY0PPvggC1Snitrl0uOUXi8vvfTSUiuxV/X73/8+C3JXPE9dHuhP81xxxRWzcHoK5//jH//Ifl7Kx55ejylM/re//W2JFehr+5iln9v0+CcpLJ6WHj16xLRp07Kf3bfeeqvS/n//+9+zn9err756iXOs+FrJ97OX+knhewAAAACA1k6gu5VYZZVVsi+M9tprr+YeCgAAAAAFaLXVVsvCo/UNdKelXKoeXTXQncLB55xzTnTr1i1uvvnmvCHNFEo97LDDKlWLToHPFMxOVbuvuOKK2HDDDePee+/NhXkrVlS+9NJL47LLLqu07Sc/+Um1K9wtbd433HBDpVDpxx9/HCeddFJWtTpVGD/wwAMr7d+9e/csNJuWVD386KOPzkLrSQpmf+9734s333wzOnfunHcMl1xySaXbKVRbNdCdKn6nMHeaf3r8UrC3ohRgTxW0lxagTUHfFAyvGDredttt817tL91Owen0nKRq1Om5KPfrX/86q8D8u9/9rlofKexeLu3zy1/+stpjnoLES5OC1uVh/PR4porXSceOHeMPf/hDFmiuqGqb6XGtGk6+/fbbs9B9CsPffffdWaC/ovTaS6+ZW2+9NbctvRZ+9rOfZSHwM888Mwtzpzmmxzq9nitKofBUnfq1117LbUth+wceeCB7HdclzJ1+XipKr+0Ukq4oBe/THy2kZfDgwdnrL1WoT1L4PFW6T2H0ddddN28/tXnM0mORXh/puUiP2eabb16tnSeeeCJ7naQ/PCiX3gNOPvnkWGeddWqcZ8XXSr6fvdRn1Z9PAAAAAIDWqLi5B0DD/ehHP8ouMSvMDQAAAEBLlSotp6vNDRkypMaKuyl8Wh7aLTd79uwshJuCtP37988qRlcNcyft27fPQsYpTFtRqtxdNRxdV6m6cgr6pivjVQ1zV5UC0BVD5cmYMWOqBXHrIp37S0HaFJpP4eCqYe4khc1T4H1J4dkU8E3VoiuGuVN16ccee6xamLuiVFk6jT89B1Wf0/R8Lkmqap6em4pSIL2uUti/vAJ7eo6rhrlr6/zzz89eP08//XS1MHeSQvdpfCnkXlGqTv3oo49mf0yQQsop5Fw1zJ2svvrq2VhT9e6K0mu4tlJl9xTOr+iMM85Y6mtohRVWyILV6eek4s9PCpKnP26orxTkT4HwFPTOF+ZO0rnpqs9rCuGn1y0AAAAAAG2oQnf6UiR9sZIu+fnVV19ll2lMl7msr1RJpaVLlz5NVWB22WWX5h4KAAAAACzR1KlTs1BqvjByRccff3wWZp02bVpuWwpIp2BqCtUuKXhcXlU8VRMuV1ZWlgVsU4Xrhkhh6lSluzZSZeV0Nb1x48bltqWK0ikIvP7669frsUuVmFOV5xTarUkKdW+99dbx0Ucf5b0/BYIrjilJFbZ79uxZq3FcfvnlWYXyVG29YmA7BeZrqj7ep0+frGp0xeckBZbfeOONSlXdlyY9nuVq+zzkk84dP/XUU9GlS5clBtjPPvvsGD58eG5bOtecKq+n+Vx11VVL7CO9RtO+N910U6Wq6zNnzsyquS9J6idVza4YwO7Vq1cW1q+NFCRPz2nFP5pIVdlTVfKjjjoq6iO9/lJ18qWF6NMfM5x33nm54H2SAubpDy0AAAAAAGjFge50AjxdtjFVQ/niiy+irUlfgrRr1665hwEAAAAAtZJCskuTQsk77bRTFt4ul8Lca6yxRhxwwAFLPf7b3/529OjRI77++uvctjfffDMaKoWxi4trd8HDNIcUXq843xTUTQHfdD6zPlKV5dqEwVMV7R133DFbX2mllXLbJ06cGHfeeWelfddee+1aPablUgg6BeMvuuiiSmHf2267Lc4666wajzvllFMqBbrLA9oVA89L8sknn8QzzzyTrW+00UbVqmfXxfbbb19jlemKdt111yzYnf4goOLr8Kc//ekSw+Dldtttt0rzS+289dZbsc022yzxuL///e/x/vvvV9qWAuw1Bebz2X///bPwdQqvV/yDgvoGutPPVPqZXJr087HzzjtnVczLvfvuu7Fo0aLsZwIAAAAAgJrV7huIFih9oZO+xLnkkkuyih/phHhjLoVAmBsAAACAQvGtb30rO59XG/mCy/vss0+tjk0h3HXXXbfStg8//DAaar/99qvT/j/4wQ+qbbv77rsrVV6uiyOPPLJW+2222WZZhee0pGrNFQPU8+bNq7RvqpydHq+GPg7XXnvtEo9JId+qz8l9990X33zzTa36TAU9yq/G2JDq3OVzro1u3bpFv3796n18vtdwbV6H119/fbVtdQndJ+3bt68WHH/11Vfr/XNQl/433HDDSrcXLFgQ48ePr1e/AAAAAABtSUEGun/7299ml4ycPn16Fr5OXzo09gIAAAAANJ5BgwbVet9evXo16PiqQdwZM2ZEQ/Tt2zd69+5dp2NS9et846hPtfBU+XirrbaKhiivcF01aF1XG2+8caXK3+UVtMeMGbPE46oGsWfNmpWFupcmBeDvuOOObD1VqT766KOjuV6HKSidKoTXRr4w+NJeh6kC+L///e9K23r27BmbbLJJ1OcPKKoaMWJE1LdCd22tvvrq1bY19OcPAAAAAKAtKLhA92OPPRbnn39+VpGlYvi6MStyF1KVbgAAAAAoBFUrNC9Jly5dGnR8Cv5WDQ83RL5qy7WxzjrrNEqotn///pWqbdfVnDlz4rXXXmu0eeU77oUXXljiMalieMeOHSttS1XDa3M+eNKkSdn6YYcdFssvv3w01+swVZhPoe76vAZr8zp8+eWXY9GiRdUe6/oUIKkaum9IoHu99dar9b7du3evtq2hP38AAAAAAG1BSRSQqVOnxgknnJCtVwxyl5SUZJcSXXXVVbNLYSZDhgyJr7/+OtvvmGOOydteOjbtM27cuHjnnXcqVfs+8MAD835xBAAAAADUXb6gZ03atWvXoOPT+cKKFi9eHA2xwgor1Ou4AQMGxIsvvlhp23vvvVfndnr06BEN8cEHH2SVrqs+ximgXB9rrbVWDB8+vNK2t99+e4nHpErThxxySNxzzz25bf/973/j1Vdfje985zs1HnfzzTfn1k8++eRoztdhQ16DtXkd5nsMU9Xuq666KuoqPa5VpUrqTf3679SpU7VtDf35AwAAAABoCwoq0H3TTTflQtopfJ2qnPz85z+PM844o1pllnTpzLRvcueddy617YkTJ8a1114b1113XSxYsCDGjx8fQ4cOFeoGAAAAgEbQ0PNszXmerr7VscuLT1RUfs5yWfRf7quvvsrbZnFx/S7gmC/YnK+PqlIgu2KguzywXVOge8yYMfH8889n66mgx7e//e1oqIa8jpr6NZjvMXzzzTezpTHU57VX13nn+2MMAAAAAACWrn5n7JtBquJx44035sLcqSpNOpn/y1/+ssGX2Uz69OkTV1xxRQwbNix69eqVVTA54ogjGmXsLBupytA///nPOP/882OPPfbIKgWlS4t26NAhlltuuaySTLo86G677Rbnnntu/O1vf3O5TwAAAIBlpPyKe811fEPUN/icL4hdn1BtQ+eeLyjckHByvnnVJtD93e9+NzbZZJNK2x588MGYPn163v1vvfXW7FxwctJJJ0VjaMhj2dSvwdo8hg1R0+Pckn/2AAAAAADaioIJdKfLb06aNCk7gZ9OIKeT+Uu6FGd9pSovf//737NLYj7++ONx1113NXofNK4ZM2bEeeedF3379o299torC+Y/9dRT8fHHH2dfgqSgd6q6nvb78MMP49lnn80uU3rggQdmge9DDz00Xn755eaeBgAAAACtTHkYubnDscuiz9r2kap0VzRnzpxqVbuTdD6v/NxsqnTeFopv5HsMjzrqqOx11BjLe++91yzzAgAAAACgFQW6X3nlldxJ7S222CIL4zaVLbfcMn70ox9lJ7kvuuiiKC0tbbK+aJgpU6bE2muvHb/73e/qVcEmfTH00EMPxTbbbBP77rtvjB8/vknGCQAAAEDhqu/5wdmzZ1fblq48uKytuOKKtRpbbX3zzTe16iOfI488slqF71tuuaXafo888kh27q+mY1qjfI9hvscaAAAAAIDWpyQKxBtvvJFbP+6445q8v8GDB8dNN90UEyZMiGHDhsWOO+7Y5H1Sd6mCT1oq2njjjWOXXXaJgQMHRq9evbJq6+nLn4kTJ8ZLL70Uzz//fMydO7daW0OGDIkXX3wxqwi0zz77NPnYJ0+enPtSqrbGjBlTLZA+f/78pR6X/hCiQ4cO1ban4/NViqpJu3btssez6heaqQp6XbRv377apYoXLVoUixcvrnUbbWlOizt0jbKidrVup3jRvCheXPl1UVZUHIs7dIu6aLdgVhSVVf7CurTdclFa0rHWbRSVLY52C6p/8WhO5lTIc6r6HrFoUd3eL+qitHRx7n2+Lb3vmZM5LYk55WdONWsrc2qNv3PN6X/MqXDmVFVd3yPqrays0rmBpnzfK59Xbc5FLGmf2rzvLW28S5tT1cc+9ZeOqe17edXxzZw5M5tTXd/Lv/7662rbunfvnvW7pOepaoA83c73mNb291OqcJ0vKJzOkaXXTF1/P+WbV76ger7nKfV12GGHxe23357b9u6778Zzzz0X2267bW5OFUPeqfhGbV535Wp6npK6tJNv/vX5econ3/O0/PLLV9tv+vTp9Xrt1WRp7xFpXFXV5bWXxpNvjPnaWNLzVC49Rkt7ztrK573GmlNr+J3bGj9HmFPbmVM+i9p1zsZUW8WlC6JdaeX327IojkUlnaMuShbNiaKoPKfFxR2itHjJ780VpcekZHHl7+kSczKnQpnT0j6X11Zb+RxhTm1rToX+O7c1fo4wp7Y1p6oqvkcsbMLvp6v+P7wtve+ZkzktiTm1zjktyHMus7kVTKC7YvXlrbfeusn722STTbIT6OnLmn/84x8C3QUghbAvueSSGDRoUI37nHfeeVkA/LrrrovLL7+8WoWbGTNmxP777x+33npr/PCHP2zS8d54441x6aWXNqiNzz//PDp16rTU/dIb54ABA/IeX5c3ppVWWilbKkpvnOPGjYu6WHPNNWO55Zar9uXU1KlTa91GW5rTN2tsF4s7Vv9CryadJr2dLRWl/6zMXG+PqIvuHzwZJfNnVNo2f6V1Ym7vjWrdRrt5M2L5D5+stt2czKmQ51T1PWLSpMnRVGbOnJXrqy2975mTOS2JOeVnTjVrK3Nqjb9zzel/zKlw5lRVXd8j6iudiKz4ntaU73vl86pNW6lIQk1q8763tMduaXOqes4nnbRNx9T2vbxqqDUVCkj91fW9/L333qu2LT0/qd8lPU/z5lX+4i7dzjff2v5+SuNOJ7MrngBPr53hw4fHWmutVeffT/nmla+Nmp6nvfbaq1KgO7n22mtjlVVWyeaUjkmFF8qvpti/f/86vYZrep6SurSTL9xcn5+nfPI9T/kqdH/44Yf1eu3VZGnvEWlcVdXltZd+Vqoq//mpy/NU8Wd5aY93W/m811hzag2/c1vj5whzajtz6hUfVNv+VY9BsbB97a9E0X3WmOj+zdhK21KgdtLK20Rd9J7y72i/qPJnptmdV4uZ3daudRvtF34Tvaf+u9p2czKnQplT56V8Lq+ttvI5wpza1pwK/Xdua/wcYU5ta05VVXyPmDK5bsUj6yLl5Cq+F7Wl9z1zMqclMafWOafPP/88Wpra/3lqM6tY9WX11Vev07F1SftX1KdPn+zfV155pV7Hs2yky60++OCD8dhjjy0xzF2uc+fOcf7558c777wT6667brX70193pArtf//735toxAAAAAAUko8//rhex33yySfVtm266aaxrHXs2DG7qt3SrgbXkMejLkU41l9//aygRkXPPPNM7hxwxercJ598crQV6dxmCt5X9OWXX8bs2bPr1V760uTll1/OLUv6QwsAAAAAAJpXwQS60+U/l3TpyXxfUlT866H6SMHeFAafNGlSvY6n6aXLxQ4dOjQOOeSQOh+b/jAgVfpZZ5118j73xx13XKNU2wEAAACgsE2ZMqXOFdfTeaWqx3Tv3j3WW2+9aA7bbFO9cuJ//vOfOrfzwQcfVCq+UX6eLVU/qYvDDjusWvj4b3/7W1aN/J577sm2rbDCCnHooYdGW5HOaW+++eaVtqXz0yNGjKhXe//+97+zqxCWL/n+wKClSZdpBQAAAABoi0qiQKQvO8rNmTMnq8pc2/2/+OKLWoXAqyoPck+ePLnOx9J0dt1119zzf+CBB2aXXa2vVIX9zjvvjO22265aJfcZM2bEGWecEY8//ng0hVNPPTUOPvjgOh2Tqibtt99+udurrrpqrb4sq+mLkHR8XSrYt2vXrtq2VDWorl/YVa00VP4FXQro11ZbmlPXT16KsqLq46xJ8aLKl2VO2i2YlfcS4EuSjqlquakfRfvp42vdRlHZ4rzbzcmcCnlOVd8jps5quj/+6d69W66vtvS+Z07mtCTmlJ851aytzKk1/s41p/9/TFXmVBhzqut7RH2l94SK72lN+b5XPq+GnIuo7fte1csr5hvfksZR9dxhSUlJnd7L0/5Vvfnmm/Gd73yn1nPKd/W3Y489NisusLTnqWKxivLb+eZblzmde+65cdttt2WB6XLDhg2LVVZZpdZzSu6///5q237yk59k/S5pTlWdcsopceWVV8a0adNy2x599NHYcMMNc9uOOeaY6NSpU9ZOY/zOTerys5DvdVCfn6d8anqefvazn8UBBxxQadvw4cPjxBNPrPPP00UXXZRb79mzZxair3rJ0apzyvc81uW1V37Vy4rStnxt5Hueqo4vPf81Pd4PPfRQ9m9xcXFsu+220b9//7xzai2f9xprTq3xd645mVNBzWntXtW2r/j1f6OsqPY1uIpLq1+2umTRnOg95d+1bqP8mKq6zBkfnebVvtBVUVlp3u3mZE6FMqe6foZt658jzKltzangf+fmYU7mVMhzqvgeMXH6R9FUUt6u4ntRW3rfMydzWhJzap1zmluhyHRLUTCB7oqB7M8//zy7LGdt93/11Vdjgw02qFN/o0ePzlX2TtWaaTmOOuqobGnM6kQnnHBC/OlPf6p23xNPPJFVsslXwaihevXqlS0N0aFDh7xfwtTl+IZKX5g0ZAwVvyDL9yVZXbXGObVb8E2D20gntkrmz2hwO8WL52dLQ5lTfuZUGHOq+h5RUpI/6NIYiovbLfX9qDW+75lTfuZUM3OqmTm1nTm1xt+55lQzcyqMOTXWe8RSFRUt9T2tsd73kjSn2rS1pH1q875XU6C8tnOqegI4nVhOx9T2ecp3IvqOO+7I/kC/NnNauHBh3HzzzdXGfNppp9U47opzqjrWujyHNc1ptdVWy86B3XjjjZWqiKfgeb4q2Pmep9mzZ1eb18orrxyDBw/O2+eSxp22p4D7Nddck9s2duzYOO+883K3TzrppCXOqT7q8rOQ73XQ1J8jUjGHjTfeON56663ctkceeSQuvfTSWHfddWv98/Tee+9lFc/LnX766dGlS5e8+1acU74x1eW1l288dTmHWbFQSzJ//vy8x6bXYgr8l0tzrRjobo2f9xprTq3xd6451cycWuKcqn8vVLK4eri1roqiNNovavhj0650QbY0lDnlZ04tb051/Qzb1j9HmFPbmlPh/86tzpzyM6fCmFPF94j2Tfj9dDqnVpv3otb4vmdO+ZlTzcyp8OfUoRHG2dhq/+epzWyttdbKrb/88stL3b9iOv8vf/lLnftL1WrK9ejRo87HU1jSFxo1ufbaa5fpWAAAAABoeUaNGhW33nprrfa96qqr4tNPP6207cwzz4z11lsvmtPll18eAwYMqLTt5z//eUydOrVWx19wwQUxZcqUSoHnVCShc+fO9RpPeWC7ovL20xX1UrXutiY9pnfddVelLyvSHwikCt0p3FwbqbJMCjsvXrw4V/H+xz/+cRTa9wDJhAkTarySYUU1VZoHAAAAACgUBRPo3mijjXLrd99991L333TTTbN/U+n2Z599Nl566aVa9zVy5Mi4/fbbcxVY+vXrV68xUzjS66X8NVPV448/nlV8AQAAAKBt2n777bPqxmeccUZW0XpJ7r///vjlL39Zadvaa68dl112WTS3dFXDhx56KDp27Jjb9sknn8Q+++wTEydOrPG4dI71N7/5TVx//fWVtv/0pz+NH/zgB/UeTwq477zzznnvO/nkk6Ot2myzzeK6666rtG3YsGFx8MEHx9dff73EYydNmhR77bVXvP7665UC4oVStOS73/1utStp5js3W7GIS/qDgprO7QIAAAAAFIplcN3XxjFo0KDc+vDhw+PBBx/MeynQct/5zndyJ6xLS0vj8MMPjyeffDK7XOXSwtz77rtvLFiwIPuiIh2/1VZbNeJMaKl22mmneOONN6ptnzdvXrzwwgvZFyEAAAAArdX48eOzc25Lkq6cl6pPl1tttdWyc3QzZ86sVL06X+j0qaeeqlQJeo899oiBAwfm7bs8jFr1inoVQ6kpTFyx7bfffjt3+5133ql0bBpfxXGnYHOqeFxb6WqAgwcPjqOOOir233//bM4/+tGPsnOQ3bt3j1mzZmXnFW+++eZqj+Hqq6+eFZxYUhXrdEx6DMpVXC+/XXH8+R6D2tp8883j+eefz0Lc5c/HK6+8kp03TVWcDzrooGy+7du3z6pl/+tf/4o//vGP1Qpm/OIXv4hf//rX0VApuJ3GU1GqKH3ggQfWua30+qx4dcd8V3qs+jgu6XW0tOchFSH5/ve/X2Pb+cZXcZ+tt946W2qqXp4uK3rqqafGokWLsm3/+Mc/Yv3118+uNrjffvtl1azT6ypV7n7rrbfiscceixtvvDGmTZuW7Z+O/8Mf/rDE85oNfcyq/uyPHTs27+u74s90+vlJ7x35HHbYYfGzn/0sV408VRtPP6tpDKnwSvpZSwH1q6++OnfMcccd1yIvjwoAAAAAUBdFZSm1XCDSCdtUYSQNuVOnTvHnP/85+wKlJt/+9rezL1JSKLv8mHQi/IgjjsiqnKQT2km677XXXstOBN95553ZyeLyY9K/f/vb37KQd6EqrzRe1cUXXxyXXHLJMh9PS5VeT0cffXTe+1JVpV/96lfR3NKXkRWr1acvmMq/+KR1efvg/D+3QPPY6K81f1wa8d+h8fs/1T1EURu7bnNQDD68cmU/ACjnMyMUzmfG5JDTm6Z6bLeuPeJPV/yrUdpKf9Ce/uC9LnbYYYfsuFTlOYWA6yKdh0tBzPr2XfG0ZmqnNlf1K9e/f/9szDVZY4014tNPP83dPvbYY7Nzh3fccUeccsopWTGIcin4vHDhwrztpMD3Aw88EAMGDFjieHbcccd48cUXo64acmo3BW/TudLnnnsu7/nEdu3a5YLEFfXt2zcL16ZzrI0hPXYp9F6xQngKDF955ZV1biud67z00kuX2euo/HWxtPOwNanN+dn0/Jx22mnxwQcf5L1/ueWWy4WfK1pllVXipptuWmoF9YY+ZvX52U9/JJBe8zVJ1cnTHxfUZq7rrrtu/Oc//4kVVlihTmNoa3xuhJZjxQ2X/JkAWLb6Xlr9j9GgrfKZEQrnXOO/X38qrrvr503S7/e3PyxOOKRp2gZoad5pgVnM/yWaC8Ruu+2WC1mnyhypWkyqLvLwww/nrfpzzDHH5NbLj0kng7fccsss3N2nT58sJJ4uMZou5XjLLbfkwtzlVl111QZdNpTCsaTq7TV9YQIAAABA23HCCSdkV3g7+OCDsyB3ki/MvcEGG2SVkVPl66WFuZtLqu48dOjQePrpp2PvvfeuVEE8nYOtGubeZJNN4je/+U2MGTOm0cLcSXocf/jDH+Zup3OzKWjO/+yyyy7ZFwmpCnaq5p2C9hVVDThvuOGGWeD+vffeK9jz2meddVbce++91YLiFeeawt3p53HEiBHC3AAAAABAq1BQFbqfeOKJ7CR0eeC6PNydnHPOOfG73/2u0v6pUk5KzH/88ce5bUubbtW20+VE02U/C5kK3bWTLpmaqgHlk/4IIH050Nxa4l+F0DT8BTS0LCp0A9AS+cwILUtrqNDN0it0V/TNN99kge2PPvoopk+fngVMU/GIVDSg4vmbQpHCsun8VzpHNmXKlOz2SiutFL17947NN988mxvNLxU2Sc9TqmienqfS0tIs0JwKk2yxxRZZEZPWIs1t9OjR8d///je++uqr7DXZvXv3WHvttbMCLYLctedzI7QcKnRDy6JCN/x/PjNCy6JCN0DbzGKWRAH5/ve/n52Qrnj5zSUFtDt06BA33nhj7LnnntnJ37pe9nKPPfYo+DA3tbf88svXeN/MmTOX6VgAAAAAaNm6du0a3/ve97KlNUiB9B122KG5h8FS9OjRIztv3RYUFxfHpptumi0AAAAAAK1dQQW60+UkL7300nj++eer3Tdo0KC8x6QvVP70pz/F8ccfX+t+Ukh8p512igceeKBB46WwdOnSpcb7ql66FAAAAAAAAAAAAADaXKA7OfHEE7OlLo455pissvepp54aH3/8cd5K3eWVvjt27Bg//vGPs+B4+/btG3HktHTpMrk1Sa8LAAAAAAAAAAAAAIi2Huiur9122y3efvvt+Pvf/x6PPPJIjB49Or788stYvHhxrLzyyrHOOuvE97///Tj88MOjX79+zT1cmsGMGTNqvK979+7LdCwAAAAAAAAAAAAAtA1tJtBdXmX5sMMOyxao6quvvqrxvtVXX32ZjgUAAAAAAAAAAACAtqFNBbopbCmIP2LEiNztDz74IJZbbrlGa//NN9+s8b711luv0foBAAAAAAAAAAAAgHIC3RSMiRMnxqeffpq7PW3atOjbt2+jtT9y5Mga79tyyy0brR8AAAAAAAAAAAAAKCfQTcH67LPPGjXQ/dxzz+Xd3rFjx9hhhx0arR8AAAAAWo7x48fHgw8+WGnbzJkzK91+55134qqrrsrdXm211eLQQw9dZmMEAAAAAABaN4FuCtYLL7zQaJWzX3nllXjvvffy3rfXXntF165dG6UfAAAAAFqWsWPHxrnnnrvEfV5//fVsKZf++F+gGwAAAAAAaCzFjdZSKzNo0KBo165dlJTIvDdEql607777xvLLL5+Fonfbbbd47bXXGqXthx9+OBrL9ddfX+N9Z511VqP1AwAAAAAAAAAAAAAVSSsvQVlZWXMPoaCNGjUqq1b0zTff5LY9++yzMWzYsHj88cdj1113bVD7qSrSE088kVXQbohnnnkm/vKXv+S9b4899ojtttuuQe0DAAAA0HLtuOOOzgMCAAAAAADNSoVumszgwYMrhbnLzZ8/P0444YRYsGBBg/s46aSTYsKECfU+/tNPP40f/ehHee/r1q1b/OEPf2jA6AAAAAAAAAAAAABgyQS6aRLjxo2LkSNH1nj/+PHjY8SIEQ3uJ4W5d9ttt3j//ffrfOwHH3wQ22+/fTaWqoqLi+POO++MtdZaq8FjBAAAAAAAAAAAAICalNR4DwXh448/zpb6Hjt06NAl7rPrrrvWq+0vv/xyqfs0pLJ2Re+++25svvnmcd5558Upp5wSK6+88hL3nzFjRvzud7+Lq6++OqsWni/Mfcstt8SBBx7YKOMDAAAAAAAAAAAAgFYR6B40aFCMHj26uYfRotxzzz1x6aWX1uvYe++9N1uWpKysrF5t9+7de6n79O3bt05tDhgwIF588cW8982ZMycuvvjiuPzyy2ObbbaJrbfeOquu3aNHjygpKYkpU6ZkIfN//etfWRsLFizI207aPz0me+21V53GBgAAAAAAAAAAAACtPtDdkIAxy1YKU2+yySbx5ptv5r1/1VVXzULXdXHHHXfEj3/846x69p///OeYOXNmtX1Sxe3nn38+W+qiqKgoDj744Lj22mvrHDQHAAAAAAAAAAAAgPoqjgKTgrfLaqFhbrvttujSpUu17cstt1wWzu7QoUOd2/zWt74Vf/zjH7Nq24888kgcf/zx0adPn3qPsVu3bvHDH/4wRo0aFQ8++KAwNwAAAAAAAAAAAADLVEFW6G7ssHV51e9CDHFfcskl2dISffvb345XXnklLrjggnjxxRdj8eLFsc0228Rll12W3dcQnTt3jgMOOCBb0vM3evToeOONN+Ktt97Kls8//zxmzJiRLXPmzMlC5Cm8nQLbAwYMyKqHb7vttrHddtvVK1gOAAAAAAAAAAAAAG0y0F01hN0YyoPcjdkm/7PxxhvHP/7xjybtIz1/KaCdFgAAAAAAAAAAAAAoJAUb6P7Xv/5Vr+NSteZZs2bFxIkT45133on//Oc/WXXnpLi4OA4//PA44YQTsnUAAAAAAAAAAAAAgKZUsIHuHXbYodHa+uijj+Kaa66J2267Le6///4YO3Zs3HfffbHmmms2Wh8AAAAAAAAAAAAAAFUpQx0R66yzTtx4443xyiuvxMorr5xV7d5qq63iv//9b3MPDQAAAAAAAAAAAABoxQS6K9hiiy3iueeeiw4dOsSUKVNijz32iM8//7y5hwUAAAAAAAAAAAAAtFIFFegeOHBgbL755rHZZps1aR/nnHNOtp5C3YMHD26yvgAAAAAAAAAAAACAtq2gAt1//vOf47XXXsuWpnT88cdn/5aVlcXTTz8dL7/8cpP2BwAAAAAAAAAAAAC0TQUV6F5W1lprrejZs2cUFRXlguQAAAAAAAAAAAAAAI1NoLsGvXv3zq2r0A0AAAAAAAAAAAAANAWB7hrMnTs3+7esrCw++eST5h4OAAAAAAAAAAAAANAKCXTnsWDBgvjyyy+rhbsBAAAAAAAAAAAAABqTQHceQ4cOjfnz5+du9+jRo1nHAwAAAAAAAAAAAAC0TgLdeapzX3jhhZW29erVq9nGAwAAAAAAAAAAAAC0XgLdFXz00Uexxx57xH//+98oKiqKsrKy7N+tttqquYcGAAAAAAAAAAAAALRCJVFAJk6cmFXQbqjUxty5c2P69Onx2Wefxfvvvx8vvvhivPLKK1FaWlpt/+9///sN7hMAAAAAAAAAAAAAoKAD3al69ujRo5us/VSRO0lVucv/XWeddWK//fZrsj4BAAAAAAAAAAAAgLaroALdFUPXTaE8yF3eT0lJSVx//fVRXFzcZH0CAAAAAAAAAAAAAG1XwQW6K4aum0oKc7dv3z5uu+222G233Zq8PwAAAAAAAAAAAACgbSq40tNNVaE7tVu+bLfddjFy5Mg45phjmqQvAAAAAAAAAAAAAICCrNDdFMHuVVZZJTbZZJP49re/HYcddlist956jdY2AAAAAAAAAAAAAECrC3SPGzeuQce3b98+unXrFl27do2ioqJGGxcAAAAAAAAAAAAAQKsPdPfv37+5hwAAAAAAAAAAAAAA0CDFDTscAAAAAAAAAAAAAID6EugGAAAAAAAAAAAAAGgmJVFA+vbtG9OnT2/uYQAArdCCouKY1q5D3vu6fv5+jcdN/mpCk41p1uzp8ckS+q7JCt1XyhYAAAAAAAAAAKDlK6hA9z//+c/mHgIA0MosjqIY0n3VeKdjjygtKsq/0xWHRXP4zxtDs6U+1lp9wzjnxN/HSj36NPq4AAAAAAAAAACAxlPciG0BABScoV37xludetYc5i5QYz97N35z4+lRVlbW3EMBAAAAAAAAAACWQKAbAGjT3uu4fLRW478cE19M+qS5hwEAAAAAAAAAACxBSRSQnXfeOft37bXXjltvvTVaksGDB8eYMWOiqKgonnvuueYeDgBQSx3KSqM169Sxc3MPAQAAAAAAAAAAaC2B7hdeeCELTE+fPj1amtdeey3efPPNbHwAQOFYa/6smFrSMVqjVfsMiJ4r9G7uYQAAAAAAAAAAAEtQvKQ7AQBau+/OmRLtWmmV7gO+f2JzDwEAAAAAAAAAAFgKgW4AoE3rXrowNps7LVqbVXqvGVtvtltzDwMAAAAAAAAAAFgKgW4AoM3bZvbkVlel+8A9BkdxcbvmHgYAAAAAAAAAALAUJVGAxowZEzvvvHO0tDEBAIVdpfu1zitFa6A6NwAAAAAAAAAAFI6CDHTPnj07XnzxxeYeBgDQyqp0j+rUMxYXFf4FTFTnBgAAAAAAAACAwlGQge6ysrJoaYqKipp7CABAA7SWKt2qcwMAAAAAAAAAQGEpyEC38DQA0BRaQ5Vu1bkBAAAAAAAAAKCwFBdide6qS232qc1xdTm2tu0BAIVXpbtQqc4NAAAAAAAAAACFp6AqdF988cW59UWLFsXtt98ekydPzoWpU+Xu/v37x8CBA6Nfv37RtWvXbCku/v+59dmzZ8esWbNi6tSp8e6778ZHH32UtVV+fNr32GOPjdVXX71OY7v55ptj0qRJjTZXAKB5FHKVbtW5AQAAAAAAAACg8BRkoPuTTz6JAw44IBfm3nbbbeO4446L/fbbL3r27FmnNufNmxdPPvlkPPDAA/HII49EaWlp9u8dd9yR9VFbf//73wW6AaAVVel+rfNKUUhU5wYAAAAAAAAAgMJUcKUnU0XtrbbaKt58883o27dvDBkyJIYNGxYnnHBCncPcSceOHWP//fePhx56KF5//fXYbLPNYubMmXHIIYfEfffd1yRzAABafpXudmWlUUhU5wYAAAAAAAAAgMJUUIHuOXPmxJ577plV5l5nnXXi5Zdfjr333rvR2h80aFAWDt9ll12ySt0/+tGP4q233mq09gGAwqrSXShU5wYAAAAAAAAAgMJVUIHuX/3qVzF27Nho3759PPDAA7H66qs3eh+dO3eOBx98MHr16hXz58+P008/vdH7AABavkKq0q06NwAAAAAAAAAAFK6SKBCzZ8+Om266KYqKiuLwww/Pqmk3lZ49e8bPf/7zOPvss2P48OExcuTI2HzzzZd4zMknnxwTJ05ssjEBAM1Tpfu1zitFS6Y6NwAAAAAAAAAAFLaCCXQ/++yzMXPmzCzQfeihhzZ5f4ccckgW6E7+9re/LTXQfdJJJzX5mACAZV+le1SnnrG4qOVe1ER1bgAAAAAAAAAAKGwtN51URaqUXW6jjTZq8v769euXVepOXnnllSbvDwBouVW6WyrVuQEAAAAAAAAAoPAVTKD7ww8/zK336dNnmfTZq1evKCsri48++miZ9AcAtMwq3SUl7aMlUp0bAAAAAAAAAAAKX8EEuqdN+//VMadPn75M+pwxY0a1vgGAtlele5etD4iWRnVuAAAAAAAAAABoHQom0L1gwYLc+rhx45q8v7lz58akSZOy9YULFzZ5fwBAy7Xf905ocVW6VecGAAAAAAAAAIDWoWAC3d26dcutP/LII03e39/+9rcoLS2t1jcA0Pas2KN3i6rSrTo3AAAAAAAAAAC0HgUT6B4wYED2b1lZWdx555256tlNIVXkvuqqq7L1oqKiXN8AQNvVkqp0q84NAAAAAAAAAACtR8EEurfaaqtcwPqrr76Ko48+OhYsWNAkff34xz+ON998s1rfAEDb1VKqdKvODQAAAAAAAAAArUvBBLr33nvvKCkpyVXpfu6552L77beP8ePHN1ofM2fOjIMOOihuvvnmLDhebv/992+0PgCAwtUSqnSrzg0AAAAAAAAAAK1LwQS6e/fuHQcffHAW5k5h6/Tvq6++Guuuu26cdtpp8fbbb9e77U8++SQuvPDCWGONNeLRRx/N2k5SPxtttFHsvPPOjTgTAKBQNXeVbtW5AQAAAAAAAACg9flfyesCcfnll8eQIUNizpw5uVD3/Pnzs4raaVl11VWz8PX6668f66yzTvTt2ze6dOmSLWn/2bNnZ8dOnjw5Pvroo/jggw9i2LBh8eGHH2btVwxylwfHf//73zfzrAGAllal+7mX/xaLFi1c5n2rzg0AAAAAAAAAAK1PQQW6+/fvHzfeeGMce+yxWdg6LRWD2OPHj4977rmnTm2WH5uUt1e+/tOf/jR22WWXRhs/ANB6qnQ/PezBZdqv6twAAAAAAAAAANA6FUeBOfroo+OKK66oFsQuX9L2uiwVjy2Xtqd+fvvb3zbTLAGAll6lu6Sk/TLtU3VuAAAAAAAAAABonQou0J387Gc/i7vuuiu6du1aKdidVAxo12apKLXVrl27uOSSS7L2AQCWVKV7WVGdGwAAAAAAAAAAWq+CDHQnxxxzTLzzzjux++6756pt11f58YMGDYrXXnstLrrookYdKwDQ+izLKt2qcwMAAAAAAAAAQOtVsIHuZNVVV40nn3wyhg0bFocffni0b98+F85eUsC74j6pSveee+4ZQ4YMiddffz022WSTZToHAKAwLasq3apzAwAAAAAAAABA61YSrcC2226bLTfccEMMHz48RowYkVXanjBhQkyfPj2+/vrrLLy9wgorZEvv3r1j8803j6222iq222676NOnT3NPAQAo0Crdz738t1i0aGGT9aE6NwAAAAAAAAAAtG6tItBdrkePHvGDH/wgWwAAllWV7qeHPdgk7avODQAAAAAAAAAArV9xcw8AAKDQq3SXlLRvkrZV5wYAAAAAAAAAgNZPoBsAoBGqdDc21bkBAAAAAAAAAKBtEOgGAGiBVbpV5wYAAAAAAAAAgLZBoBsAoIVV6VadGwAAAAAAAAAA2g6BbgCAFlalW3VuAAAAAAAAAABoOwS6AQBaUJVu1bkBAAAAAAAAAKBtKYk24Jlnnol//etfMWrUqJg6dWrMnj07VlxxxejVq1esvfbasccee8T2228fJSVt4uEAAJqwSvdzL/8tFi1aWO82VOcGAAAAAAAAAIC2pVUnmB966KG48MILY8yYMbltZWVl2b9FRUW5bb///e+je/fucf7558dPfvKTaN++fbOMFwBoHVW6nx72YL2OV50bAAAAAAAAAADanmYPdJ9zzjnx3nvvLXWfXXbZpdZtlpaWxkknnRR33HFHLsBdUQpzV90+Y8aMLNB9++23x5AhQ2L99devwywAABpepVt1bgAAAAAAAAAAaHuaNdC9YMGCuOmmm2L+/Pl570+h6xS+Puyww+rU7imnnBJ/+tOfqlXirijf9tRfqua94447xr/+9a/YYIMN6tQvAEB9q3Srzg0AAAAAAAAAAG1TcXN2/sorr8S8efNyYerypSHuuuuuuO2227LAdnlou7btlh8zefLk2HPPPWPOnDkNGgsA0HardJeUtK/TMapzAwAAAAAAAABA29Ssge7nn3++Wpg6LeUB7DXXXDP22Wef2HDDDWvV3tSpU+OnP/1p3iB3xXbTes+ePWPllVeutL2izz77LC699NJGnS8A0LaqdNeW6twAAAAAAAAAANB2NWuge/jw4ZVup1B1+/bt4+yzz4633norxo4dG48++mhsscUWtWrvyiuvjGnTpuXaSiqGxHfZZZd44oknYubMmTFlypSYOHFiLFiwIEaMGBFnnnlmlJSU5I5J+1977bUxfvz4Rp83AND61aVKt+rcAAAAAAAAAADQdjVroHvUqFGVKmRvtNFG8fbbb8dVV10VAwcOrFNbs2bNiltuuSVXnTspbzu55JJL4tlnn4099tgjOnfunNunuLg4vvOd72Th7f/+97+x2mqr5e5btGhR3HfffY0yVwCgbaltlW7VuQEAAAAAAAAAoG1rtkD3xx9/HDNmzMjdXnfddeOFF16Itddeu17t3XvvvVnl7SSFuMvD3OnfAw88MC666KKltrHhhhvG0KFDo3v37rnj//znP9drPAAAtanSrTo3AAAAAAAAAAC0bc0W6B45cmT2b3no+uabb46ePXvWu70HHnggt16xSnfHjh3j6quvrnU7KVB+8cUX5yp7v/fee/H555/Xe1wAQNu1tCrdqnMDAAAAAAAAAAAlzdXx+++/nwtfb7311rHjjjvWu63x48fHyy+/XCnIXR4UP+qoo2K11VarU3unnXZa/N///V9Mnz49u/3mm2/GqquuWu/xAQCF7cuL16r3sVsvLoqh0SUWx///nFJul9nvxqRL123g6KBt6Xvp2OYeAgAAAAAAAABA66jQ/fHHH+fWU+i6IR577LFcRe3yfyuGs+uqffv2ccAB/7+aZgp0AwDUxwrtymL3TguqbV+7ZFEM6rCoWcYEAAAAAAAAAAC0HM0W6B43blxuPVXoboh//vOfufVUlbs81L3xxhvHt771rXq1ud122+XWx4wZ06DxAQBt226dFsTunebHCsWl0bGoLAZ1WBg/6jY3iqsX7QYAAAAAAAAAANqYkubqeMKECblq2BtssEG925k7d2688MILWZC7onT7kEMOqXe7KQxebsaMGfVuBwAgfUzZs/OC2KPTgiiNiHaC3AAAAAAAAAAAQHNX6J41a1b2b8+ePaOkpP658ueeey7mzZuXrZdX5i530EEH1bvdVVddNbc+c+bMercDAFAx2C3MDQAAAAAAAAAAtIhA9zfffJNV0e7WrVuD2nniiSdy6xWrdK+99tqx7rrr1rvdiuMqD58DAAAAAAAAAAAAALSKQPeCBQuyf5dbbrkGtfP4449XCnKnKt3p9l577dWgdhs6LgAAAAAAAAAAAACAFhvoThWwU/h65syZ9W5jxIgRMWHChGw9tVXR3nvv3aDxVazK3bVr1wa1BQAAAAAAAAAAAADQogLd3bt3z/6dNm1avdt4+OGHc+sVq3SntnfYYYcGjW/GjBm5dYFuAAAAAAAAAAAAAKBVBbp79OiR/Tt79uwYN25cnY9ftGhR3HfffZWC3KlKd7q92267Rbt27Ro0vo8++ii3vsIKKzSoLQAAAAAAAAAAAACAFhXoHjhwYG59+PDhdT7+sccei0mTJuWC3BUddNBBDR7fyJEjc+trrLFGg9sDAAAAAAAAAAAAAGgxge7NNtsst37vvffW+fjf/va3ufWKVbq7dOkSP/jBDxo8vqeffjq3PmDAgAa3BwAAAAAAAAAAAADQYgLdW265Za669nPPPRcvvPBCrY996KGH4vXXX8+C3OXVudO/6fZ+++0XHTt2bNDYPv7440rjWWuttRrUHgAAAAAAAAAAAABAiwp0b7311tG/f/9cKPu4446L8ePHL/W4sWPHximnnFKpKndFZ555ZoPHdu655+aC4qmfjTbaqMFtAgAAAAAAAAAAAAC0mEB3cvTRR+cqa3/22WdZyPvRRx+tcf+//vWv2T5ff/11drv82PJ/d9hhh9hiiy0aNKYbbrghG0N5YHy99daL5ZdfvkFtAgAAAAAAAAAAAADkUxLN6PTTT88C1DNmzMgC1BMmTIiDDjoo+vXrFzvttFP2b/n2F154Ifu3aoi7XFq//PLL6z2W+fPnx//93//FFVdcUan97373u400WwAAAAAAAAAAAACAFhTo7tWrV/zmN7+JU045JQtPlwepU3D7vvvuq7Rv2p5UDHGXb0/bTjzxxNhqq63q1P+iRYvi9ddfj2effTZuvvnmmDhxYrWg+DbbbNOgOQIAAAAAAAAAAAAAtMhAd3LSSSfFiy++GH/5y19yoe6KAe5yFUPWVUPXm2++eVx99dW17vOqq66KX//61zFr1qxKbVbtJ63vscce9ZwZAAAAAAAAAAAAAEALD3Qn9957b1Yt++GHH84FqqtW4q6oYuh70KBB8fjjj0fnzp1r3d+MGTNi5syZedusePvb3/529OnTp46zAQAAAAAAAAAAAAConeJoAdq1axcPPfRQ3HnnnbHSSitlQe2qFbrLld+XAtw///nPY8SIEdGrV68Gj6G83Yp977fffg1uFwAAAAAAAAAAAACgRVfoLnfsscfGwQcfHE888UQ8+uij8dZbb8XEiROzitrdu3fPwt6bbbZZ7LDDDnHYYYfF8ssvX69+dt555ygpWfrUjzzyyHq1DwAAAAAAAAAAAABQcIHuJFXeTqHutDSVnXbaKVsAAAAAAAAAAAAAAJpTcbP2DgAAAAAAAAAAAADQhgl0AwAAAAAAAAAAAAA0E4FuAAAAAAAAAAAAAIBmItANAAAAAAAAAAAAANBMBLoBAAAAAAAAAAAAAJqJQDcAAAAAAAAAAAAAQDMR6AYAAAAAAAAAAAAAaCYC3QAAAAAAAAAAAAAAzUSgGwAAAAAAAAAAAACgmQh0AwAAAAAAAAAAAAA0E4FuAAAAAAAAAAAAAIBmItANAAAAAAAAAAAAANBMBLoBAAAAAAAAAAAAAJqJQDcAAAAAAAAAAAAAQDMR6AYAAAAAAAAAAAAAaCYC3QAAAAAAAAAAAAAAzUSgGwAAAAAAAAAAAACgmQh0AwAAAAAAAAAAAAA0E4FuAAAAAAAAAAAAAIBmItANAAAAAAAAAAAAANBMBLoBAAAAAAAAAAAAAJqJQDcAAAAAAAAAAAAAQDMR6AYAAAAAAAAAAAAAaCYC3QAAAAAAAAAAAAAAzUSguwaDBg2Kdu3aRUlJSXMPBQAAAAAAAAAAAABopaSVl6CsrKy5hwAAAAAAAAAAAAAAtGIqdAMAAAAAAAAAAAAANBOBbgAAAAAAAAAAAACAZlISrcT8+fNj4sSJMWvWrGxZuHBhg9r75ptvGm1sAAAAAAAAAAAAAACtKtCdAttPPPFEPPzwwzFy5MgYM2ZMlJaWNvewAAAAAAAAAAAAAABad6D73nvvjXPPPTemTJmS3S4rK2v0PoqKihq9TQAAAAAAAAAAAACAgg50H3fccVmgu2KIW/gaAAAAAAAAAAAAAChEBRXovuCCC+Kee+7J1oW4AQAAAAAAAAAAAIBCVzCB7o8//jiuvvrqvEHuitW6AQAAAAAAAAAAAAAKRcEEuv/4xz/GwoULKwW6U5C7Q4cOseeee8ZOO+0UG264YfTv3z9WXHHF6NSpU3Ts2LHe/Q0aNCjefPPNRho9AAAAAAAAAAAAAEABB7qfeeaZXJg7BbnT+h577BG33nprrLLKKs09PAAAAAAAAAAAAACA1hnonj9/frzzzjtZiLs8zP2d73wn/vGPf0RxcXFzDw8AAAAAAAAAAAAAoF4KIg09derUatt+/etfC3MDAAAAAAAAAAAAAAWtICp0z5gxo9Ltjh07xo477tikfZ588skxceLEJu0DAAAAAAAAAAAAAGjbCiLQ3b1790q3+/TpE+3atWvSPk866aQmbR8AAAAAAAAAAAAAoDgKQL9+/aJLly652x06dGjW8QAAAAAAAAAAAAAAtJlAd3FxcWy33XZRVlaW3Z44cWJzDwkAAAAAAAAAAAAAoG0EupMTTjghtz5z5swYPXp0s44HAAAAAAAAAAAAAKDNBLoPOuig2HLLLXO3b7vttibtb++9944BAwbEWmut1aT9AAAAAAAAAAAAAABtV8EEupP7778/evbsGWVlZXHzzTfHsGHDmqyvCRMmxCeffJItAAAAAAAAAAAAAADR1gPda665ZgwdOjR69eoVixcvjr322isee+yx5h4WAAAAAAAAAAAAAEDrD3Qnm266abzxxhuxxx57xOzZs+OAAw7Igt3PPfdclJaWNvfwAAAAAAAAAAAAAABqrSQKyD333JNbP/TQQ2P55ZePBx98MJ566qls6dGjR2yxxRax0UYbRZ8+faJbt27RqVOnevU1bdq0Rhw5AAAAAAAAAAAAAECBB7qPO+64KCoqqra9rKwsF8J+9tlnswUAAAAAAAAAAAAAoKUrqEB31QB3uYoh76r31Ve+4DgAAAAAAAAAAAAAQLT1QPeSwtaC2AAAAAAAAAAAAABAoShu7gEAAAAAAAAAAAAAALRVBVmhu6SkJPr169ekfXzxxRexcOHCJu0DAAAAAAAAAAAAAGjbCjLQPXDgwBg1alST9jFo0KB48803m7QPAAAAAAAAAAAAAKBtK27uAQAAAAAAAAAAAAAAtFUC3QAAAAAAAAAAAAAAzUSgGwAAAAAAAAAAAACgmZREAenevXsUFRVFt27dmryvffbZJzbddNMm7wcAAAAAAAAAAAAAaLsKKtA9ffr0ZdbXpZdeusz6AgAAAAAAAAAAAADapuLmHgAAAAAAAAAAAAAAQFsl0A0AAAAAAAAAAAAA0ExKmqvjlu7xxx+PadOmZevHHHNMcw8HAAAAAAAAAACAVmBRFMX0dh3y3vfFpE9qPO7rGVOabEzfzJ25xL5rsny3ntGlc/cmGRNAWyLQXYMLL7wwRo8ena0LdAMAAAAAAAAAANAQi6Monui+SrzVsUcsLirOv9Ov9ovmMPy1f2ZLXRUVFcUGa20WPz7hd7FC9xWbZGwAbUENvxVIysrKmnsIAAAAAAAAAAAAtAJDu/aNNzqtWHOYu0Azdu+OGRm/veXM5h4KQEFrPb8ZAAAAAAAAAAAAoAVKpUXf6bhCtFZjP30nJn81obmHAVCwBLoBAAAAAAAAAACgCRVFRElZabRmHdov19xDAChYJdEKzJ07N/7973/HSy+9FF9++WV89dVXMWPGjCgtrf8vwDFjxjTqGAEAAAAAAAAAAGi7Biz4JkaVtM7Q82p914oVuq/U3MMAKFgFHej++OOP47LLLov77rsvFi5c2NzDAQAAAAAAAAAAgLy2njM5/tupZ5QVpXrdrcsBu5/Y3EMAKGgFG+j+/e9/Hz//+c9j8eLFUVZW1ujtF7XCX5oAAAAAAAAAAAA0j56LF8Qm876ONzr1jNZklT4D4rubfa+5hwFQ0Aoy0H3qqafGLbfckgtyC18DAAAAAAAAAADQ0m07e1K82bFHq6rSfdD3B0dxcbvmHgZAQSuOAnPrrbfGzTffnIW5U5C7PMydbjdkqSjfNgAAAAAAAAAAAGiMKt2thercAG2wQvdnn30WP/7xjytV5E7B65VXXjl22mmnWHXVVaNbt27Z9hT6njRpUrbvRRddlLe9dOzXX38d48aNi2HDhsXMmTOz7emYwYMHR58+fZbRzAAAAAAAAAAAAGgLWlOVbtW5AdpgoPv666+PefPmZYHrFMbu1atXXHXVVXHkkUdWCnknf//737NAd3LxxRcvte2FCxfGn/70p/jlL38Z06ZNi6FDh8a///3vrA8AAAAAAAAAAABozCrdb3TqGYVMdW6AxlMcBWL+/PlZ4Lo8zJ2qcb/88stx1FFHVQtz10f79u3j5JNPjtdffz3WXnvt+Pjjj2P//feP0tLSRhk/AAAAAAAAAAAAlFfpLiori0KmOjdAGwx0p6D1jBkzsjB3CnDfe++9MWDAgEbvZ4011ognnngiOnfuHCNGjIhrrrmm0fsAAAAAAAAAAACg7Sqv0l2oVOcGaKOB7lSNO0lh7p122il22GGHJutrnXXWiTPPPDMLj19xxRWxYMGCJusLAAAAAAAAAACAtqeQq3Srzg3QRgPdb7/9dm79yCOPbPL+jj322OzfadOmxXPPPdfk/QEAAAAAAAAAANB2FGqVbtW5AdpwoPvrr///L66tttqqyftbd911o2fPntn6kCFDmrw/AAAAAAAAAAAA2pZCrNKtOjdA4yvIQHe/fv2Wun9RUVFuvbS0tF599urVK/t31KhR9ToeAAAAAAAAAAAAWkuVbtW5Adp4oHvBggW59W7dui11/06dOuXWZ8yYUa8+Fy5cGGVlZTFp0qR6HQ8AAAAAAAAAAACtpUq36twAbTzQvfzyy+fWZ8+evdT9u3fvnlv/9NNP69xfqur95ZdfZutTpkyp8/EAAAAAAAAAAADQWqp0q84N0HQKJtC9wgor5NY//vjjOgXA//Of/9S5v9deey3mzJmTrRcXF8zDBAAAAAAAAAAAQIEphCrdqnMDNJ2CSSqvu+66ufXhw4cvdf911lknt37vvffWub8//vGPufUePXrU+XgAAAAAAAAAAABoDVW6VecGaFoFE+geOHBgbv32229f6v6bbrpp9m9ZWVm88sor8eijj9a6r6effjruv//+KCoqypbVVlutnqMGAAAAAAAAAACA2lXpbqkVsFXnBmhaBRPo3mKLLXLro0ePjuuuu26J+2+11Va5QHYKdR9//PHx7LPPLrWff/zjH3HooYdmx6Ql2WabbRphBgAAAAAAAAAAAFBzle7tv7N3tDSqcwM0vYIJdK+zzjqxxhprZOspaP3Tn/40rr322hr3X2WVVWK77bbL9k2h7pkzZ8b3v//9OOCAA+Lhhx+O8ePHx7x582LBggXZ+kMPPRR77rln7Lffftm+Fe22225NPj8AAAAAAAAAAADatgN2/1GLq4StOjdA0yuYQHey++675wLaixcvjnPOOScGDhwYV199dYwdO7ba/qkqd7nySt2PPfZYVoE7hcO7dOkSnTp1ytYPP/zwePrpp3Ptlx+z3nrrxa677rpM5wkAAAAAAAAAAEDb02fl1VpUlW7VuQGWjYIKdB922GHVAtrvvfde/OxnP4tbbrml2v7HHHNMDBo0qNoxS1rKw9xpPfm///u/ZTI3AAAAAAAAAAAAaElVulXnBlg2CirQvcMOO8Saa66ZC1+Xq7heUQpn33rrrVkV7orblrRUdNxxx8VBBx3UhDMCAAAAAAAAAACAllelW3VugGWnJArMNddcE6+88kresHc+m2++eTz66KOxzz77xIIFC2rVRwqIH3XUUVkYHAAAAAAAAAAAAJZ1le5hrz4epaWLm20MqnMDLDsFF+hOwey01MX3vve9eP3112Pw4MHVwuCpKnfFCt99+vSJSy65JNsXAAAAAAAAAAAAmqtK9wsjHmuW/lXnBli2Ci7QXV8DBw6Mf//73zFixIh45JFHYvTo0fHll1/G4sWLY+WVV4511lknvv/978eee+4ZnTp1au7hAgAAAAAAAAAA0IY1Z5Vu1bkBlq02E+gut9VWW2ULAAAAAAAAAAAAtFTNVaVbdW6AZa+4GfoEAAAAAAAAAAAAalGle1lXyladG2DZE+gGAAAAAAAAAACAFlyle1lRnRugeQh0AwAAAAAAAAAAQAu1LKt0q84N0DwEugEAAAAAAAAAAKCNV+lWnRug+bTqQPecOXPiiy++iPfeey/efvvt5h4OAAAAAAAAAAAAtMgq3apzAzSfkmhFhg0bFi+88EK89tpr2TJlypTcfZtuummMHDmy2jGnnHJKbL311rHvvvtG9+7dl/GIAQAAAAAAAAAAoHZVul8Y8ViTtK86N0DzKvhA98yZM+P222+PW2+9NT766KPc9rKyskr7Vb1d7qmnnsqO7dq1a5x55plxzjnnxAorrNDk4wYAAAAAAAAAAIC6VOke9urjUVq6uNHbVp0boHkVRwF7+umnY+DAgXHuuefGhx9+mIW2y5eioqJKy5Kk/WfNmhWXX355bLTRRvHSSy8tszkAAAAAAAAAAABAbat0NzbVuQGaX8EGus8666zYc889Y8KECUsMcJcHvJek/Ji03xdffBG77LJL3HfffctgFgAAAAAAAAAAAFD7Kt2NXUlbdW6A5leQge7BgwfHDTfcUCnI3ZBg+KBBg3Kh79TWokWL4oQTTojnnnuuEUcNAAAAAAAAAAAALadKt+rcAC1DwQW6b7755rj99tuz9XyVuMuXzp07R+/evZfa3o9//ON4/fXXY9iwYbHVVlvlQuILFy6M448/PmbPnt2k8wEAAAAAAAAAAIDmqNKtOjdAy1BQge4vv/wyzjvvvGpB7hTePvzww+O+++6Lt956KxYsWBCzZs2KL774ItunNhW8t9122xg+fHicffbZuVD3hAkT4je/+U2TzgkAAAAAAAAAAACWdZVu1bkBWo6CCnRfffXVWVC7XElJSZxzzjlZcDuFuVOoe+DAgdn2+iguLo6rrroqBg8enKv0fdttt8XixYsbcRYAAAAAAAAAAADQvFW6VecGaDkKJtC9aNGiuOuuu7LK2eVVuf/5z3/GlVdeGd26dWvUvq655pro379/tj516tR46qmnGrV9AAAAAAAAAAAAaK4q3apzA7QsBRPofumll2LatGlZmDuFulO4e5dddmmSvjp16hSnn3567vawYcOapB8AAAAAAAAAAABY1lW6VecGaFkKJtD98ssvZ/+mMHcKch944IFN2t/++++fWx81alST9gUAAAAAAAAAAADLokq36twALU/BBLrff//93PpRRx3V5P0NGDAgVlhhhWx97NixUcimTp0aRxxxRBaGr7hccskl0Rp9+eWXceutt2Zz3njjjWPFFVeMDh06ROfOnaNv376x3XbbxRlnnBFPPvlkzJ8/v7mHCwAAAAAAAAAAsMyqdKvODdDylESBmDBhQm592223XSZ99urVK6ZPnx4zZsyIQvXXv/41Tj/99Jg8eXK0dm+//Xb8+te/jocffjgWL15c7f6FCxfG3LlzY+LEiTF8+PC44YYbsuc4PT5nn312dOnSpVnGDQAAAAAAAAAA0JAq3S+MeKxW+6vODdAyFUyF7tmzZ+fWU5XlZSFVsU5mzZoVhWbSpElx0EEHxSGHHNLqw9wpvH3hhRfGZpttFg8++GDeMHdN0mNz0UUXxcCBA+O5555r0nECAAAAAAAAAAA0Z5Vu1bkBWqaCqdBdHq5OFi1atMxC0UlJScE8TJn77rsvzjzzzJg2bVq0djNnzsxC608//XS1+1LF7QMPPDC222676NevX8yfPz8+/PDDePzxx7MK3RV9+umnsfvuu8e1116bVewGAAAAAAAAAABoTVW6VecGaLkKJqncrVu33Prnn38eG2ywQZOHuadPn54FyXv06BGF4IsvvoiTTjopCyy3BXPnzo299947XnrppWr3HXnkkVk4e6WVVqp233nnnRfDhg2LE044IcaOHZvbnip7n3HGGdm6UDcAAAAAAAAAAFBIVbqHvfp4lJYurnEf1bkBWq7iKBD9+/fPrT/55JNN3t+QIUNy62uuuWa0dHfccUcMHDiwWph7xRVXjNbq+OOPzxvmvuCCC+LPf/5z3jB3ue233z5eeeWV2Gijjardl6qb//Of/2z08QIAAAAAAAAAADRlle6aqM4N0LIVTIXuTTbZJLd+8803x2mnnRbLLbdck/SVKjXfcMMNudubb755tFSfffZZnHjiifHMM89Uu+/AAw+MP/7xj9GnT59obW677bZ48MEH8875sssuq1UbK6+8chaA/9a3vhUzZ87MbS8rK4tjjz02Ro8eHX379m3UcQMAAAAAAAAAANTky4vXqvex2y0uimHRJUqjqNp9u37zTky6dN0Gjg7alr6Xjm3uIdCGFEyF7p133jm3Pnbs2Dj33HObrK8rr7wy3nrrrdztXXbZJVqiO++8M6swXTXM3bt373j44YezJa23NpMnT877/K+wwgpx00031bny+29+85tq26dOnRo//elPGzROAAAAAAAAAACAZWWldmWxU8cF1bavVbIoNu2wqFnGBEArC3QPHDgwW8orKKfK06lK98KFCxu1n1SZ+xe/+EUUFf3vr5R69OgRe+yxR7RE1113XcyaNavStiOPPDLefffdrFJ1a3XhhRfGjBkzqm3/yU9+klXdrqtU4XzNNdestv3++++PESNG1HucAAAAAAAAAAAAy9LenRdkoe6uRaXRIcriWx0Wxo+6zY3i6kW7AWhBCibQXR7YTWHuFLZO/958881ZherHHnssu90Q48aNi/333z/OOuusrK3yfk499dRo3759tHSrrLJKPP744/HnP/85evbsGa3VF198EXfddVe17ek5Ovnkk+vV5pKOveyyy+rVJgAAAAAAAAAAwLKWgtv7dZkfv+oxO37T85v4Ybd50bmgUoIAbVNBvVUfe+yxsfHGG2fr5aHujz76KA444IBYbbXV4uyzz87C3VOmTFlqW4sWLYq33347br/99thtt91i3XXXjSFDhuSC3EmfPn3i3HPPjZbuRz/6Ubzzzjux1157RWuXKqgvWFD9siCpinqvXr3q3e4xxxyTe94reuKJJ+KDDz6od7sAAAAAAAAAAADNEewuUZUboGCURAFp165dVp15m222ifnz5+cCuCmEnSo3X3fdddmSdO/ePdZcc83csWPGjIntttsuZs2aFTNnzowJEyZkoe5y5RW+y4PixcXFWdi7W7du0VKl+V199dWxyy67RFtQWloa99xzT9779tlnnwa1ncL7W265ZYwYMaLS9vRaSH2q1A0AAAAAAAAAAABAtPUK3cmgQYOyUHcKXJdLIezyIHb5MmPGjHjjjTey+9Pt2bNnx8svvxyjR4+OTz75JBYuXFhp//I2yv32t7/Nqj63ZA8//HCbCXMnL774YhbEz6cxHoea2rj//vsb3DYAAAAAAAAAAAAAtIpAd3LIIYfEAw88EF26dKm0vTyUXTWcnZQHt/PtV7HSd0lJSVbl+5xzzolCqFjelvzzn//Mu32VVVaJNdZYo8Htb7vttnm3pz8AeO+99xrcPgAAAAAAAAAAAAC0ikB3ctBBB8XIkSNjk002yQW1q6oY6q4p6F0utbH66qvHsGHD4owzzmiycVN/zz77bN7tm266aaNVf6/JM8880yh9AAAAAAAAAAAAAECrCHQn66yzTowYMSJuuOGGGDBgQKUq3OWWFuJOS69eveLyyy+Pt956K7bccstlNHrqYvbs2dnzk89GG23UKH307t07Vl555bz3vfLKK43SBwAAAAAAAAAAAABUVBIFrkOHDnHqqafGKaecEkOGDIknn3wyXnzxxfjggw+WeFz//v1jhx12iF133TUOOeSQrB1arv/+979RWlqa97611lqr0fpJbU2ZMqXa9lQNHgAAAAAAAAAAAAAaW8EHusulKtz77rtvtiRfffVVjB8/PqZNm5atpzDwiiuuGD179oy+fftmC4Vj9OjRNd63xhprNFo/qa1U9b2qsWPHxpw5c6Jz586N1hcAAAAAAAAAAAAAtJpAd1UpvJ0WWocUqK5Jv379Gq2fmoL+ZWVlMW7cuBg4cGCj9QUAAAAAAAAAAAAAxc09AKiNFKauycorr9xo/SyprY8//rjR+gEAAAAAAAAAAACARKCbgvDll1/WeF/Pnj0brZ+VVlqpxvsmTpzYaP0AAAAAAAAAAAAAQCLQTUH46quv8m5fbrnloqSkpNH66dKlS53HAAAAAAAAAAAAAAD11XhJWGhC06ZNy7u9a9eujdrPktqraQz1NXny5JgyZUqdjhkzZkyl2wsWLIj58+cv9biioqLo0KFDte3p+LKyslr3365du2oB+tLS0li4cGHURfv27aO4uPLfkyxatCgWL15c6zba0pwWd+gaZUXtat1O8aJ5Uby48uuirKg4FnfoFnXRbsGsKCorrbSttN1yUVrSsdZtFJUtjnYLvqm23ZzMqaDnFMWxqKRz1EXJojlRFJXntLi4Q5QWV/+Zr0l6TEoWz6m2fVG7ztnjXFvFpQuiXemCStvM6X/MqTDmlH5P+xxRnTnVrK3MqVX+zjWnjDkVzpyqqut7RL2VlVU6N9BW3vfM6X/MKT9zqpk51aytzKk1/M5tjZ8jzKntzCkf543MaUnMqWnnlH7X+xyRnznl15bmVOi/c1vj5whzaltzqqrQf+e2xs8R5tR25pTOv/sc0TrntGBB5ddGtPVA984775z9u/baa8ett94aLUkaz8SJE7P1iy66qLmH0+bNnTs37/Z8bwgNsaT2ahpDfd14441x6aWXNqiNzz//PDp16lSreQ0YMCDv8XV5Y1pppZWypaL0xjlu3LioizXXXDOrrl7R9OnTY+rUqbVuoy3N6Zs1tovFHZevdTudJr2dLRWl/6zMXG+PqIvuHzwZJfNnVNo2f6V1Ym7vjWrdRrt5M2L5D5+stt2czKmQ55Q+2E9aeZuoi95T/h3tF1X+z/vszqvFzG5r17qN9gu/id5T/11t+1c9BsXC9rX/A6fus8ZE92/GVtpmTv9jTgUyp4ULfY7Iw5xq1lbm1Bp/55rT/5hT4cypqrq+R9RXOhFZ8T2trbzvmdP/mFN+5lQzc6pZW5lTa/id2xo/R5hT25lTr/ig2nbnjcxpScypaefUefp0nyNqYE75taU5Ffrv3Nb4OcKc2tacIua0qt+5rfFzhDm1nTnNGTfO54hWOqfPP/88WppmDXS/8MILWUo/PWgtzU033RSjR4/O1gW6m19NP+DprzMaU9W/9KjNGAAAAAAAAAAAAACgvmpff74NqkvZd5pWKn2/LALdS2qvriX8AQAAAAAAAAAAAGBpisqaMbVcXFycVejeZJNNYtSoUdGSDBo0KN58881sfOnStYUszSGfiy++OC655JIoBO3bt88b6l5ttdXis88+a7R+hg4dGt/73vfy3jd48OC45ZZbGq2vyZMnx5QpU+p0zJgxY2K//fbL3U4/NxtuuGGtXgPpEgf5qo7X5S0gBd6rVjEvLS2tc9g9PZ/p57+i9PzW5Wettc/p7YP//8/t4g5do6yo9n+8ULxoXhQvnl9pW1lRcXZZobpot2BWFJWVVtpW2m65KC3pWOs2isoWR7sFlS+jkpiTORXanFbc8P9fIqYsirNL8NRFyaI5URSV57S4uEOUFld/H6tJekxKFle+tFayqF3n7HGureLSBdGutPJVJ8zpf8ypMOa06i/+63NEHubUNudU8TNja/mdW5E5/Y85Fc6cNn5o8RLfI44+Z8toCt26rBA3/t/TbeJ9ryJz+h9zys+camZObXNOFT83tobfua3xc4Q5tZ059Vq7V7XtzhuZ05KYU9POqd/5I32OqIE5tb05VfzM2Bp+57bGzxHm1LbmtPJ6q7Sq37mt8XOEObWdOfX+2as+R7TSOb377rux2Wab5W6//fbbMXDgwGhOlR8FaKHSD36+QHdNlbvra0nt5XvzaYhevXplS0OkMS233HINOr6h0ptgQ8ZQLr0pV31jro/WOKd8H/jrKvugM39Gg9tJ/xGq+p+h+jCn/MypQOYUpdF+UcPbaZfnPw71ke8/MXVlTjUzp5Y3p6r/AcvG4nNEjcyp7cypVf7ONacamVNhzKmx3iOWqqhoqe9prfF9z5xqZk41M6f8zKltzak1/s41p5qZU0ucU/XvhZw3ys+camZOjTenfL/nfY6omTm1rTkV/u/c6swpP3MqkDkV+O/cfMypZubUsuZU0+9nnyMKf04dGjkP2moC3SmRP378+Dql8pfFmGg5OnXqFHPmzGny52lJf9XRsWPt/+oOAAAAAAAAAAAAAAom0P3ee+/FGmus0dzDoAXr0aNHfPXVV9W2f/NNw/8CqKJZs2bVeF/Pnj0btS8AAAAAAAAAAAAAqH698maQKnO3tIWWZaWVVsq7ff78+bFo0aJG62f27Nk13rfiiis2Wj8AAAAAAAAAAAAA0GIqdBcVFTX3EGjh+vTpU+N906ZNi169ejVKP/mqgNdmDAAAAAAAAAAAAADQaip0Q1VrrrlmjfdNmTKl0fqZPHlyjfcNGDCg0foBAAAAAAAAAAAAgBYT6K5aobtqwLs5FlqWtdZaq8b7vvjii0br58svv6zxPoFuAAAAAAAAAAAAABpbSTSjTp06xdy5c7NAd1pSkDr927t379h9992bc2gxZMiQ+Prrr5t1DPx/3/rWt2q875NPPmm0fmpqKwXKO3fu3Gj9AAAAAAAAAAAAAECzB7pTePaKK66IW265JebMmZOr1D1p0qR444034qKLLor999+/WcY2aNAgge4WJD0fxcXFUVpaWu2+sWPHNlo/NbW1xRZbNFofAAAAAAAAAAAAAFCuOJrRyiuvHFdffXV8/PHH8ZOf/CSr2J2qdCejR4+Ogw46KDbddNP429/+1pzDpAXo2rVrbLzxxnnve/vttxulj/SHBFOmTMl731ZbbdUofQAAAAAAAAAAAABAiwl0l+vVq1cu2P3jH/84OnbsmAW705KC3QcffHAW5v3rX//a3EOlGX3ve9/Luz1Vc28MS2pnt912a5Q+AAAAAAAAAAAAAKDFBborBrt///vfx7hx47Jgd6rYnaRg97vvvhuHHXZYDBw4MB588MHmHirNYM8998y7fcKECfHpp582uP3hw4fn3d6/f//YcMMNG9w+AAAAAAAAAAAAALToQPeSgt3lFbvff//9OOKII7KA7QMPPJBtawpN1S71t8MOO8Qqq6yS977nnnuuwe0PHTo07/b0egMAAAAAAAAAAACANhPorhrs/vjjj+Oss86qFuw+6qijYoMNNoj77ruv0QPYTz75ZBYoT33TMhQXF8fRRx+d977HHnusQW1PnDgx/vOf/1TbXlRUFMcee2yD2gYAAAAAAAAAAACAggx0l+vdu3dcc801lYLdSQpxf/jhh3HMMcfE+uuvH/fee2+UlpY2Sp99+/aN/v37Zwstx+mnnx4dOnSotv2pp56KKVOm1Lvd9NrJ90cBe+21V6y33nr1bhcAAAAAAAAAAAAACj7QnS/YfeaZZ0bHjh1z93300Udx3HHHZeHbu+++u9GC3TTMO++8E/vuu28sv/zy0bVr19htt93itddeq3d7q6yySvY8V7VgwYK46aab6tXmokWLajz2ggsuqFebAAAAAAAAAAAAANDqAt0Vg93XXnttLti93HLL5e4bO3ZsnHDCCbHOOuvEHXfcEYsXL27WsbZlo0aNiq222iqGDBkSM2fOjNmzZ8ezzz4b2223XQwdOrTe7f7qV7/KAuJVpbB/fap033bbbTFu3Lhq24844oj47ne/W+9xAgAAAAAAAAAAAECrDHSX69OnT43B7hTQPfHEE7Ng9+23355VYWbZGjx4cHzzzTfVts+fPz8L3aeq2vXRq1evuPLKK6ttnz59epx66ql1auuzzz6L888/v9r2lVZaKa666qp6jQ8AAAAAAAAAAAAA2kSgO1+w+4wzzqgU7P7kk0/ipJNOirXXXjtuvfVWwe5lJAXqR44cWeP948ePjxEjRtS7/RTWP+SQQ6ptf/jhh+PCCy+sVRtTp06NvffeO2bMmFFpe1FRUdx9993Rt2/feo8PAAAAAAAAAAAAAGqjJFqRFOy+7rrrsorLv/nNb+K2226LefPm5Soxn3LKKXHZZZdl9//whz+M9u3bR6FLIfa01PfYoUOHLnGfXXfdtV5tf/nll0vdZ8KECdEQd911V3zxxRcxfPj/Y+9OoOSu6nyB/7o7K1kIoUNI2EmAaBKWSBAwyM57sgiKrOogY1zYBGQTARmGAeahgKwOjyg89LDIIhydwBjAUYIRhiVAggiJCAkJ2fe9O/3OvzxgkqoKvVT6dlV9PufUIX1v1V2aTupW9ff/q/Hrtf/bv/1bLsif/Sz07du34GOzx3zta1+LqVOn5vVljzvyyCPbtDYAAAAAAAAAAAAAqLpAd6Fg97XXXhtjxoz5KNidVYY+66yzcu3f+973YvTo0dGlS5coV/fee29cddVVrXrsz3/+89xtY5qamlo1dv/+/T/2Pm2tgN29e/f4z//8z/jSl74U48aNW6/vF7/4RTz22GO5vgMOOCA316pVq+Ktt97KPeYPf/hD3nh1dXVx00035aq8AwAAAAAAAAAAAEB7qI0KlgW7b7nlllwV5rPPPju6du36UUh5+vTpueDuzjvvHLfddlsu7EvpDBo0KPbYY4+i/dtuu23sv//+bZ6nd+/eMXbs2Fx4v1On9a9PWLp0aa6Kd1aNPau4/YUvfCEuueSSgmHu7bffPp544glhbgAAAAAAAAAAAADaVUUHuj+UVWf+MNidVeTO1NTU5ILdM2bMiHPPPTd22WWX1MusOHfddVf06NEjrz0L1v/sZz8rWWX0LMidVVx/5ZVX4oQTToja2ub/WNfX1+cqnE+ePDkOP/zwkqwHAAAAAAAAAAAAAJqrKgLdmZUrV8aDDz4Yv/nNb3Jh7kz23w+D3R988EGUo3/5l3/JrX9T3dpi5MiRMWHChDj66KOjV69esdlmm+VC088+++wmCU8PGzYsfvnLX8Z7770Xd9xxR5x00kkxdOjQ6NOnTy703a1bt+jfv3+uMviZZ54Zv/71r2PatGnxgx/8IHr27Fny9QAAAAAAAAAAAADAx+kUVRDk/slPfhI//OEPY9asWbm2D4PKHwa72XSGDx+eC063p2222SbOOOOM3A0AAAAAAAAAAAAAOrJOlRzkzqo0Z0Hu2bNnt7naNAAAAAAAAAAAAABAqVVcoHvFihVx++23x49+9KOYM2fOx1bjzvq7d+8e3/rWt9p5pQAAAAAAAAAAAABAtetUSUHu2267LW644YZmB7k322yz+Pa3vx0XXXRR9O/fv51XDAAAAAAAAAAAAABUu7IPdC9fvvyjIPfcuXObFeTu2bNnnHnmmXHhhRdGfX19O68YAAAAAAAAAAAAAKDMA93Lli37KMg9b968ZgW5e/fuHeecc06cf/750bdv33ZeMQAAAAAAAAAAAABAmQe6syD3rbfeGjfeeGOzg9x9+vSJc889N84777zYfPPN23nFAAAAAAAAAAAAAABlHuheunRp3HLLLXHTTTfF/PnzmxXkzqpwZ9W4v/Od70SvXr3aecUAAAAAAAAAAAAAAGUe6M6C3DfffHMuyL1gwYJmBbn79esXF1xwQZx11lnRo0ePdl4xAAAAAAAAAAAAAECZB7qXLFmSC3L/+Mc/bnaQe+utt44LL7wwzjjjjOjevXs7rxgAAAAAAAAAAAAAoMwD3YsXL/4oyL1w4cJmBbkHDhwYF198cXzzm9+Mbt26tfOKAQAAAAAAAAAAAADKPNCdBbmzEHd2W7RoUbOC3Nttt11873vfi69//evRpUuXkq7n//7f/xsffPBB7s8/+MEPSjo2AAAAAAAAAAAAAECHCHRn4e2bbropbrnllmYHuXfccce49NJL42tf+1p07tx5k6zrJz/5Sbz22mu5Pwt0AwAAAAAAAAAAAAAVF+i+8sorc0HurDp3c4LcgwYNiu9///vxT//0T1FXV7fJ15fNWWw9AAAAAAAAAAAAAABlHei++uqrc4HpYsHpD0Peu+22W1x22WVx6qmnRm1tbYKVAgAAAAAAAAAAAABUWKD7Qx+GuT8Mdn8Y5P7kJz8Zl19+eZx00kkqZQMAAAAAAAAAAAAAFadDBLrXrdCd/XnrrbeOCy64IL70pS/l2qdNm9bua1q9enW7zwkAAAAAAAAAAAAAVJcOEejOfFiVOzNr1qy4+OKLczcAAAAAAAAAAAAAgErVIQLdH1bnLhbw7ihrAgAAAAAAAAAAAACouEB36vB2uawJAAAAAAAAAAAAAKgsHSLQvf3228dVV10VHckPfvCDmDZtWuplAAAAAAAAAAAAAAAVrEMEuvv27RunnXZadCQ//vGPBboBAAAAAAAAAAAAgE2qdtMODwAAAAAAAAAAAABAMQLdAAAAAAAAAAAAAACJCHQDAAAAAAAAAAAAACTSKRJramqKjmjAgAGxcOHC1MsAAAAAAAAAAAAAACpY0kD3O++8k/tvly5doqMZO3Zs6iUAAAAAAAAAAAAAABUuaaB7hx12SDk9AAAAAAAAAAAAAEBStWmnBwAAAAAAAAAAAACoXgLdAAAAAAAAAAAAAACJCHQDAAAAAAAAAAAAACTSKdXEAAAAAFAu1kbE9M49YnrnzaKxpiav/60n70qyrlWrVsSjrZi7S+dusdugPWPwDsOipsB+AAAAAAAAaD8C3QAAAACwEY0R8djm28fkblsUv9Nvbo8UVq9ZGQ+0Ye7DR50QXz/x0qit9UF+AAAAAAAAqfhNDQAAAABsxBvd+mw8zF3Gxo1/KF7/y59SLwMAAAAAAKCqCXQDAAAAwEa827lnVLI33n4p9RIAAAAAAACqmkA3AAAAAGzElo2ropIN2GqH1EsAAAAAAACoagLdAAAAALARn1i5MGqbmqISde3SPUYMOyD1MgAAAAAAAKqaQDcAAAAAbESftWtijxXzoxJ97sBTonfPLVIvAwAAAAAAoKoJdAMAAADAxxi1bHbFVenOqnMffehXUy8DAAAAAACg6gl0AwAAAMDH2GLt6oqr0q06NwAAAAAAQMcg0A0AAAAAVValW3VuAAAAAACAjkOgGwAAAACqrEq36twAAAAAAAAdh0A3AAAAAFRRlW7VuQEAAAAAADoWgW4AAAAAqKIq3apzAwAAAAAAdCwC3QAAAABQJVW6VecGAAAAAADoeAS6AQAAAKBKqnSrzg0AAAAAANDxCHQDAAAAQBVU6VadGwAAAAAAoGMS6AYAAACAKqjSrTo3AAAAAABAxyTQDQAAAAAVXqVbdW4AAAAAAICOS6AbAAAAACq8Srfq3AAAAAAAAB2XQDcAAAAAVHCVbtW5AQAAAAAAOjaBbgAAAACo4CrdqnMDAAAAAAB0bALdAAAAANDGKt11tZ2iI1KdGwAAAAAAoOMT6AYAAACANlbpPnDfY6IjUp0bAAAAAACg4xPoBgAAAIA2+uIRoztclW7VuQEAAAAAAMqDQDcAAAAAtNFW9dt0uCrdqnMDAAAAAACUB4FuAAAAAKiwKt2qcwMAAAAAAJQPgW4AAAAAqLAq3apzAwAAAAAAlA+BbgAAAACooCrdqnMDAAAAAACUF4FuAAAAAKigKt2qcwMAAAAAAJQXgW4AAAAAqJAq3apzAwAAAAAAlB+BbgAAAACokCrdqnMDAAAAAACUH4FuAAAAAKiAKt2qcwMAAAAAAJQngW4AAAAAqIAq3apzAwAAAAAAlCeBbgAAAAAo8yrdqnMDAAAAAACUL4FuAAAAACjzKt2qcwMAAAAAAJQvgW4AAAAAKOMq3apzAwAAAAAAlDeBbgAAAAAo4yrdqnMDAAAAAACUN4FuAAAAACjTKt2qcwMAAAAAAJQ/gW4AAAAAKNMq3apzAwAAAAAAlD+BbgAAAAAowyrdqnMDAAAAAABUBoFuAAAAACjDKt2qcwMAAAAAAFQGgW4AAAAAKLMq3apzAwAAAAAAVA6BbgAAAAAosyrdqnMDAAAAAABUDoFuAAAAACijKt2qcwMAAAAAAFQWgW4AAAAAKKMq3apzAwAAAAAAVBaBbgAAAAAokyrdqnMDAAAAAABUHoFuAAAAACiTKt2qcwMAAAAAAFQegW4AAAAAKIMq3apzAwAAAAAAVCaBbgAAAAAogyrdqnMDAAAAAABUJoFuAAAAAOjgVbpV5wYAAAAAAKhcAt0AAAAA0MGrdKvODQAAAAAAULkEugEAAACgA1fpVp0bAAAAAACgsgl0AwAAAEAHrtKtOjcAAAAAAEBlE+gGAAAAgA5apVt1bgAAAAAAgMq38c9zBQAAAAA+1swrB7Xp8ft07hoTVnXJaz+gdmEs++HesaxNo0P1GXDV1NRLAAAAAACAZlOhGwAAAAASO3qz1bFVbeN6bdvXNcbhm61OtiYAAAAAAADahwrdAAAAAJBYz9qmOKv3ipiwqnNMa6iLnTo3xn5d10S3mtQrAwAAAAAAYFMT6AYAAACADqBPXVN8TkVuAAAAAACAqlObegEAAAAAAAAAAAAAANVKoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEhEoBsAAAAAAAAAAAAAIBGBbgAAAAAAAAAAAACARAS6AQAAAAAAAAAAAAASEegGAAAAAAAAAAAAAEikU6qJ2bRWrVoVzzzzTDzxxBMxceLEePvtt2PRokXR0NAQvXr1ioEDB8awYcPioIMOis9//vMxYMCA1EsGAAAAAAAAAAAAgKoj0F1hli1bFjfddFPceuutMXv27IL3mT9/fu42adKkeOCBB+Kss86K448/Pi6//PIYPnx4dHRZCP33v//9Jp+nqalpk88BAAAAAAAAAAAAQHWrTb0ASieryD106NC44oorioa5C2lsbIxf/vKXMWLEiLjssstyXwMAAAAAAAAAAAAAm54K3RXijjvuiO985zsFw9gHHHBAHHXUUbHrrrtG165dY8aMGfHss8/Gww8/HMuXL//ofg0NDXHttdfG//zP/+T6evfu3c67AAAAAAAAAAAAAIDqItBdAW677bY455xz8toHDx4cd999d4waNSqvb/To0XHDDTfkQuD333//en3jxo2Lo48+Ov7rv/4runfvvknXDgAAAAAAAAAAAADVrDb1AmibsWPH5kLZG9p9991jwoQJBcPcH6qvr4/77rsvLrnkkry+rIL36aefXvL1AgAAAAAAAAAAAAD/INBdxmbOnBmnnXZaNDU1rdfep0+f+M1vfpMLbDfHddddF8cee2xe+4MPPhhjxoyJjuzAAw/M7X9T3AAAAAAAAAAAAABgUxPoLmMXXHBBzJ07N6/9+uuvj+22267Z49TU1MR//Md/RK9evfL6LrroopgzZ06b1woAAAAAAAAAAAAA5BPoLlMTJkyI+++/P6998ODB8c///M8tHm/rrbeOc889N6994cKFccUVV7R6nQAAAAAAAAAAAABAcQLdZeqaa64p2H7WWWdFXV1dq8Y844wzCj72nnvuiZkzZ7ZqTAAAAAAAAAAAAACgOIHuMvTmm2/G2LFj89qzMPZXvvKVVo87cODAOOKII/LaV61aFbfeemurxwUAAAAAAAAAAAAAChPoLkP33ntvNDU15bXvt99+UV9f36axjz322ILtP//5z2Pt2rVtGhsAAAAAAAAAAAAAWJ9Adxm67777CrYfeuihbR672BjTp0+PZ599ts3jAwAAAAAAAAAAAAD/INBdZiZPnhzvvvtuwb5Ro0a1efzBgwdH//79C/aNHTu2zeMDAAAAAAAAAAAAAP8g0F1mxo0bV7Rvzz33LMkce+21V8H23/72tyUZHwAAAAAAAAAAAAD4O4HuMjNhwoSC7VlV7fr6+pLMMXz48ILtr732WixbtqwkcwAAAAAAAAAAAAAAAt1l56WXXirYPmjQoJLNUWystWvXxsSJE0s2DwAAAAAAAAAAAABUO4HuMpJVx546dWrBvh133LFk82xsrKxKNwAAAAAAAAAAAABQGp1KNA7t4K9//WvRvoEDB5ZsngEDBrRqDamtWLEixo4dG3/84x9zlcSztS5evDh3q6uri8022yy3tyywvtdee8VnPvOZOPjgg6Nbt26plw4AAAAAAAAAAABAlRLoLiPvvPNO0b5+/fqVbJ6NjdURA90zZ86Mq666Ku67775YsmRJwfs0NDTEqlWrYsGCBfHGG2/kgt+ZHj16xMknnxznnXdeDBs2rJ1XDgAAAAAAAAAAAEC1q029AFoWXC6mb9++JZunvr6+aN8HH3wQHcmkSZNi1113jTvvvLNomHtjli1bFj/96U9jjz32iNNPPz3mz5+/SdYJAAAAAAAAAAAAAIWo0F1G5s2bV7SvZ8+eJZunc+fOuduaNWtatIYUNlxPTU1N7LPPPnHQQQfFkCFDYsstt8y1zZ49OxeIf/rpp2P8+PF5e1u7dm3cc8898dvf/jYeeeSR2HfffTf52rM1zZkzp0WPmTJlynpfr169Old5/ONk34MuXbrktWePb2pqavb8dXV10alTp7zvXaGflY3Jfr5qa2vzqqg3NjY2e4xq2lNjl57RVFPX7HFqG1ZGbeP6PxdNNbXR2KVXtETd6iVR07R2vba1dV1jbaduzR6jpqkx6lYvzWu3J3sq6z1FbTR02ixaolPD8qiJ9ffUWNsl1tbm/50vJvuedGpcntfeULdZ7vvcXLVrV0fd2tXrtdnT39lTeewpe552jshnT8VVy54q8jnXnnLsqXz2lLeWMn/OrcRzhD1V154KcY6wp42plj1VwnNuJZ4j7Kl69lRIuT/nVuI5wp6qZ0/Zc71zRGH2VFg17ancn3Mr8RxhT9W1pw2V+3NuJZ4j7Kl69pRl85wjKnNPq1ev/7PREQh0l5GNVY8uZaD7w/EWLFjQojWklP1F/Kd/+qe47LLLYvDgwUXvl/Vn+7rmmmvilltuyftLP2PGjDj44IPj0Ucfjc997nObdM133HFHXHXVVW0aY/r06dG9e/ePvV/2D+fOO+9c8PEt+Ycpq96+YQX37Hv4zjvvREvstNNO0bVr1/XaFi5cGHPnzm32GNW0p6U7HhCN3TZv9jjdZ03K3daVvVhZvFvLfqZ7/+WJ6LRq0Xptq+p3iRX9hzV7jLqVi2Lzt57Ia7cneyrnPWUH+1n9PhMt0X/Oc9G5Yf0X78s22y4W9yr+nLWhzmuWRv+5z+W1z9tir1jTufnngN5LpkTvpVPXa7Onv7OnMtnTmjXOEQXYU3HVsqdKfM61p7+zp/LZ04bK/jm3Es8R9lRVe9quQLtzhD1tTLXsqRKecyvxHGFP1bOnreIvFfecW4nnCHuqnj1ttnChc0QR9lRYNe2p3J9zK/EcYU/VtaeI5RX1nFuJ5wh7qp49LX/nHeeICt3T9OnTo6Np/uUKJLdixYqifYWudGiLYuNtbA2p9O/fP5566qm4++67Nxrm/tAWW2wRP/rRj+LFF1/MPXZDK1eujOOPPz6ef/75TbRiAAAAAAAAAAAAAPg7ge4ysrGrFrKy86W0YQn75qwhhW222SbGjx+fq6rdUrvvvnv8/ve/j379+hUMrp944okFq5QDAAAAAAAAAAAAQKnUNDU1NZVsNDap008/Pe65556Cfb/73e/ioIMOKtlcO+ywQ7z33nt57TU1NbF27dpI6eabb46333479+dvfvObuWB2W9x///1x6qmnFuw766yz4rbbbotNYfbs2TFnzpwWPWbKlClx3HHHffT1yy+/HJ/85Cc/9nHZ/7dCVdezgH5L/gnILhzYMOyf/TxkH3HQEp07d47a2vWvJ2loaIjGxsZmj1Hpe5p0Qs1H7Y1dekZTTfMv2qhtWBm1javWa2uqqc19rFBL1K1eEjVN6/99X1vXNdZ26tbsMWqaGqNu9fofo5KxJ3sqtz1t+cl/fERMU9TmPoKnJTo1LI+aWH9PjbVdYm1t8z9hI/uedGpc/6O1Mg11m+W+z81Vu3Z11K1d/wIte/o7eyqPPW172SvOEQXYU3Xuad0zY6U8567Lnv7OnspnT/Wf2LGinnMr8RxhT9W1p+1+8Hpeu3OEPVXrntY9N1bCc24lniPsqXr2tNXgrSruObcSzxH2VD17GnjpS84RRdhT9e1p3TNjJTznVuI5wp6qa0/9dtumop5zK/EcYU/Vs6f+F7/gHFGhe3rjjTdixIgRH309adKkGDp0aKQk0F1GvvGNb8SYMWMK9j399NNxyCGHlGyubbfdNt5///289uwv4KpV6x+EKsFhhx2W+x4W+sdi6tSpuYB7RzB58uQYNmxYh/pHhE1jwxfMQFrrBrqBtAZcNTX1EqDDcGaEjsWZEToW50b4B+dG6DicGaFjcWaEf3BmhI7FuRE6DmfGyjW5A2Yxm3+5AskVupph3SsJSqnYeBtbQzk7++yzC7ZnV2dsqgrdAAAAAAAAAAAAACDQXUa6d+9etC8rU19KxcrVd+vW/I8SKSdHHXVU9OvXr2Dfgw8+2O7rAQAAAAAAAAAAAKA6CHSXkS222KJo39KlS0s615IlSwq29+3bNypR586dY9SoUQX7pk2bliuvDwAAAAAAAAAAAAClJtBdRurr69sl0J1V5y5WoXvLLbeMSjVixIiifX/605/adS0AAAAAAAAAAAAAVAeB7jKy9dZbF+2bP39+yeaZN29eq9ZQ7oYPH1607y9/+Uu7rgUAAAAAAAAAAACA6iDQXUZ22mmnon1z5swp2TyzZ88u2rfzzjtHperbt2/Rvvfee69d1wIAAAAAAAAAAABAdRDoLiMbC1PPmDGjZPPMnDmzaN+gQYNKNk9Hs/nmmxftW7x4cbuuBQAAAAAAAAAAAIDqINBdRnr27Fk01P23v/2tZPNsbKzhw4dHperRo0fRvlWrVrXrWgAAAAAAAAAAAACoDgLdZeZTn/pUwfapU6eWbI5iY9XW1sZee+0VlWrp0qVF+7p169auawEAAAAAAAAAAACgOgh0l5n999+/YPusWbNi7ty5JZnj9ddfL1qde2NVrMvdokWLivb17t27XdcCAAAAAAAAAAAAQHUQ6C4zhx9+eNG+iRMnlmSOYuMcccQRUcnmzZtXtG/77bdv17UAAAAAAAAAAAAAUB0EusvM0KFDi4aLn3vuuTaPP3Xq1Pjggw8K9h155JGR0qhRo2LHHXfM3UaOHFny8V999dWifbvttlvJ5wMAAAAAAAAAAAAAge4ydOqppxZsf/rpp9s89lNPPVWwfdttt43PfvazkdL06dPj3Xffzd1ee+21ko//0ksvFe379Kc/XfL5AAAAAAAAAAAAAECguwyddtppBdv/+Mc/xty5c9s09uOPP16w/atf/WrU1nacH5fVq1fHrFmzSjbeypUri1Y432677XKV0QEAAAAAAAAAAACg1DpOQpdmGzJkSBx55JF57Y2NjfGLX/yi1ePOnDkzxo0bl9fepUuXOPvss6Oj+e///u+SjfWrX/0qFixYULDvxBNPLNk8AAAAAAAAAAAAALAuge4yddlllxVsv/3223PB7ta44447oqGhIa/99NNPj4EDB7ZqzKzq9aGHHho9evSIPn36xPHHHx9vvfVWlMLDDz9cknGampritttuK9hXV1fXIcPsAAAAAAAAAAAAAFQGge4ytf/++8fJJ5+c1z5lypS4++67WzzerFmz4uabb85rz0LYV199davWOHbs2Dj44IPjmWeeieXLl8eiRYvi0UcfjX322ScmTZoUbfXII4+UZJy77ror/vjHPxbs++Y3vxk77rhjm+cAAAAAAAAAAAAAgEIEusvYjTfeGPX19XntF198cUybNq1FY33729+OJUuW5LVff/310a9fvxavbfXq1fGNb3wj1qxZk9eXBbtHjx4dpaisfeqppxZcd3O98sorcdFFFxXs23bbbePf/u3f2rBCAAAAAAAAAAAAANg4ge4yNmDAgLjnnnuipqZmvfYFCxbEMcccE/PmzWvWON///vfjsccey2s/8cQTc6Hs1njuuedixowZRfuff/75eO+996KtXn/99TjyyCM3OlcxWVXuQw45JBYvXpzX161bt3jwwQejb9++bV4jAAAAAAAAAAAAABQj0F3mjjrqqLj55pvz2l999dXYd999c8HqYrLA95e//OW47rrr8vpGjRqVC4u31syZMz/2Pu+//36Uwvjx42OPPfbIfR+aU6171qxZuYrkBxxwQCxcuLBgmPuRRx6J/fffvyTrAwAAAAAAAAAAAIBiOhXtoWycc845uSrd5513XjQ2Nn7UPmXKlFww+8ADD8wFv3fZZZfo2rVrrpp1FoJ+6KGHYtmyZXnjHXbYYfHwww9H9+7dW72m/v37N6vCeEvsvPPO8e677xbsmzt3bm7/l19+eS6onYWxt99++9hiiy1y/bNnz47p06fHuHHjYsKECbF27dqC42y77ba5MPc+++zTorUBAAAAAAAAAAAAQGsIdFeIs88+O4YMGRKjR4/OCz3//ve/z90+TqdOneLCCy+Mq6++OvfntvjMZz6TC3Vn1bALGTlyZOy4444tGvOZZ57JVRy/8847c2H0lStX5t1n6dKl8cQTT+RuLZHtN/ve/fu//3tsvvnmLXosAAAAAAAAAAAAALRWbasfSYeTVdaePHly/Ou//mv069ev2Y+rra2NL33pS/Hyyy/Hdddd1+Ywd6Zbt25x1113FRwrC0yPGTOm1UHxe++9N1dl/Oc//3mcfPLJH1Xhbo36+vo499xz480334yf/OQnwtwAAAAAAAAAAAAAtCsVuitMjx494oorroiLL744xo0bF08++WRMnDgxpkyZEosWLYqGhobo2bNnbLPNNjFs2LD47Gc/G8cee2zu61I75phj4umnn44rr7wynn/++ejcuXMudJ6Fxnfdddc2jZ2FuL/yla/kbo2NjfHSSy/Fq6++Gq+//nou1J4FvrP9ZreskncWMO/du3dun4MGDYq99tort/d99903F2gHAAAAAAAAAAAAgBQEuitU165d4+ijj87dUspC07/73e826Rx1dXWxzz775G4AAAAAAAAAAAAAUE6UJgYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABIR6AYAAAAAAAAAAAAASESgGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEBLoBAAAAAAAAAAAAABLplGpiNq1Vq1bFM888E0888URMnDgx3n777Vi0aFE0NDREr169YuDAgTFs2LA46KCD4vOf/3wMGDAgKsU777wTjz/+eIwfPz4mTZoUs2bNiqVLl0bXrl2jT58+sdtuu8WIESPiqKOOigMOOCDq6upSLxkAAAAAAAAAAACAKiXQXWGWLVsWN910U9x6660xe/bsgveZP39+7paFnR944IE466yz4vjjj4/LL788hg8fHuXqj3/8Y1x99dXx5JNPFuzPwuzZ9+f999/Phd1/9KMfxQ477BDf/e5344wzzojOnTu3+5oBAAAAAAAAAAAAqG61qRdA6WQh5aFDh8YVV1xRNMxdSGNjY/zyl7/MVa2+7LLLcl+Xk+XLl8e3vvWtGDVqVNEwdzHvvvtunHvuubm9v/LKK5tsjQAAAAAAAAAAAABQiArdFeKOO+6I73znOwXD2AcccEAcddRRseuuu0bXrl1jxowZ8eyzz8bDDz+cC0OvW8H62muvjf/5n//J9fXu3Ts6upkzZ8YxxxwTL730Ul5f375948QTT4x99903ttpqq9xes6rkjz32WEycOHG9+2btWSD83nvvzVUrBwAAAAAAAAAAAID2INBdAW677bY455xz8toHDx4cd999dy6ovKHRo0fHDTfckAuB33///ev1jRs3Lo4++uj4r//6r+jevXt0VHPnzo1DDz00/vznP+f1nX/++XH11VdHjx491mvPwtpXXnllLtT9zW9+M+bMmfNRXxb4Pumkk+KRRx6JY489tl32AAAAAAAAAAAAAEB1q029ANpm7NixuVD2hnbfffeYMGFCwTD3h+rr6+O+++6LSy65JK8vq+B9+umnR0eVVRM/7rjjCoa577zzzrjxxhvzwtzryh6bfX8GDhy4XntW4fzkk0/Oq+ANAAAAAAAAAAAAAJuCQHcZmzlzZpx22mnR1NS0XnufPn3iN7/5TS6w3RzXXXddwYrUDz74YIwZMyY6oqzK9nPPPZfX/t3vfjdXebs5Bg0alKvU3anT+oXqV65cmavUvWzZspKtFwAAAAAAAAAAAAAKEeguYxdccEHMnTs3r/3666+P7bbbrtnj1NTUxH/8x39Er1698vouuuiimDNnTnQkb7zxRm6PG9p5553jmmuuadFYI0eOjPPPPz+v/a233oprr722TesEAAAAAAAAAAAAgI8j0F2mJkyYEPfff39e++DBg+Of//mfWzze1ltvHeeee25e+8KFC+OKK66IjiQLYDc0NBSs2t2tW7cWj/e9730vevfundd+ww03xLvvvtvqdQIAAAAAAAAAAADAxxHoLlPFKlGfddZZUVdX16oxzzjjjIKPveeee2LmzJnREbz44ovx29/+Nq+9X79+cfLJJ7dqzL59+8aXv/zlvPZVq1bFj370o1aNCQAAAAAAAAAAAADNIdBdht58880YO3ZsXnsWxv7KV77S6nEHDhwYRxxxRMFg86233hodwY033liw/ZRTTokuXbq0etyvfe1rBdt/+tOf5qqUAwAAAAAAAAAAAMCmINBdhu69995oamrKa99vv/2ivr6+TWMfe+yxBdt//vOfx9q1ayOlRYsWxa9+9auCfZ///OfbNPbIkSNjwIABee0rVqyIhx56qE1jAwAAAAAAAAAAAEAxAt1l6L777ivYfuihh7Z57GJjTJ8+PZ599tlI6dFHH42VK1fmtXft2jVGjRrVprFramrikEMOadH3GwAAAAAAAAAAAADaSqC7zEyePDnefffdgn1tDTVnBg8eHP379y/YN3bs2Eip2Px77713LtTdVsW+f+PHj4/Fixe3eXwAAAAAAAAAAAAA2JBAd5kZN25c0b4999yzJHPstddeBdt/+9vfRipr166NZ555Jsm+Gxoa4ne/+11J5gAAAAAAAAAAAACAdQl0l5kJEyYUbM+qatfX15dkjuHDhxdsf+2112LZsmWRwl/+8peYP39+wb5hw4aVZI5snJqamhZ93wEAAAAAAAAAAACgLQS6y8xLL71UsH3QoEElm6PYWFmV7IkTJ0ZH2ncp996jR49cML6l8wMAAAAAAAAAAABAawl0l5GsOvbUqVML9u24444lm2djY2VVulPY2LztsfdU+wYAAAAAAAAAAACgsgl0l5G//vWvRfsGDhxYsnkGDBjQqjVsSsWC7O2199mzZ8fSpUtLNg8AAAAAAAAAAAAAZAS6y8g777xTtK9fv34lm2djY6UKdBfbe/fu3aNHjx7tsveNff8BAAAAAAAAAAAAoDUEusvIzJkzi/b17du3ZPPU19cX7fvggw+iI+29lPvuqHsHAAAAAAAAAAAAoHIJdJeRefPmFe3r2bNnyebp3Llz7tbSNWxKxeYt5b4zG6v2nWrvAAAAAAAAAAAAAFSuTqkXQPPNnz+/aF+pg83ZeAsWLGjRGjaVpUuXxpo1a9pt38WUeu+zZ8+OOXPmtOgxb7zxxnpf//nPf47Vq1d/7ONqamoKhvSz72tTU1Oz56+trY1Ondb/Z2Pt2rXR0NAQLZGNkY21rmyMbKzmqvQ9TVn0j/bGzt0jauqaP07jqqhtXP/vTFNNTaztXPyChUJq1yyLmg2+l2vrOkdTXdfmD9LUGHVrVuQ125M9ldue5s7+x7+1TVETjZ26N38tEVHXsCJqYv09NdZ2jqbawhdQFdS0Njo1rsxrbqjrFlHT/Gv0atauibq1G/x/sqcceyqPPc19/XXniALsqTr3tO6ZsVKec9cb2p5y7Kl89rTumbESnnMr8RxhT9W1p/mTJ+c1O0fYU7Xuad1zYyU851biOcKeqmdPW25wZqyE59xKPEfYU/XsafarrzpHFGFP1bendc+MlfCcW4nnCHuqrj313eDcWO7PuZV4jrCn6tnTB6+84hxRoXuaOnXqel+vWrUqUhPoLiMrVuQ/6X+oS5cuJZ2r2HgbW0Ml7/vj1tEad9xxR1x11VVtGuOEE04o2XroyErxs5c9SS4twTjZIafwBRYtY0+F2VOH3dO46SWYGyiJ23ZPvQLooCrkOXc99lScPXXMPTkzQody+7DUK4AOqhKeczdkT8XZU8fbkzMjdCi37Zl6BdCBlftzbiH2VJg9dcw9OTdCh3HbiNQroJ1MmzYtRoxI+/+7+ZcrkNzGKjHX1TX/Sq7m2PCKh+asoZL3/XHrAAAAAAAAAAAAAIDWEOguIxsrIV/qYHOx8bJy+NW471R7BwAAAAAAAAAAAKCyFS9HTIezserRa9euLelcjY2NBds7d+4c1bjvTbH3M888M0444YQWPWbx4sXx4osvRu/evaNPnz6x3XbbRdeuXUu6LgD+YcqUKXHcccd99PVjjz0WgwcPTromAAA6FmdGAAA+jjMjAADN4dwI0H5WrVoV06ZN++jrAw88MFIT6C4jXbp0aVUV69YoNt7G1lDJ+/64dbTGVlttlbu11H777VfSdQDQfNmL5aFDh6ZeBgAAHZgzIwAAH8eZEQCA5nBuBNi0RowYER1JbeoF0Hzdu3cv2rd69eqSzrVmzZqC7d26dYtq3HeqvQMAAAAAAAAAAABQ2QS6y8gWW2xRtG/p0qUlnWvJkiUF2/v27RvtrWfPntG5c+ek+061dwAAAAAAAAAAAAAqm0B3Gamvry/aV8pgc1alulil6i233DJSKDZvqQPdy5Yta/EaAAAAAAAAAAAAAKC1BLrLyNZbb120b/78+SWbZ968ea1aw6ZUbN5S7ruj7h0AAAAAAAAAAACAyiXQXUZ22mmnon1z5swp2TyzZ88u2rfzzjtHR9r7ihUrNlpVuxL2DgAAAAAAAAAAAEDlEuguIxsLFM+YMaNk88ycObNo36BBgyKFjc3bHnvfaqutomfPniWbBwAAAAAAAAAAAAAyAt1lJAsUFwt1/+1vfyvZPBsba/jw4ZHC7rvvXrSvPfaeat8AAAAAAAAAAAAAVDaB7jLzqU99qmD71KlTSzZHsbFqa2tjr732io6071LufdmyZTFr1qyCfXvvvXdJ5gAAAAAAAAAAAACAdQl0l5n999+/YHsWRJ47d25J5nj99deLVqnu0aNHpDBkyJDYYostCvZNmjSpJHNk4zQ1NRXs23fffUsyBwAAAAAAAAAAAACsS6C7zBx++OFF+yZOnFiSOYqNc8QRR0QqWXXwQw89NMm+O3XqFIccckhJ5gAAAAAAAAAAAACAdQl0l5mhQ4fG9ttvX7Dvueeea/P4U6dOjQ8++KBg35FHHhkpFZv/pZdeitWrV7d5/PHjxxds/8xnPhO9e/du8/gAAAAAAAAAAAAAsKFOeS10eKeeemr8+7//e177008/HVdeeWWbxn7qqacKtm+77bbx2c9+NlL64he/GGeeeWasXLlyvfbs6yyM3ZYq2k1NTfHMM88U/X4DUJ369eu33nNr9jUAAKzLmREAgI/jzAgAQHM4NwJUt5qmLMlKWXnzzTfjE5/4RF57XV1drrp2fX19m6pgP/HEE3ntl156aVx77bWR2imnnBIPPPBAXvt3vvOduPnmm1s97gsvvBCf/vSn89q7d+8eM2bMiD59+rR6bAAAAAAAAAAAAAAoprZoDx3WkCFDcsHrDTU2NsYvfvGLVo87c+bMGDduXF57ly5d4uyzz46O4Lvf/W7B9izkvWbNmlaP+//+3/8r2P71r39dmBsAAAAAAAAAAACATUagu0xddtllBdtvv/32XLC7Ne64445oaGjIaz/99NNj4MCBrRrzueeei0MPPTR69OiRC0Yff/zx8dZbb0VrjRw5Mo444oi89tmzZ8f999/fqjEXLFhQMAjftWvXuPDCC1s1JgAAAAAAAAAAAAA0R01TU1NTs+5Jh3PKKafkKlNv6K677orRo0e3aKxZs2bFLrvsEkuWLFmvPQthZwHsfv36tXh9Y8eOjeOOOy6vcvbmm28e48ePj2HDhkVrTJ48Ofbcc8+88PmgQYNi0qRJ0a1btxaNd8kll8T111+f1/79738/rrnmmlatEQAAAAAAAAAAAACaQ4XuMnbjjTdGfX19XvvFF18c06ZNa9FY3/72t/PC3Jks6NyaMPfq1avjG9/4Rl6YO7No0aIWB87XNXTo0NweNzR16tS4/PLLWzTWiy++mPs+bigLt2eBbgAAAAAAAAAAAADYlAS6y9iAAQPinnvuiZqamvXaFyxYEMccc0zMmzevWeNkweXHHnssr/3EE0/MhbJb47nnnosZM2YU7X/++efjvffei9a66qqrYv/9989rv+GGG2LMmDHNGuOvf/1rroL4hpW+u3btGg8++GD06NGj1esDAAAAAAAAAAAAgOYQ6C5zRx11VNx888157a+++mrsu+++uWB1MVng+8tf/nJcd911eX2jRo3KhcVba+bMmR97n/fff7/V43fq1CkXQh8yZEheXxZCv+CCC2L58uVFH//444/Hfvvtl7eGurq6eOCBB2KvvfZq9doAAAAAAAAAAAAAoLlqmpqampp9bzqs2267Lc4777xobGzM6zvwwANzwe9ddtklV306q5w9fvz4eOihh2LZsmV59z/ssMPi4Ycfjs0337zV63n66adz42zMO++8EzvuuGO0RbaXo48+Ol555ZW8vi233DJOOumkXLC9X79+uYD35MmT41e/+lXB+3fv3j3uvffe+NKXvtSmNQEAAAAAAAAAAABAcwl0V5CnnnoqRo8eHe+++26rq15feOGFcfXVV+f+3BYrV67MhbVnzZpVsH/kyJHxwgsvRClkQe1zzz03fvrTn0Zrf5yHDh2aC3OPGDGiJGsCAAAAAAAAAAAAgOaobda9KAtZReysAvW//uu/5ipSN1dtbW2uKvXLL78c1113XZvD3Jlu3brFXXfdVXCsrPL3mDFjolQ222yz3FzPPvtsHHHEES167Pbbbx8//vGPcxW7hbkBAAAAAAAAAAAAaG8qdFeoVatWxbhx4+LJJ5+MiRMnxpQpU2LRokXR0NAQPXv2jG222SaGDRsWn/3sZ+PYY4/Nfb0p/OEPf4grr7wynn/++ejcuXMudJ6FxnfdddfYVKZOnRqPP/54jB8/Phdwz6qEL126NLp27Rp9+vTJzZ2Ft4866qjc/ksRYAcAAAAAAAAAAACA1hDoBgAAAAAAAAAAAABIpDbVxAAAAAAAAAAAAAAA1U6gGwAAAAAAAAAAAAAgEYFuAAAAAAAAAAAAAIBEOqWaGAAA1rVmzZq4/vrr4+qrr45Vq1Z91P673/0uDjrooKRrAwAgjcbGxnj55Zfj+eefjzfeeCMmT54c06dPj0WLFuVuXbt2jc033zz69OkTn/jEJ2LEiBFxwAEHxKhRo6Kmpib18gEASMR7jQAArMv7jEA5qGlqampKvQgAqAbZm8S///3vSzZeXV1d7gXFFltsEf3794+99947Pv3pT8dhhx0WW221Vcnmgfbw4osvxte//vV47bXX8vr8kgWAauPcSLWbPXt2PProozF27Nj4wx/+kPuFSkvttNNO8Y1vfCPOP//86Nat2yZZJwCk5MwIxXmvEQD+zpmRaud9RqDcCHQDQJm+YC6mS5cucfzxx8d5550X++yzzyafj7QWLlwYzz33XPzpT3+Kt99+O6ZOnRozZsyIZcuW5W6dO3eOnj17Rq9evWKHHXaI3XbbLXdF8YEHHhi777578quJV6xYET/4wQ/ipptuyl0VXYhfsgBQbZwbqdZzY1Yh5+KLL47//u//Lno2zPTu3Tvq6+tzv0BZsGBB7hczxe6/8847x1133RWHHHLIJlw5ALQ/Z0aq9cy4Md5rBID1OTNSrWdG7zMC5UqgGwDayejRo3OVQdY1f/78mDZtWt59t9tuu+jbt2/RsZYvX557QTFv3rwo9lSevRA655xz4rrrrovNNtusBDugo8g++umhhx6KBx98MF544YWiPwMfJ7tS/sgjj8xVq8k+Kqq9ZS+gs6uZp0yZstH7+SULANXGuZFqPTeOGTMmdz4sZNCgQXH22WfHUUcdFbvssst6fUuXLo3x48fHT37yk/j1r3+dt8/sl0j3339/7heLAFApnBmp1jNjMd5rBIB8zoxU65nR+4xAuRLoBoCE7rnnnjj99NPz2u++++742te+9rGPz14wP/vss3HnnXfGk08+WfA+e+yxRzzzzDMbfQFOeciucr7hhhviV7/61UavJO7UqVPuo86yK4q7d++ee+G5ZMmS3Bs0xY5+Q4YMiUsvvTS+8pWvRG1t7SbcReQ+yuqiiy7KvZBedz3ZvGvXrs27v1+yAIBzI9Vxbiz2i5bsF4E//OEPo2vXrh87RvaLlmxtixcvzttr9rGq++23X0nXDAAdiTMj1XBm3JD3GgGgZZwZqYYzo/cZgXK1aV9BAwCb1JZbbhnHHXdcPPHEEzFu3LjcxwFt6NVXX40jjjgi94KJ8vTmm2/mrhDOXhQ+/PDDeS+WBw8eHGeeeWb88pe/jEmTJuWukM8+DiqrRvP666/HO++8E3Pnzs39DLz00kvxs5/9LE4++eT1fl6yOU477bTYc88946mnntpke3n88cfjk5/8ZO7jqD588V5XV5f7pcuFF164yeYFgGrn3FgdKunc+KGzzjorbrnllmb9kiVzzDHH5Pa34S+BGhoa4lvf+lbuvwBAYc6M1aGSzozeawSA9ufMWB0q6cz4Ie8zAuVAoBsAKsRhhx2Wqy7So0ePvL7sRdIVV1yRZF20XvYi8PLLL4/hw4fH2LFj894sOe+882Ly5Mnx9ttvx+233x4nnHBCDB06NPdRT4VkPxsjRozIXXWffRTUzJkz49FHH43Pfe5zH90ne4GdvcFy7rnnxsqVK0u2lzVr1sRJJ52Ue4NnxowZ612h//zzz8f111+fu1obANj0nBsrTyWdG9e1ww475CrmtNT/+l//K0455ZS89mzN2S8bAYCP58xYeSrpzOi9RgDoGJwZK08lnRnX5X1GoFwIdANABRk2bFj8n//zfwr2ZS+o3njjjXZfE63zt7/9LUaNGhXXXHPNelf39urVK66++uqYNm1a3HTTTbkKNK2VfRzUF77whdyL8RdeeCEOOeSQXHtWzSa7OnnffffNvaguhWXLluWuYP5QduVzto8XX3wxPvWpT5VkDgCg+ZwbK0elnRs3/AjU1gZxsscW8sgjj7RxVQBQPZwZK0elnRm91wgAHYczY+WotDPjurzPCJQLgW4AqDCjR4+OrbbaKq89e9GVfZQRHd/LL7+ce7GaVZNZ11e/+tXcx1RlV0WXusLMyJEj4+mnn4577rknevfu/dHHoe2///7x1ltvlXSubMyJEyfm9pG9aAcA0nBuLH+Vfm784he/2OrH7r333tGzZ8+89izkAwA0nzNj+av0M6P3GgEgPWfG8lfpZ0bvMwLlQqAbACpMVo0k+0iiQh5//PF2Xw8t84c//CEOPPDAmDVr1kdt3bp1i7vuuivuvffegm+GlNJpp50Wr732Wu5q+g+vxD744INj+vTpbR47e6GbXVn97LPPxpAhQ0qwWgCgLZwby1slnxszffr0iZ122qnVj6+rq4vtt98+r31TVPgBgErmzFjeKvnM6L1GAOg4nBnLWyWfGTPeZwTKiUA3AFSg/fbbr2B7dvXswoUL2309NM8rr7wSxxxzTCxduvSjti222CLGjx+fu7K9veywww65X4RkL9wzM2bMiCOPPDIWL17c6jE322yzmDRpUu4jqWprHUEBoKNwbixPlXpuPPHEE+Ptt9/O3bJqPKX4Zc2G1v2eAQDN48xYnir1zJjxXiMAdDzOjOWpUs+M3mcEypVXuABQgfr371+074MPPmjXtdA877//fvzv//2/13tR2qtXr3jyySfjU5/6VLuvJ3tR+p//+Z+5j5DKvP7667mro1urS5cuuRfiAEDH4txYfir53Jh9tOrgwYNzt0JVb1pqyZIleW319fVtHhcAqo0zY/mp5DNjxnuNANDxODOWn0o+M3qfEShXnVIvAAAovewFSjHz58/f6GOXL18eEyZMyFU4mTx5cu7q10WLFsXKlStj8803z12Ru80228SnP/3p2H///TfpG+cvvvhi/OlPf4qJEyfmXuhvuI5dd901dt9999yLukGDBpXtvA0NDXHSSSfF7Nmz1/vopuwjyPbZZ59IpUePHjF27NgYOXJkvPvuu/HYY4/FmDFj2vVqbABg03JuLK95nRtbptAvC4cMGZJkLQBQzpwZy2teZ0YAIAVnxvKa15mxZbzPCLQXgW4AqEAb++ihvn375rXNmTMn7rvvvvj1r3+d+/ikVatWNXuu7EXz+eefH1/4whdyL/LaavXq1XHLLbfEnXfemfsIrubaeeedc1cQH3/88XHwwQdHTU1NWcybueaaa+K5555br+3SSy/NjZdav3794mc/+1kcdthh0dTUFJdcckkcd9xxrjgGgArh3OjcWKnnxj//+c+5n9cNHX744UnWAwDlzJnRmbFSz4wAQOk4MzozVuqZ0fuMQLtqAgCSufvuu5uyp+MNb1l7W9x+++0Fx81uCxYs+Oh+DQ0NTccdd1xT586di96/ubcDDjigadq0aW1a94svvti02/9v7z6gnSi+B44PvfeH9N6kSJMmIFUpCqhIU6SIgDRFqmIBLIgFEMWCoCIiAqKiSBFREVEp0kSKdAEBkSa9k/+58zvJf5Nsel528/L9nBN9b192d1KZvXPnTrlyEbelaNGijrFjx9r+vGLXrl2OjBkzuh3n5ptvdly+fNlhJ3379nW1b/DgwclyjlGjRpk+r8uWLUuW8wEAEE/oN7qj35jY/cZgjB492ut5z5Qpk+Off/6xrE0AACQ3+ozu6DMmdp+RWCMAAOboM7qjz5jYfcZgEGcEEEupY5s+DgAAYuHXX3813V66dGmVM2dO1+9XrlzRyxTJ/40yZsyoly1auHCh+vvvv9WFCxf07eDBg2rJkiVq2LBhKk+ePG77rFixQlWtWlUvhRUOmQHcpEkTtX37dtc2mVF93333qblz56qdO3eqM2fO6OWfZFmudevWqddee01VrFjR61j79+9XkydPtvV5nYYMGaKXyzJ6++23Vbp06YI+xrlz5/TyV4888oienXzLLbfomeny88iRI/USZZ66d++uZ2s7b8WLF/d7jrFjx6ps2bLpn+UxBloaDQAAxAf6jfQbU2K/UZZAnTBhgtd2qeaTL1++mLcHAIB4R5+RPmNK7DMCAIDoos9InzEl9hmJMwKIuZimjwMAgGSfAX3+/HlH3rx5TY87bNgwt/teuHDB6z5Vq1Z17N69O+B5zp496+jVq5fX/iVKlHAcPXo0pDafOnXKUbBgQbfjFChQwLFu3bqA+167ds0xbtw4R+rUqd32L1asmG3P67Rp0yZHqlSp3PZv2bJl0PvLayAzgrNkyRJwdnblypUdS5Ysce3brVu3kNs9cOBA1/1ff/11R7RRNQcAAN/oN/4P/Ub6jcG8VtWrV/d6XM2bN9evBQAAKRl9xv+hz0ifURBrBADAHH3G/6HPSJ8xEOKMAKxAhW4AAFKYqVOnqqNHj3ptT5s2rXrwwQf97lukSBH1ww8/qJIlSwY8T5YsWdSUKVPU4MGD3bbv3btXDR06NKQ2v/LKK+rQoUNu26ZNm6aqV68ecN/UqVPrWcQyKzlUVp3XeH6HQ677/t+oUaOC2ldmbN90001q9OjRegZ0IJs2bVLNmzdXgwYNUtevXw+rvY8++qh+3GLWrFlhHQMAANgH/Ub7n9d4fvqNgX3zzTe6KtP69evdtt977726YpCzTQAAIHj0Ge1/XuP56TMCAAAr0Ge0/3mN56fPGBhxRgBW4dsFAIAURC6KRowY4fNip3z58gEv4HLlyhXSOV9++WVVtmxZt20zZ85Uu3fvDvoYn3zyidvvBQsW1Bd3oZDlnKpUqRLSPladV5w+fVp99tlnbtsqVaqkateuHXDfjRs3qnr16unghFHRokX1ayjLip06dUqdPXtWX1jLEllybDFx4kTVtWtXrwv1YEggxflY16xZo44dOxbyMQAAgD3Qb6TfGM/9xmvXrqmTJ0/qx7VgwQI9iCTv2ZYtW7o91qSkJPXBBx/o5y9DhgxRbQMAAImAPiN9xnjuMwIAgNigz0ifMZ77jMQZAdgNCd0AAKQQS5cuVU2aNFHnz5/3+lvNmjXVs88+63f/7Nmzq3bt2oV8XplZLReNRlevXtUXNME4fPiw14VfsWLFQm5HqlSpVIcOHYK+v1Xndfriiy/UxYsX3ba1b98+4H5yISwzf48fP+62vVOnTmrLli1q2LBhqmLFivr1lFnqEszo27evvsh2vgckoOF5sR6sRo0a6f/LLOpVq1aFdQwAAGAt+o30G+O13/jnn3/q51HeS7lz59YDOq1bt9btlb8J+dutt96qK0Pt378/YBUoAABgjj4jfcZ47TMCAIDYoc9InzFe+4zEGQHYVVqrGwAAAMInF00//fSTXlpKlv0xU61aNbVkyRKVNWtWr7/JRYgsyyRKlCihfw9H06ZNvbYtW7Ys6AtXTzILNhyBZnjb4bxOX3/9tde2YC6YH3vsMbVnzx63bTJD+OOPP1Zp0qTxuZ/8beTIkSp9+vR6lrxZYCUYDRs2dC3jtWHDBtWqVauwjgMAAGKLfqM7+o0ps98obZfBJRlIlPNnypQp2c8JAEBKQp/RHX3GlNlnBAAAkaHP6I4+Y8rsMxJnBGAVEroBALAhubCRpYd8kZmzJ06c0EsK+VqaKHXq1HoZqxdffNHnBYZcII8bNy7i9hYuXNhr22+//aYuXboUcMkhmS3tSWa9bt26VVWoUCHkC/eVK1fqn+16XiGvmQQ6jPLkyRPwwvuvv/5SM2bMcNuWOXNm9e677/q9WDZ6/PHH1cKFC9XPP/+swlG8eHG39gAAAGvRb6TfmEj9xowZM7qWWJXn5r///tODXmfOnHEtkTpnzhx9E7JcbJ8+ffSSrvI+BwAgUdFnpM+YSH1GAAAQHvqM9BkTqc9InBGAXZHQDQCADR04cEDfwiEXbDJbdNCgQapGjRoqFrJly2Z6QXr06FHTi2mjQoUKmW6Xi6Fvv/1WL3EULFnGqU6dOkHd16rzip07d+pgh5HzgtGfN998U188Gt1///2qSJEiQZ9blo566qmn9KzpcCQlJbl+ltcXAABYi34j/cZE6jfK4I0s1erp77//Vr/88oteunXBggWuQcXVq1fr2xtvvKGXRr355puj0g4AAOINfUb6jInUZwQAAOGhz0ifMZH6jMQZAdgVU0YAAIhTMuM1V65ceimqunXr6tnOcmEhFxny/1hdLAdaciuYC9dy5cp5bV+3bp2qXLmymj59uuls5UhZdV7nTGtPVatWDbif2bJlnTp1Cvn8t99+u+kSZ6FeMIe7LBYAAIgt+o2Rod9o/36jDNJ17NhRzZ8/X23atEm/LkayFGuDBg3UvHnzkrUdAADEM/qMkaHPaP8+IwAAiBx9xsjQZ7R/n5E4IwCrUaEbAAAbmjZtmurevbsl55aLRJldKhcjcpFy5MgRdfr0aX3znH0byKlTp4K6n8zWliWKPB08eFA/D08++aT+v1w8eV40RcKq8+7atctrW8mSJf3uIzOmZbktz9nMtWrVCivYUr16da9ltYJhDCKkS5cu5P0BAEB00W/8H/qN/y/R+42VKlVSq1atUnfddZdaunSp22BP+/btdbWjJk2axKw9AADYAX3G/6HP+P8Svc8IAAC80Wf8H/qM/y/R+4zEGQFYgYRuAADguoiTJYJmz54dtaWKrl+/HtT9evXqpb766iu1ePFi078fOnRIvfjii/pWunRpfdHUpk0bVa9ePX3xFy6rzitBCE85cuTwu8+OHTtcSzo5FS1a1HQ5sWDccMMNYe138uRJ18/hzqIGAADxjX5j7M5LvzE8mTJl0u9PGSTat2+fa7sMAEoFoT/++EPly5cvpm0CACDR0GeM3XnpMwIAgHhFnzF256XPGB7ijABiLXXMzwgAAGzlypUrauTIkXqG6aRJk6J2sRyK1KlTqy+++CKo5Znkwn78+PGqYcOG+uLowQcf1LNfQ52dbeV5z54967Ute/bsfvc5ceKE1zZZ0ixcWbJkifhiv0iRImGfHwAAxB/6jfQb46nfmDt3bjV69Giv7fK+feWVV2LeHgAAEgV9RvqM8dRnBAAA1qDPSJ8xnvqMxBkBxBIJ3QAAJLBLly6ptm3bqueff17/7Dkjd8CAAerrr79W+/fv1xd5MgPX1y1SGTNmVLNmzdIXsOXLlw9qn+PHj6sPP/xQNW/eXJUoUUJNmDBBL3Fk9/NevHjRa1ug2cRmy4NZUbVm7dq1rp/Lli0b8/MDAABr0G+k3xiP/cbOnTubVv6ZPHmyOnPmjCVtAgAgJaPPSJ8xHvuMAAAgtugz0meMxz4jcUYAsUJCNwAACeyhhx5SCxYs8NreqlUrtXv3bj0jWn6Wma7hzpgN1T333KO2bNmiL9TbtWunL2iDceDAATVkyBBVsWJFtXTpUluf1+zYngGLYC6OQw0ORMPq1atdP9epUyfm5wcAANag32jNeek3RiZdunSqUaNGps/HTz/9ZEmbAABIyegzWnNe+owAACCe0Ge05rz0GSNDnBFArJDQDQBAgpLlmGbOnOm1/dZbb9WzgfPkyaOskipVKn2hPnfuXPXvv/+qGTNmqNatW6sMGTIE3Pevv/5SLVu2VFOmTLHtecO5+DV7PcxmRUcyCzuYfb788kv9c4ECBVTlypXDPj8AAIgf9ButOy/9xsjdcsstptsZaAEAILroM1p3XvqMAAAgXtBntO689BkjR5wRQCyQ0A0AQIIaP3686faJEyfqGaZ2kS1bNvXAAw+o+fPnqyNHjqjp06erFi1aqLRp0/rc59q1a6pfv37qxx9/tOV58+bN67VNzuFP8eLFvbbt27cv4MxpX44dOxbyPp9//rk6efKk/rljx44qdWq6kgAAJAL6jdadl35j5PLly2e6/eDBgzFvCwAAKRl9RuvOS58RAADEC/qM1p2XPmPkiDMCiAWujAEASEBy0bNs2TKv7eXLl1fVq1dXdpUjRw7VtWtXtXjxYn2xOHr0aJUrVy6fF6+DBw+25XlLlSrltU2O60/hwoVViRIl3LZdvXpVbdy4UYXK4XCo33//PeTZz6NGjdI/p0mTRj366KMhnxcAAMQf+o3Wnpd+Y+Ry584dtQEkAABgjj6jteelzwgAAOIBfUZrz0ufMXLEGQHEAgndAAAkoG3btqkrV654ba9du7aKFwULFtQXcHv37lVdunQxvc+GDRvUli1bbHfeMmXKmL4mgdx2221e22T5rVCtWbMm5AvLMWPGqN27d+uf+/Tp43XxDgAAUib6jdaeN5H6jVLZ5+zZs/p24cIFFS2XL1823Z4xY8aonQMAgERHn9Ha8yZSnxEAAMQv+ozWnjeR+ozEGQHEMxK6AQBIQIcPHw56qaVgZ9RaRWYnf/TRR6pDhw6mf1+1apXtziszzbNnz+62be3atQHP2b9/f69tstTWqVOnVCjGjRsX0v3nzJmjL5hFsWLF1AsvvBDS/gAAIH7Rb7T2vInUb3z44Yf1srJya9iwoYqW48ePm27Pnz9/1M4BAECio89o7XkTqc8IAADiF31Ga8+bSH1G4owA4hkJ3QAAJCBZdslM6tSpw14iK1wyq3bo0KH69vzzz4d9nBdffNF0+9GjR211XueSUHXr1vUKYuzZs8fvuapUqaJatmzptk1mMg8fPjzo9n7++efqs88+C+liWZbzkqBIlixZ9L45c+YMen8AABDf6DfSb7Si3/jvv/+qaPG1BKwsGQsAAKKDPiN9xmARawQAIHHRZ6TPGCzijAASGQndAAAkIF8znY8cORLW8WR5p3AdOHBAjR8/Xt9eeumlsI9TqlQplSdPHq/tcpFnp/M6tWjRwmvb/PnzA55v8uTJKleuXG7bpkyZop599tmA+y5YsMC1DFeqVKn83vf06dNqwIABqlOnTnr5KHk80r4aNWoEPA8AAEg56DfSb7Si37h///6oDbYsXbrUdPvtt98eleMDAAD6jFaeN5H7jAAAIL7QZ6TPSJwRAAIjoRsAgARUpkwZ0+0rV64M63jfffediobz58+rv/76S0VToUKFbHnejh07es04nzt3bsDjFi1aVC+llS5dOrfto0ePVo0aNVJLlixRV65ccW2Xmctr1qzRF8pt2rRRFy5cULVr11bt2rXzeVErM8KLFy+u3nrrLb2tZMmS6pdfflFNmjQJ2D4AAJCy0G+0/ryJ2G+Utnz55ZcqUvJ+27p1q9f2AgUKqFq1akV8fAAA8D/0Ga0/byL2GQEAQHyhz2j9eROxz0icEUC8IaEbAIAEJMv+VKxY0Wv79u3b1apVq0I6lsyOffvtt6PWNllyKRyyJNTx48fdtqVNm1Y1bNjQlufNnz+/12zdX3/9VW3evDngOVu1aqVnM2fNmtVt+/Lly/XM6uzZs+uL3HLlyqkcOXLoC+SPP/5YX7A2bdpUffvttypz5sxeS2NVrVpVFStWTI0cOVIvU5Y+fXo1cOBAtWnTJr2cFgAASDz0G60/b6L2G8eMGaMuXrwY9v6XLl1Sjz/+uOnfBg0aFLAiEAAACB59RuvPm6h9RgAAED/oM1p/3kTtMxJnBBBPSOgGACBB9ejRw3T7Y489pi+Cg/XEE0/oWbPR8vLLL4e1tJZztq5Ry5YtTZebsst5hw4d6rVtwoQJQZ23WbNmasOGDeruu+/2+ptckMoyYzt27FBnzpzR27Jly6Yfo1wsywW1p3Pnzqnff//dteTZkCFDdABl4sSJAZfnAgAAKRv9RuvPm4j9RnmvdO3aVV2/fj3kfa9du6Z69uyp1q9f7/W3ChUq6PcuAACILvqM1p83EfuMAAAgvtBntP68idhnJM4IIJ6Q0A0AQILq37+/XrbI0+rVq/VyS2fPng14jGeffVa99tprUW3X0aNHVfPmzfUFX7BmzZqlLwaNMmXKFPTFp1Xnve2221TNmjXdtslyVdu2bQtq/9KlS6t58+apjRs36iWt6tWrp5e8ypgxo8qQIYP+WWZLy0W9XKgOHz7ctYyW/F/aesMNN+iZz7LElcxOlqWr/vnnHzVu3DjT9wcAAEg89ButP2+i9htlyVcZjDpw4EDQ+0ibpCqQVADylJSUpGbPnu21PCwAAIgcfUbrz5uofUYAABA/6DNaf95E7TMSZwQQL1I5ZG0DAACQ7GTm5tq1a922nThxwvSioUiRIip37tymx5GLo2iRmaSy9JLZxXGJEiX00kEywzZfvnyu7efPn1ffffedGjt2rGv5K7loNFtmqFSpUm7LLhUsWFAtWrTI7T4//vijaty4sde+MutWnrNOnTqpatWq6QtAo1OnTul9p06dqhYuXOj2N7lwmjlzpmrfvr3Px27VeT3JMlb169fXy005NWnSRD/HKXF5pjvuuEMdOnTI50Wx2Sx0z/eRUZs2bdRzzz0X9XYCAGAl+o30GxOx39i9e3c1ffp007/JQI88V/Jc16pVy6vakFQAkvfYZ599pj744AN14cIFr2PIQJE8VzfddFOyPQYAAGKJPiN9xkTsM3oi1ggAgH/0GekzJmKfkTgjgLgmCd0AACD5NWzYUK6IIr5F208//eTInz+/33MWKFDAUaFCBUexYsUc6dKlc21Pnz6945133tHHCabtsr+nXbt2OW6++Wa/+6VNm9ZRuHBh3YYbb7zRb3vz5cvnWLx4ccDHbdV5zfTo0cPreG+++aYjJZL3QDQ+B85bt27drH5IAABEHf1G+o2J2G/ctm2bo3///o4cOXIEfH8kJSU5SpcurZ9vec8Z32tmt+7duzuOHj1q9UMEACCq6DPSZ0zEPqMnYo0AAPhHn5E+YyL2GYkzAohnJHQDAJDgF8zi33//1RcfcgEcbDtuu+02x6ZNm1zHCPeC2Wnjxo2OgQMHOkqVKhXW85InTx7HsGHDHKdOnQrpsVt1XiPZ1/P88lqsWLHCkdIwyAIAQGD0G+k3JnK/8dy5c44PP/zQce+99wY16OLrJvs+9NBDjtWrV1v9kAAASBb0GekzJnKf0YlYIwAA/tFnpM+YyH1G4owA4lEq+Y/VVcIBAIA9HD58WM2dO1etWLFCbdq0SR07dkydPn1aZc+eXS9rVbRoUdWsWTO99GTp0qWTrR27du1Sy5YtU5s3b1Zbt25Ve/fu1e2Q2/Xr13V7cuTIodtQtWpVdeutt6rmzZvrJaXi8bzO5cXq1q2rLl265NqWlJSkfvjhB5ZrAgAAtkO/kX5jLFy9elWtXLlSbdiwQT/P27ZtU4cOHXI91/L3bNmy6ec7Z86cqmzZsnpZ2urVq+ulezNmzGj1QwAAIKHRZ6TPCAAAEAh9RvqMsUCcEUC8IKEbAADAJubNm6fat2+vrl275nbRvHTpUn2BDgAAAAj6jQAAAAiEPiMAAAACoc8IAPaS2uoGAAAA4H/uuece9d5776nUqf+/iyaz0GWmtcxMj4UzZ87omfAAAACwL/qNAAAACIQ+IwAAAAKhzwgA9kJCNwAAgI10795dzZkzR2XIkMG17ezZs6pDhw6qb9++6tSpU8l27o0bN6oaNWqojh076iW8AAAAYF/0GwEAABAIfUYAAAAEQp8RAOyDhG4AAACbadeunfr+++9V4cKF3bZPnjxZlS9fXs+SvnLlSlRnPY8YMULVrFlT7dixQ61YsUJNnTo1ascHAABA8qDfCAAAgEDoMwIAACAQ+owAYA+pHA6Hw+pGAAAAwNuJEydU79691eeff+71tyJFiqh+/fqpzp0765/DceTIEX1hPHHiRHX8+HHX9p49e6o333zTbRY2AAAA7It+IwAAAAKhzwgAAIBA6DMCgLVI6AYAALC5xYsXq0GDBqnt27d7/S1VqlSqTp06qnHjxqp+/fqqQoUK+gI6dWrvhVhkaazNmzerVatWqYULF6rly5e7zaSWGddvv/22at26dbI/JgAAAEQf/UYAAAAEQp8RAAAAgdBnBABrkNANAAAQB+TCdtasWWr8+PFq06ZNfu8rM5fz5MmjsmbNqn++cOGCOnnypNssZ6OkpCQ1dOhQNWDAAJUlS5ZkegQAAACIBfqNAAAACIQ+IwAAAAKhzwgAsUdCNwAAQJz55Zdf9MXzZ599ppelCkf69OlVw4YNVbdu3dS9996rMmbMGPV2AgAAwFr0GwEAABAIfUYAAAAEQp8RAGKDhG4AAIA4Jd24rVu3qhUrVqiNGzeqnTt3qr1796r//vtPnTt3Tl27dk1lypRJZcuWTRUqVEgVK1ZMValSRdWoUUM1aNCA2c4AAAAJgn4jAAAAAqHPCAAAgEDoMwJA8iKhGwAAwMTVq1fV6NGj1eLFi/XF5UsvvaSXhwIAAACMpL8o/cbcuXOrt956S5UsWdLqJgEAAMBmDhw4oPr3768OHz6sRowYodq2bWt1kwAAAGAzV65cUSNHjlTffvutaty4sRozZgzj0wCQYEjoBgAAMDF37lzVoUMH1+9TpkxRvXr1srRNAAAAsJezZ8+qvHnzqosXL+rfmzVrppYsWWJ1swAAAGAzsqT8F198oX9OmzatXqZeJgQCAAAATp988onq3Lmz6/dp06ap7t27W9omAEBspY7x+QAAAOLCmjVr3H5ftWqVZW0BAACAPW3evNmVzC3oMwIAAMDM6tWr3VYGXL9+vaXtAQAAgL37jIJYIwAkHhK6AQAATHguYsKiJgAAAPBEnxEAAADBoN8IAACAQOgzAgBI6AYAAAAAAAAAAAAAAAAAAAAAi5DQDQAAAAAAAAAAAAAAAAAAAAAWIaEbAAAAAAAAAAAAAAAAAAAAACxCQjcAAAAAAAAAAAAAAAAAAAAAWISEbgAAAAAAAAAAAAAAAAAAAACwCAndAAAAAAAAAAAAAAAAAAAAAGAREroBAAAAAAAAAAAAAAAAAAAAwCIkdAMAAAAAAAAAAAAAAAAAAACARUjoBgAAAAAAAAAAAAAAAAAAAACLkNANAAAAAAAAAAAAAAAAAAAAABYhoRsAAAAAAAAAAAAAAAAAAAAALEJCNwAAAAAAAAAAAAAAAAAAAABYhIRuAAAAAAAAAAAAAAAAAAAAALAICd0AAAAAAAAAAAAAAAAAAAAAYBESugEAAAAAAAAAAAAAAAAAAADAIiR0AwAAAAAAAAAAAAAAAAAAAIBFSOgGAAAAAAAAAAAAAAAAAAAAAIuQ0A0AAAAAAAAAAAAAAAAAAAAAFiGhGwAAAAAAAAAAAAAAAAAAAAAsQkI3AAAAAAAAAAAAAAAAAAAAAFiEhG4AAAAAAAAAAAAAAAAAAAAAsAgJ3QAAAAAAAAAAAAAAAAAAAABgERK6AQAAAAAAAAAAAAAAAAAAAMAiJHQDAAAAAAAAAAAAAAAAAAAAgEVI6AYAAAAAAAAAAAAAAAAAAAAAi5DQDQAAAAAAAAAAAAAAAAAAAAAWIaEbAAAAAAAAAAAAAAAAAAAAACxCQjcAAAAAAAAAAAAAAAAAAAAAWISEbgAAAAAAAAAAAAAAAAAAAACwCAndAAAAAAAAAAAAAAAAAAAAAGCRtFadGAAAAAAAWOfAgQNqzpw5fu9TqVIl1aJFC5WIvvnmG7V582a/9+ndu7fKnj17zNoEAAAAAACQnMaNG+f37zly5FC9evWKWXsAAACAREJCNwAAAODDuXPndDLfX3/9pY4cOaJ/dzgcKlu2bDqBLykpSZUrV06VKFFCpUmTxurmJqxff/1VrV69WtWqVUvVq1dPxRu7t3/Lli2qe/fuau3ata5te/fuVcWLF7e0XYjc7t271bBhw/zep1u3bgmb0D179mw1ffp0v/dp164dCd0AAAAAAJjEHBYuXKgKFSqk7rnnHpU6NQuHx4tAsaJixYqR0I2ou3jxolqzZo3atm2bOnHihP7OkPGXihUrqho1aqi0ae2Z2nT9+nX9fbd161Y9hvTff/+pq1ev6okPuXLlUhUqVNC39OnTW91UAAAQJ+zZ6wEAAAAsIgFDSeL7+uuv1aZNm9S1a9cC7pMhQwZVtmxZnYzbqFEjfcuXL19M2pvoXnvtNTV48GDX7xMmTFCDBg1S8cLO7ZfA8yuvvKKeffZZdfnyZWVXo0eP1m2MBhkgCFSROVrke2L58uUq1kjGBwAAAAAgPuMan3/+uWrbtm2ytmfkyJHq+eefD3g/mQD+4Ycfmv7t559/1pPDpTiGuPvuu9W8efOi3lakfD/++KNq3Lhxsh2/YcOG+hxmJH62b9++iM+RLl06XQwgZ86cujhMtWrVVMuWLW1Z2MMK27dvV2PHjlWfffaZ6zvDkyRGP/DAA2r48OGqcOHCykpS8EcSz5cuXaq+//57XajlwoULfveRZO5mzZrpoilMcAEAAIHQUwAAAACU0oE3GWiQagnPPfec2rBhg1cytyRumwXbLl26pP744w81efJk1alTJ5U/f35VvXp19eKLL6odO3bE8FEkFql2MWLECLdt8rtsjwd2br9U5b7lllvUU089ZetkbkSeVC6DEMabVFnC/8jAtPG5mTZtmtVNAgAAAAAksDFjxiTr8U+fPq0mTZoU8XGGDBnilpj55Zdfqm+++Sbi4yI2PGNFkryP8F25ckUdP35cV3FetGiR/hzXr19fF4j54osvVKKS95YkcleuXFmvkGf8zpBq3MZxmJMnT+rvpvLly6sPPvjAkvYePnxYV6+XRP86deqoZ555Rk8G8EzmljGkVKlSuW2T+PqCBQv0Sn+y78aNG2PcegAAEE+o0A0AAICEJoHCxx57TL3//vs6iOiUN29e1aFDB3XbbbfpoGLRokV1IFHuc+bMGXXgwAH122+/6UoMUh1HkrqNJCFcbpIQK5UXSASMvl27dnk97/K7bJdlGO3Oju2Pl6rcKcHixYvdJo3s379fVwj3rD7z5JNPRnwumVwiAx8AAAAAAMBepGpr1qxZ3bbNmTNHrV271uu+69ev1wmhd9xxR7K0RRImzQoNmMUnKlWq5LdQgCdZEU2KadjJxIkT3R5v1apVdTVxJI4cOXL4/JvE9U+dOuX6XRKypaCLUcmSJVXfvn1N97948aIeR9i5c6eu6Hzw4EG3v8v2e++9VyfMS5JyIlVtljEWGTP56KOPXNsyZcqkhg4dqrp27apKly6trl+/rr83pk6dqt555x0dRz179qx66KGH1J49e9QLL7wQ84JA48aN89peokQJ1a9fPz2OJAnnktAtbZXxo59++km99dZb+vV3kjElSeifP3++atKkSUwfAwAAiA8kdAMAACBhSRC2VatW6s8//3Rty5Ytm15WtE+fPjr45kmqK8gSiZJ4KTcJPMpghwxqTJkyxS0p3Fi9AdEnAXNJspckZCf5XYKo8cBu7ZfBNnk/GwcMZQlQCZTLAAOiSwYpjDJnzux1H/mukYGMSL355pskdAMAAAAAYEN169bVNyNJYjRL6BaSxJgcCd1S9EISnM2EGp8oV66cTj43kmrEdiOPd9++fa7fJbGWhO7E0qVLF59/69Wrl9vvUo3ZM6G7SJEiQX02ZMxg2bJlavDgwer33393+5tUp86SJYtO/E0Uw4cPd0vmTkpKUt99952qUqWKa5skuEuhHRl7adOmjb5JkryQKucFChRQ/fv3V1aS5HKJu2bMmNFte5o0aXQlb7nJe0yKbTz99NNu37fyeCQez2qJAADAU+JM8wMAAAAMtm7dqishGJO5b7zxRh1QHThwoGkyty+5c+fWwVxZIjFdunTJ1GKYPe+jR4922zZy5EiVJ08eFQ/s0n6pGCJB5erVq7sGC9OnT6+ee+45XT2kYMGCKl7s3bvXa1nYUG4yYAoAAAAAAGBXK1euVN9//33UjysVcI8dOxaVY8nqb8YER6lcK0U1gEhIwn0kcT+5NWjQwHU8iXnGKoFfisRINWap8mxsg/HzZ6zinJItWbLErdK1PDeffvqpWzK3p9tvv10nThtJcryVsdzWrVvr6uGeydye5PFJtXcpIGS2ciwAAIAnEroBAACQcKRitizx+c8//7hVS5Yl8CKpjiwBYKmogdiRYOjy5ct1EFiqnDzzzDMqntih/bL8o7Tj8uXL+vebb75ZJ3ZLW6RiOAAAAAAAAOyxwpezSnc0SdXb8ePH+zxfqJo2baqLZkgF7NmzZ6vFixfraruA1QVeJP7v1LNnz5jHPqWIzLRp07zOK8nmzs9gSnb9+nWviuadO3dWjRs3Dqoadp06dVy/Syz7iSeeUFaQ77PXX39dJ2sHS4qneCZ/L1iwQB0/fjwZWggAAOIZV04AAABIKBI07Nixo05idZIA6ty5c1XevHkjPv59992n2rVrF/FxEDypajJkyBDVqFEjFY/s0n4ZUJDlKletWqVuuukmS9sCAAAAAAAApXr06OG17ccff1S//PJL1M7x3nvvuQpfSJJrNJQtW1avgihxWAoGwA6kCraTvCd79+5tSTuksIxMevC0dOlSvZJiSvbJJ594VdUePnx40Pt73nfhwoVR/S4MJZ4famEgGXuqW7eu27arV6/q73MAAAAjEroBAACQUKQyzIoVK9y29evXT1WvXj1q53jxxRdDqs4AWK1mzZpq3bp16sknn2SQDQAAAAAAwCZq1Kihmjdv7rX9+eefj8rxpcrtK6+8on/OkyePevjhh6NyXMBOzp07pz766CPX723atFGFChWyrD3GStNOJ0+eVAcPHlQp2UsvveT2e9WqVUMqLNK6dWuVK1cuv8e06vULRoUKFby2pfTXHAAAhI6RegAAACSMEydOeA12pEuXTo0YMSKq5ylTpoxq1qyZWrJkSVSPCyQHGbxYuXKlSpMmjdVNQTL59ttv9QCtsHKwCgAAAAAAhO6pp57yijPK72vXrtUJ35GYPn26ayXDxx57TGXJkiWi4wHRVLhwYV3pXdSqVSvs48ycOVOdPn3a9Xvfvn2VlfLly2e6/ejRo6po0aIqJdq+fbvasmWL27Y777wzpGNIIRKZ4DJ79my3yuZnzpxR2bJlU8mpRYsWru/KnDlzhnWM7Nmze207f/58xG0DAAApCxW6AQAAkDAmTZqk/vvvP6+qDvnz54/6ue66666oHxNIDjKpgWTulE2WOa5UqZK+yesNAAAAAADix6233qrq16/vtf2FF16I6LjXrl1zVbeVRMMBAwZEdDwg2kqXLq1X3JTb/fffH/Zx3nnnHbc4WdOmTZUdyWcypfriiy9Mv9tC5bnPpUuX1MKFC1Vyy5gxo55gILesWbOGdQzPsSl/yf0AACBxUaEbAADAROrU7vPerly5YllbEL1g6JQpU7y233PPPclyviZNmriSZCNNlj106JBat26dOnLkiK7SkTlzZpWUlKQr7dauXVtlypRJJbfr16+rNWvWqA0bNuhK5xK0LFmypKpbt65ejjUQqTTx888/q23btuklLmWfEiVKqAYNGuhgqN3t27dP/frrr+rvv//Wz4W0X4L/srxi+vTprW4eYEsXL15UW7du1Z/748eP62o5UulLlkYtXry4qlmzpv4+iyapRL5582Zd8Ue+q86ePavPmTt3blWuXDm9jGu0z+npzz//1N+Xhw8fVqlSpdLf15JMf/PNNzN5IgXy7CN69iEBAAAAQawR0ajS3bJlS7dt8+fPV3/88Ye+1g3HJ598ovbs2aN/7tevn646a5ZwaCU7XOdHm8RJV61apeMH8nxLrEAS6vPmzavjraVKlVI5cuQIeJyDBw+qTZs26arBchyJWUrMReKW1atX18eC0isTbty40a06t8RrrCRxfjPJUXjGLr7++muvbdWqVQv5OGb7yLE7deqk4qFKuScZXwCMiDUCAEjoBgAAMOEZMLVbIBuhk2RiSYz2dNtttyXL+WRA4erVqxElQr711lt6OUhJovYlQ4YMukJP//79g05Ob9SokVq+fLnf+zgcDtfPH374oRo9erROavYk1X7bt2+vq/kUKVLE6++SvD1mzBj9WIzLWjpJMvrDDz+sRo0aFdRShdKWBx980O99pk2bprp37276N3kczz77rN/9ly1bpp8jIUncTz75pM/nSwaQZMBL7hOL9gOhfL6N7+VY2bVrl/r000/VN998owfM/H0PyjKp8v316KOP6lUNIglOy6SX119/XVf7ke8dX+Qcshy1DILffffdqmrVqipapBrQM8884/M7Wwabhw4dqpewjsVEHMSGZx8x3GV3AQAAkPJjjTJJ3IlYI0LVokULnaS7fv16t/idVOmeM2dOyMeT5N8XX3xR/yzXqIMGDQq7bX/99Zcu3OBPt27ddFzMyut8mVxuFt90mj59ur6FEjMNJc63f/9+HZecPXu2Tur2pWHDhurHH380TW5fsGCBvi1ZssQ01u2ZHNylSxf1yCOPmMZtE4WxOrck/9sh7ioxZ09S+blo0aLJcj55PzVu3Djo+/t6DwY7tuD5WZGfZfKBZ5zshhtuUKGqUKGC1zZjwr5dyb/7MkZlJN9X5cuXt6xNsCdijQAApvIAAACYkGCSkbNSCeLXokWLvLZJgNSOVS+++uornRAuiX+eiYGSQO25pOD333+v2rZtqwcvpBJutMggQceOHfXAhK/BDqkWINV8Kleu7BWQlOowUkF87Nixpsnc4sKFC3rJTKn0LdVs7eTVV1/VSzj6C1DLgJLcTyppyOOFfRw7dkx99913eiDujTfeUO+++6767LPP9OsplaIRXXv37tWDPWXKlNFVw1asWOGWzC2Vj2QCipH8XQaH5PtLErtlYDOc1Rdk0LlWrVpqxowZXoO8ck5j1SXnagMygCoVfWQwXL5zIyGDUgMHDlStWrXyOwFHKonJ5A+ZSETyRsp67xtJNTQAAADAE7FGRINcU3qSWIdZ1ddAZD+pEC169uwZVmJlcrDDdX5ykCRsiZ9+8MEHfpO5fZH4Y8GCBdW9996rk8Q9k7llBUHPifL//POP3k9iNVJsIxHJinFz5851/S5VnK1OjpRiCFKEwVOPHj1USiVjC56fZZlgEe4EKc/XcOfOnbZf+UIm38hYjpN8j0mRHMATsUYAABW6AQAATHguU7l161a1Y8cOVbZsWcvahMhIpdhgqjlYbcKECTqR21jBQpKKJVlQql/IkpmSaL179269lOC4cePU0aNHXZVrJDFaKtf4q7ghVUiMVXslodIsablr16660q4s+ymJmhUrVtQVqSXpcunSpa7zCklObNOmja4SJMFY+b1Zs2Z6YChr1qz6Z1nmUwZZZJBJktClCrmTJKJLpW9JAvW35GXNmjX1QITTyZMnXdWEguFZrVgqCJlV3nnzzTfV8OHD9c/FihVTTZo00cn/sqyrLGUryevGZFV5TDKgIsul+qsyHGn74Z98NuS1kwpJ8nnwVxlaBvjkPdenTx/9HkXkAzM//fST2zb5zpBq1E2bNtXfC/LZls+9/JsqFbzffvttvTyw8zv65ptv1v8vXbp00Od94IEHdGUrJ6k4Jeds3ry5HrCUAU353pEJF/JdJ9W9jEnX8vO8efN0hfBwDRs2TE8acP67Uq9ePZWUlKQnssj7UAaWpQ3GKkwPPfSQ+vzzz8M+J+zD83WUAXoAAADALNYoMQ8nuQ55+umnLW0T4o9MiJZqrsaCDs5K24EqSxtJ3FFW1HMWj5Dr2kgnLBjjXULaJHGvUCXndb5MQD916pTPNkqxDCluESypriurDjpJPFTa40niqPLaScKpPN8NGjTQxTwkziqTOyRG4q8KuXNVMElOdpLjSGzh/vvv14nsEtuS1/XIkSM6Pinvhy+//FLfV5JIBwwYoBOJX3vtNZVIJIHeGIOWlRatJG2R2Lzninby3h4yZEiynbdUqVKuz6i8R2RMwTOGZ6xc7q+ie9++fXVRAyGfRVmtTmKy8p6UpGWz2PiWLVu8thUoUCDsxyNxemOxBPlsSVK3Hcd7hEzCkHEfI3ne5HsNMJLVXFavXu13vBoAkAAcAAAA8HL16lVH/vz5JaPWdXvhhResbhYikC1bNrfXU279+vVz2Mn48eO92jhmzBi/+5w8edLRoEEDt32yZMni2L59e9DnHTVqlNd5p06dqv9fr149x65du7z2OXfunKN3795e+7Vv317/vUuXLvr3Xr16OU6fPu21/759+xw1a9b02n/mzJmOUOzdu9frGNOmTQt6/2XLlnntP2nSJEe6dOkcuXLlcsyaNct0v61btzoqVKjgte8HH3wQ0/bHQsOGDb3aKO22C+P71/N7O5hb7ty5HW+88YbVD8P0vVCsWLGIXi95f4dKzmlsQ7du3cL6LPXo0cNx5coVv/ucOnXKcffdd7vtd+ONN+rvl2DMmDHDbd9q1arp78RA/74PHTo0rMcon03P12jy5Mn6/0WLFnUsXbrUdL+1a9fqv3vu+8MPPwR1XtjXoUOHHKlSpXJ7Xb/66iurmwUAAAAb+v77772uCcziLYCQ61RfcaKPPvrI672UNm1ax549e4I+/pdffuna96GHHkqW+EQ48YVYX+eHGwPxxey5e/XVVx1JSUmuWMnRo0e99tu/f7+jUqVKrn0ktuMv3pMnTx7HmjVrArZn8eLFOk4cSezS8/0Y7vvBCtevX3eUKlXK1e5atWpFdDyzOLLZa+WrLRI3Mr7OzlvZsmUdO3fudMSKtEXOaWxD1qxZTWP4wbzHnMdo166dz/tJ7NXzcUfyefMcD5Hb119/7bCb9evXOzp27OjWzvTp0zsmTJhgddNgUxMnTnR7v+TIkcNx6dIlq5sFAIgxKnQDAACYkIrEUm3XuBTh1KlT9fKT+fLls7RtCN2JEyfUmTNnvLbbZSlRIbPun3jiCbdtjzzyiOlSqkayvKBUaKlUqZKukCukootUZ5FKt1IZIxwjRozQFYyXLFmiq8V4ypw5s5o8ebKutCvVqo2VSqUKjiyJKpWP33nnHdPjFy1aVFcYl0q8UvXa6b333tNtt5Is0SoVnKU6jizvakaqIclSqVJNx7ic4/vvv68efPDBGLYWnkvJCnlfde7cWd155526ooxUipLvAalsL58X+T4/duyYvq9sf/TRR9Uvv/yiXz+z97tVpMKzZ8WcQKRCldXku+Pdd9/VnyN/smfPrubMmaNuueUWXd1fSFV/+W4ZPHhwwPOMHz/e6/dAy+bKv+9SkUgqAy1evFhFSirqyb8lUhWsRIkSpveRyuPy3SgV+o3k/eZvNQXYm1Q+k6VxjStqyHtaVqQAAAAAPElF3rx587qtdibVgSUO4m+lMsDTfffdp6tC792717VNqv2OHTtWTZkyJahjOKtzyzWyZzzSSna4zo+2V155RcegRo8e7VbN20hiV7LinOfKgr5IXMszxmCmRYsWulJ3u3bt3GK+nTp1UpkyZVIp3bfffqtjgcbK0tEmcThfsTupjC6xPakcLau3OVepc5L3dv/+/dXjjz+usmXLpmJF/s3p3bu3XiXUSeLzM2fO1PH8UEgMz8nfvvI8eJIYSrjMni+zc8SKrGgqK506X3P5fpZYp/H9J+M0MuYoYw+sBAwzMk7hOZ52991365UpAACJhYRuAAAAH9q3b++W0C3JspJ4tWzZMpK648yhQ4dMt0uSpx3I0oSypKExMVgSBCVRLBiypKYMDhiXBF23bp1OlJRlSsMhS3hKQrO/5FYJ/krSpTGhWx5Lly5d9LKHgRJR5XMk9zUGqSQpUoKekQR0IyWDLCNHjvSZzO0kyZsShDUuAytJwbLcY6DBJiQPGcSTBFuZCOEZ6JT3pNzq1aunlxJ++OGH1dy5c11/l8/LhQsX9HK0dhlMl2V/I1322AqyRGygZG4neZ1kULNNmzaubfLdMWjQIL+vgywPu3HjRrdtderUCbqNcvxoDPTK94UMjvpK5jYu23zrrbe6LbG+aNGiiM8Pa0gSt3zPvPHGG27b5X2cMWNGy9oFAAAA+5JrpLZt2+rJr04ffPCBnjAv/Uq7XIciPt5Lw4cP90pOlWtTiWcVLlzY7/5SvOG3335zxb9lUrwd2OU6P9pkEodM6PCVAlVqMgAAIaBJREFUzO0k8Sp5bSU53x9JBL3nnnuCPr/ELmXivXMivTzP8l4JNXE3Hr399ttu4wCSyB5te/bsCSl2J3GwVq1a6bGA5s2bW5aoKeeXGOrFixdd2+Tfp1DeF5KgLgVPRJkyZVSTJk183tdY0MUpkqIaZvuanSNWduzY4fN9kCNHDp24LwVFGFeEv2Tu2267TW3fvt1tu/w7DQBIPKmtbgAAAIBd1a9fXydgGW3btk1Vq1ZNVy+QisrGqoywL6lYbSZDhgzKDiSBVKrSGkmyqQzqBUsC+Xny5HHbNmnSpLDbJAMNUlU2EAkyeQ46yvMt7Q8mKOtZyVQ+U3/88YeyklTLkEGgYNx+++1e2zZt2pQMrUIg8j6cNm2aTg4ONBiSK1cuNWvWLK+A6Pz58/XgJ0InA8YDBw7UNxmQCoV8jiQZ3+nw4cMBvwfMKpFLFZxgBVPJKhgFCxYMeuJM06ZNvRL2pYIP4msVAJns17BhQ69JV/IdJINzAAAAgC+yEpvn5Fepylu3bl31+uuve1VvBXyR1eEKFCjgtu3y5cu64EMgL7zwgusaJtDKgLFkl+v85PDcc88FvI98N0iFZIltmVVNlyrbEnMJJ27VsmVLr8rVKZ28n2SVPuNnxg4TsOVz+sUXX+jXU5LtJT557dq1mLdDxhE846IyoULGvIIlK8852y4Vv/1NTDJbPTXYYhBmzPY1O4cdnDp1Sq/IUaFCBZ3Y/fvvv1vdJNiEjIVJYSb5zq9atarasGGD29/lPcNKgACQmKjQDQAA4IMkl0lwTapyG5dGk0QzWe5RblLtVZLYpBpvJAGoRCcJcVJVxjOBPlp8Bf/t8pp5VvgUUrUp1CRkqeQiCalOspSjVIcIZwm/u+66K+jlDeX18xx0DHb/G2+80WubtFkei1WkmkiwFbYlqOZJvi8kIR6x0bVrVz0BR5K0g5mEYPyOl8ozy5cvV//++69ruwx+9uzZUxUrVkxZTdrw119/hbSPLM8rjynWpKLXxIkTw9pXBtSSkpJ0lSgnqfxfuXJln/vIagBmk2OkwlAw5DPurEgm5w6XfNelTh3cXPny5cubfl8ULVpUJRepTjRhwgS1atUqJqGFSZ43eR6lUo5MvjJ7HmXQUgYy7ZxAAAAAAOtVrFhRJ2zef//9bgl80l+X22OPPabKlSunE+0k3kLV7vBJ31xWlUupK6hJkQopOCIrZBlNnTpVPfXUUz6rwMrKeM6V9lq3bq1uuukmZRd2uc6PNhk7CDZO2KFDB59/GzBgQERtMDKutphSTZkyxfU9K9+lyVWRXCZ8y+fKF6mALas5SnxPvuc//vhjncApid1S3VpuMslCqonLsWJJnpMZM2a4bZs8ebKqXbt2wH3luX3vvfdc30eBPqeyKqInY3GHUJmN65idI1ak6I0zXiSvuax+umvXLh2jlUka8vpLXEleZ3mOJfb86quvWrpKabikmI1MRjObhIPgyOdHvhdkZWFfk/nke1vG+mTcDwCQeOyRwQIAAGBTRYoUUcuWLfNK6jZWapQbIifBCUmU8qwyHQ2+KnGfP39eWU2qWf/yyy9u22QJyCpVqoR8LEl+NCZ0CwkUh5PQLZXog3XDDTe4BZ4kyFSpUqWg9pVkcLOqFVYKJSHPLBHT6vYnmpIlS+pbOCQJXAbMjRWpZEBFqqYYl8FG8vOskiQBbX9KlSqlB+SMybUyWC/bb7311qDOGY1JRKEcw4rvCxng572cvJzJ3FLtCwAAAAg2YdMzqdtp+/btFrQq5fnmm290LFcS6FMqWR1P4heSOOgkiYTjxo3TiYL+qnMLO1XnttN1frTdcsstlk/O8Iy5HD16VF29etU2xU6i7cqVK65kY+fKcFKIwKrnXooCya1OnTo6DilFhB566CGd0Cm2bt2qE4IlftOjR4+YtU1Wh5DxBONqk3PmzFGvvfZawMkwixYtciX0SqXxQJMoMmXK5LUtksrkZvuancOq17xQoUL6Jkn68l0rK6kOHz5cf+5k8opMOJBq6IsXL/ZabcHOpAq6VPwPFLdFZCSZWyaKyL9/AIDElDJ76QAAAMmQ1C0DLYlQvcIqx44dU3///XeyJHRnyZLFdPvp06cjOq5Ulw5loM2squevv/6qA3mexw0n0G8WOJWEbqlgHKpQksA9n9/ixYsHXTkgc+bMtlseUaphBcusiobV7UdounTp4jWI+emnn6p33nkn6MrLMCcDFDt37lTbtm3Tg1TynSsJ88F8HxsHpM3IvxVSTf/77793bTt58qQeKHEuQxyLSv+hfF9kzZo15t8Xnkt1IrpkAtZbb72lOnXqZHVTAAAAEGdJ3VJ8oFevXjqxEskjpV8PSTxOrn1Hjhzptl2qvz7xxBNeMVaJETqvoZs2bRpUFd5Ysst1frQFW/QiHDJJ/Pfff9eFNiSuIqtLmcWf165d67VN4i6+KrnHu3nz5rkV4enXr5+yE1mZUwqkSBEh57iA/L937946ri6fg1hW6TY+P1Llevr06frz5o+xeEAw1c9l1QlPnmMioTDb1+wcdiATJwYNGqTKlCmj2rRp4/qMyme3VatWenzIV0Eiu5E+C8ncyatWrVrqk08+IZkbABIcCd0AAABBJnX/9NNP6o8//tCJfnPnzlU7duywulkpilSElkTm5OCryoEdEm83b95sWrVbqumEas2aNV7bZDm/cISy3J/n8oih7GtWCSaS6hzREMpyvNGuLhKMUJP9R40apUaPHp1s7UkJFS9KlCih9u7d69omycey/KlZtXapjiEDLqGQSUGNGjVSiUKqy3z44Ydq4cKF+vssHMEskyrfk1LpSiqQOcmgiPwbLTcJfMugrwyWSBWk5EjQz5Ejh62/L2Qymtm/DQifJObLe6p9+/aqefPmtqkABQAAgPhy1113qRYtWqilS5fq65cvv/wy4sID8L4eSukeeeQRfW1sfO9IUq9U2DVW4xbPP/+86+ennnpK2ZEdrvOjTVaHi6bDhw+radOm6erzUtk5XMHEXcJldexMijQYx3UkadZu6tevrxOh33zzTbcYkVTuluIIvqqnS8L3vn37gj5Pt27ddIzOlwceeEBXjpbvDSepHu0voXv//v069icqVKgQVAV9syIH4cYMhbG9/s5hJ/I+HDJkiNu4z/r163XcfuzYsSoeyOqDUtldktARPbL6qMQZZcKfjJNavaoDAMB6JHQDAAAESS6iZQk6uUkAXAKmf/75p64UItUwzKpfIDgFCxbUAa3kqkQgFV6kQoNnArdUBY/E448/7rOK7LBhw4I6htn+Up1BbtEg789oVjVP7n3tIJT2eyazIz5JoNSY0C1k2UuzhG74JhOdpJrQ8uXLY3K+qlWrqvnz5+tgt3OZWiNZXvvll1/Wt7x586o777xTLwMrSbjBriIQ798Xjz76qH4fr1y5kn5KBGQ1CRmEdy6RTBI3AAAAokHiYBIPk9ulS5d0BWVJ1pRYTiSJbvhfhUtJmEzppCiBVNd96aWX3LZLkqjEJp2TkKVa+aJFi/TPkjAdarJtrNjhOj/aopVkKtf0kqgvhRvMklnxP7JKnCSUO0mcyq7xW/nsGhO6ncVZ5syZozp37hyTNsiYiUx+kSRuJxn3kuJGDRo0MN1n6tSpelU+8fDDD1uyyqXZvnat0G0kyfOy0ptxQoXz+1pWgbM7mWiwZMkS/e+JJPYj/LFm+fdZYo1Suf2mm24iiRsA4IaEbgAAgDDIxXXFihX1DfFBBgRWrFgRsDp2KB588EGff4skoTuazAY/ghFJACneg0/x3n6EzmyJ2X///VfFI+OgVSxJRZlmzZp5fafJcshSXUgGjAsVKqQHMs0+Y6FWGHK6/fbb9WCdVBebMWOGunLlis8lQaUikdxkgEQG9GS5U1niNiV/X0j7pHKO3AAAAADYO7m7YcOGVjcDcUiubV9//XW3BEEpPjJp0iT19NNP69+N1bqffPJJZWdWX+dHWzTiBpI8K4/v/fffd9suCYESc5HE9kqVKukE//Tp03vtL8+Rvzh2SmKszi1J/r169VJ2Vb58ef1+9YxByop3sUroFn379nVL6BaTJ082Tei+evWq630ok927du0a1DlkdURPkRTbkc+/WZVju5OJKPIdJxNXnGSCxrx58/RnOR5IbFcm3QAAgORj/3WIAAAAgCiQipqeJKHb6oqlZkF9WepQ2hWNmwyAAPDPWbEqmhX8E4lUxZHljj2TuWVA+eeff9aDhjfeeKOulJMcCdBSNVkGk6SK0SuvvKIrrvtz4sQJXb2sXLlyavr06VFvDwAAAAAAsSIJoT179vTaPnHiRJ0ouGXLFp0sKKpUqaIrwtsd1/ner6VnMre8lhLbHj9+vGrSpIl+H5glcyeS8+fPq48++sj1e9u2bU2LONiJJOJ7+u2332JeCKd27dpu2z7//HPT2KgkIstKEqJjx456EkEwzAojHTp0KOw2O9tgTN4vW7asigdmRResKtABAADsiYRuAAAAJASzwQoZ1Ni1a5eyUp48eby2sWwm/Ak1qX/06NFWN9n2zCZ2+Eo8btSoUcivgeyTksmg6cGDB9229enTRz366KMxbUfBggX16ghSLXznzp3qxRdf1INS/lYw6N69u3r11Vdj2k4AAAAAAKJJroUlodFIJl1LteIxY8a44h52r87tiet8pU6ePKlGjRrlVdH/q6++UoULF1Z2ZFXsbObMmbo6vbHytN3lypUrpFUDZZJDKM+rVGcPhsTxjC5fvqw++OADr/u9++67PvfxR1bmy5w5s9djCYe8xp6rkpYuXdrrO9CuihUr5rVtz549lrQFAADYEwndAAAASAj16tVThQoV8tr+5ZdfKrsldJ8+fdqStgCJyjjY45SUlGRJW+LRrFmzvLYNHz5cWUkGckaMGKE2bNigtm/frpdqlgpfZp544gm1bt26mLcRAAAAAIBoKFKkiOrSpYvX9pdffll9+umn+mepXtuuXTsVrxL1Ol8qInsW/5DKyGZJoYlOJjAYK0I3bNhQ2Z0k53u6dOlSzNsh7ynP5PIpU6a4FcGQpOOlS5f6rOrtjxTOqFy5sldlfX/J675s3brVa5u/iR52kzt3bq9tnqseAgCAxEZCNwAAABJCmjRpVO/evb22f/LJJ8pKN910k9e23bt3W9IWIJ7I4MbPP/+sb3/88UdExzpy5IjXtrx580Z0zEQhlbn37t3rNZBcokQJZRcyaP3CCy/odo4dO9ZrsOz69et6+WIAAAAAAOKVJDGnTp3aK0nw2rVrPv8erxLpOl/iXp7iIVE51lavXq2T/eOpOrfwrDRtVZGJTJkyqW7dunmNUXz33XemCd4PP/xwyOdo3bq117aNGzeGfBzj6+zv2NEkyefSVrn9888/ER3LLGE/Y8aMER0TAACkLCnjqg0AAAAIwoABA1TOnDndtjkDcVapW7euSp8+vdu2/fv3qzNnzoQdEJRAq/MW7tKFgN0dPnxY3Xrrrfomy+lGQpbu9VSzZs2IjplIr4OnAgUKhHwcY8Wf5CKDIzKA/fHHH3v9bfny5cl+fgAAAAAAkkuZMmVUhw4dTP9WtGhR9cADD6iUJhGu86MRd4lFzMVqb7/9tuvnrFmzmlast6NDhw55bcuXL58lbenTp4/XtsmTJ+v/X7lyRU2bNs31/Hbu3Dnk499zzz1e21asWBHxJAcZW7nzzjtVclfKr1atmr69+uqrER3LrCq5Va85AACwJxK6AQAAkDBkObtRo0Z5bR80aJCyilS/kIRUzyD7Dz/8ENbxZNnD22+/3XXbuXNnlFoK2LtKdLhkAsW+ffvctuXIkUPVqFEjCi1L+aI1KHjy5MmQ7v/nn3/qRH65hTpIK0tMV6lSJWCVdgAAAAAA4smTTz6pUqVK5bV9+PDhKl26dMrOrLjON3uuUmLcJdSYS7yR6smffvqp63dJNs6ePbuyu/Pnz6vNmzd7ba9Tp44l7SlXrpxq1KiRVyKzTCqYN2+eKxFZnt9s2bKFfPzy5curChUquG1btGhRSMe4evWqWrJkids2GQOJ5esdSRzaWU3ebNUBAAAAJxK6AQAAkFAeffRRrwTqH3/8Uc2aNcuyNj322GNe2+bOnRvWsYwVaSSBvXHjxhG1DYiFb7/9Vi+RK59DqfgSKhmk27FjR1jn/vDDD722tW3bVqVJkyas4yWaG264wTRJPhThrEogy5tOnz5d31auXKlC5TmAFA8DfQAAAAAA+HPTTTepVq1aeVV+feihh5TdWXGdnyFDBrffr1275vO+s2fPdt08CwPYPe5iljSckkjl6IsXL7p+79evn4oHkiQtCcqe7rjjDmWVvn37uv0u7Xv//fddlbrFww8/HPbxH3/8ca9VE7ds2RL0/gsXLtQJ/P6OGcznYdy4cfoxHTt2TIVKJpyEO9Hi8uXL+jF4Su4K4wAAIL6Q0A0AAICEkjp1ajVnzhxVpEgRr2ClVcFtCdjJgIuRVBUJNUF127Zt6rPPPnP9PmDAAJU2bdqotRNIDj179lTNmzfXVaTuv/9+VbduXXXhwoWQj2McWAiWBO0nTZrktk0+M0899VTIx0pUsmxz/vz5vQZhN27cGPQxwp3A4mup1WB4Dv6ULl06ojYAAAAAAGAHI0eO1HEW500m0GfMmFHFk1hd53smfUvFZDPnzp1T9913n+smSaixUqtWLa9tixcvDnp/ibGZJZCmFJJYa4wJSlyxcuXKyu4uXbqkXn75Za/tZcqUUc2aNVNWueeee/QkEKPXX39dLVu2zPV+rFatWtjHf+CBB1TFihXdtr3yyitB7+9535YtW3oV7/FHJoxINf9hw4bp8SCZCCJjKqGQuOcXX3yhwiHJ8YcOHXLbVqBAAYryAAAANyR0AwAAIOFIkEwC38bg5KlTp3Tg7Ndff43o2P/9958aMWJESPvI8p5SJdhYFUaqFPfq1UsHd4MNznft2tVVSSYpKcm08jdgJ0uXLtWBbKO1a9eql156KeRjvfPOO6ZLVvoin5XevXt7VWKRKv6lSpUK+fyJSr6/ZLDH06hRo4IeBAll4MbMN998E1I1n+PHj6sVK1a4bbv33nsjagMAAAAAAHZQo0YNfZ3svD344IMq3sTqOt8z/nPw4EHT++3atcvt90KFCqlYad26tUqXLp3btq+//lqtW7cuqP0lof/ff/9VKTm2aHx94qE6t1S9lirXf/zxh9ffJkyY4PV6x5Kcu0ePHm7bjLHTSKpzO4vtvPrqq27bZsyYoX766aeA+8r4iXHsRtoaSgxZvick7nv9+nXXtqNHj6o+ffqoUA0ePFgdPnw4pH1+//13NXToUK/tktjvuVoAAABIbCR0AwAAICFJJQgJ9JctW9YtONmwYUP1xBNPqJMnT4Z0vL/++ks9++yzqmTJkqaBxECB2OrVq+tqF0YSyGzfvn3Athw5ckRX+ZZEWGOCeK5cuUJ6DIAVgy5mlixZEvKxZGlVWVb422+/DWriRadOnfTSpka33357xMnFiUgmsXhW+5o/f75e8tTfEqTy3SUDk5EOLEpy/l133aV2794d1OQXqQZkrLolKzZEOiAFAAAAAACiI1bX+bfccovb75s2bdLVuD3Nnj3b9XPmzJlV1apVVawULlzY67FIQqpMrg9UWXjq1KlqzJgxKiWTAg9OefPm1bF0O/vhhx9UgwYNdKVos+R7iW1aTQpgSOK1p5w5c+p4aqSkqraxEI3EDuV1k8+fL99//71Xsv64ceNCqsYuhUBOnz5tuiKAr+r8vuzfv181atQo6IkVUiVf7u95no4dO+rvLwAAACPWXwcAAEDCkiUMJej2yCOP6ARoZ4UMqYrw5ptv6kouEmCUZOvixYur9OnT6/ucOXNGV3SQKhqSRC2B2F9++cU0cVGC/D179jStvuBJgvMSLJXgpLTDWXHlxhtvVAMGDFB33323rhwjx5TK3XL+r776Sr399tuuZUVl/0mTJukEb1+kkoWxmoVZVXIJiBoZ2y9VcjZv3uz6/cCBA273ld+N+1eqVEm1aNHC57HN2me8jyyVKTch1XmMy4qaJbtL+4yVQ4znl7bNmTPH9TezgSH5uzM53hlYlYEg42CIVHS3ov3JRYLZU6ZMMf2b5+vrfA7MJgwYH2swfCX7+ksC9keeN3muZEnhbt266bZI1aQ0adKos2fPqh07dujP1FtvvaUrsBhJYvHHH3+s7xsL8j4zPrdm7wV5Xcw+L9F6T3h+HpznNJL3rLEN8lmQz4SRbJNK6zIAYXztJDleJqZI1RoZtJCBNRl0lM+dJNPLceV1kKVOpQqW8X3ved5Aj1mOKYM4UkWobdu2enDV+R69fPmyfu1lAsEbb7yhJ+AYl1eWgdls2bJ5HdPz82r8XvD1WZBBL+OSzcbHEMznXf7N8Vx6FgAAAACQcpldmxurU3teN4pg4ozJHZ8wxoDM4kqB4gs5cuTQqxPG8jrfkySnDh8+3LVCoiSHS5uknQULFtQxYIkZjx8/3rVP9+7dXTFiz8cdzHV/OHEdSfRduXKlW/KovGY1a9bUMWOJ08gxpaCItFni1BLbliRSiRXLqpTLli0LOp7h+Vp7Vks3ez9E8z0ZrL///lvH+ZzkveJ8baLBMwZsFkf2jIN7kveWHGPPnj1q1apVplXg5bMgMTR5DexAxkHk/blo0SK37V26dNHjEtEgnymJCc6cOVP/LsUeZILFsGHD9HlkDERijPLek9dBxj+c4yVCCvJIte1Q+ItDhxOLlu+g2rVrqzvuuEPdd999+meZgCHvQVl99dChQzouKt8hMn7kqUOHDjoWLcV5AAAA3DgAAAAAOH7++WdH48aNJXLn85YhQwZH2rRp/d7HeatWrZpjzJgxjqNHj4bclu+++85Rrlw5v+0w216oUCHH/PnzAx5/1KhRQT0G482oW7duIe0r9zcK9dzSXqdp06aFvL/x/MuWLQt5f9nHqFixYpa1P7ns3bs35HYFeqzBWLRokelxnn766aD2/+GHHxwdOnRwpE+f3m+7/P09R44cjnHjxjmuX7/uiKWGDRuG/TxH6z0RzudB2u3Lxx9/7MiaNavPfeX7M3Xq1G7bWrZs6Th16lTAz5XnY/7nn38cPXr0cGTJksXnPmnSpHFkzJjR59+rVKni2Lhxo8/HE87nVT5LRqHuL+cEAAAAACSOcK7N7RCfMMaAwokrSRwg1tf5ZiZOnBh0/LVs2bKOkydPRvS4zWIcwZAYc7NmzUKKf2XPnt3x+eefBxXfMMYzwnlMVnjmmWdc55d4k2dMJlKhxoBDveXPn98xcOBAx5EjRxx2I2MMnu3dvHlzVM8hsdjnnnvOkS5dOtMYonzePbfL98OUKVPCOt+///5r+v1Sv379gPtevHjRMWPGDEfdunUDvq6+xm6ctxtuuMExc+bMsB4DAABIDN5rpQAAAAAJqF69erpSwu+//64ef/xxXSHVszqCVNQwVoJwkkonpUuX1ktdvvbaa7oqzPr169WTTz6pkpKSQm5L06ZNdQVsqfAilW48KwY7q8Y4VahQQVcCkWU2pcowEC+kGrFnNSapiD9ixIig9pcKQ1JJSiqeyFKlUtkkT548XveTyk1G8pmqUaOG/txIJZ0hQ4ZQDSUKOnfurJdH7dOnj8qSJYvX3+X7Uyp0y3emfLctWLBAV/sxVrQOVr58+XRV8MOHD6uPPvpIV6PKnTu31xLNFy9edNsm55Zq4fJ+kcpWUh0cAAAAAABYy6rr/IEDB6oZM2aoEiVK+Iy/ZsiQQVd/lgrLOXPmVFaQGLNU+5a2VqtWzfQ+zviXrJD22GOP6YrSUt08JZIY03vvvecWY5TK0nYjVdPlfVy0aFEd85TYmVRc//HHH3W17okTJ6obbrhB2Y2s/iltdqpfv37UV5STWOwzzzyjY4ldu3Z1q/4tr6983p3kc9e/f389/uGvsr8/8rmQyvXymjhJHHny5MkB95XvAFmZUKrf//nnnzqm3KRJE9OK5Z5jN0LipM2aNdMVyfft26fuv//+sB4DAABIDKkkq9vqRgAAAAB29N9//+nE6r179+olAGXZTRkkkORD561QoUI6oTpayw2akeU6ZcDgn3/+0e2QhEgJYsoSfpKUmj9//mQ7NxALy5cvV7/99ptezrZdu3YRL48qAyLy2ZX/y+f4/Pnz+vMqAyhyjlq1aqmsWbNGrf3wJkuLrl27Vm3dulWdOHFCD8LIIEmBAgX0BBrjsr7RtH//fr0cqyy7K0sAy/e2DJrId2aZMmX0wG4wyy4DAAAAAADrxeo6X+Ktkli6YcMGdfz4cZ2UKbEkKeJxyy23WJbI7YsUN1i5cqVOgD916pSOc0nSt8Spq1atSuECxB35bK9evVonTMt4iDMJWxLJa9asqdKmTRuV80hS+HfffacLfrRv316fI5LvDRk7kmMeOXJEf0edO3dOZcqUyRWLls9kuXLl9LgSAABAMEjoBgAAAAAAAAAAAAAAAAAAAACLMA0MAAAAAAAAAAAAAAAAAAAAACxCQjcAAAAAAAAAAAAAAAAAAAAAWISEbgAAAAAAAAAAAAAAAAAAAACwCAndAAAAAAAAAAAAAAAAAAAAAGAREroBAAAAAAAAAAAAAAAAAAAAwCIkdAMAAAAAAAAAAAAAAAAAAACARUjoBgAAAAAAAAAAAAAAAAAAAACLkNANAAAAAAAAAAAAAAAAAAAAABYhoRsAAAAAAAAAAAAAAAAAAAAALEJCNwAAAAAAAAAAAAAAAAAAAABYhIRuAAAAAAAAAAAAAAAAAAAAALAICd0AAAAAAAAAAAAAAAAAAAAAYBESugEAAAAAAAAAAAAAAAAAAADAIiR0AwAAAAAAAAAAAAAAAAAAAIBFSOgGAAAAAAAAAAAAAAAAAAAAAIuQ0A0AAAAAAAAAAAAAAAAAAAAAFiGhGwAAAAAAAAAAAAAAAAAAAAAsQkI3AAAAAAAAAAAAAAAAAAAAAFiEhG4AAAAAAAAAAAAAAAAAAAAAsAgJ3QAAAAAAAAAAAAAAAAAAAABgERK6AQAAAAAAAAAAAAAAAAAAAMAiJHQDAAAAAAAAAAAAAAAAAAAAgEVI6AYAAAAAAAAAAAAAAAAAAAAAi5DQDQAAAAAAAAAAAAAAAAAAAAAWIaEbAAAAAAAAAAAAAAAAAAAAACxCQjcAAAAAAAAAAAAAAAAAAAAAWISEbgAAAAAAAAAAAAAAAAAAAACwCAndAAAAAAAAAAAAAAAAAAAAAGAREroBAAAAAAAAAAAAAAAAAAAAwCIkdAMAAAAAAAAAAAAAAAAAAACARUjoBgAAAAAAAAAAAAAAAAAAAACLkNANAAAAAAAAAAAAAAAAAAAAABYhoRsAAAAAAAAAAAAAAAAAAAAALEJCNwAAAAAAAAAAAAAAAAAAAABYhIRuAAAAAAAAAAAAAAAAAAAAALAICd0AAAAAAAAAAAAAAAAAAAAAYBESugEAAAAAAAAAAAAAAAAAAADAIiR0AwAAAAAAAAAAAAAAAAAAAICyxv8BIJ/XzaR2TrcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 3000x2400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT: ANALYSIS EXCLUDING UNVERIFIABLE (BASELINE & PIPELINE SEPARATELY)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from curlyBrace import curlyBrace\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "# Calculate ordinal metrics using existing MSE columns\n",
    "baseline_pass1_values = gemini_df['baseline_pass1_MSE'].dropna().tolist()\n",
    "pipeline_pass1_values = gemini_df['pipeline_pass1_MSE'].dropna().tolist()\n",
    "baseline_pass3_values = gemini_df['baseline_pass3_MSE'].dropna().tolist()\n",
    "pipeline_pass3_values = gemini_df['pipeline_pass3_MSE'].dropna().tolist()\n",
    "\n",
    "gemini_ordinal_metrics = {\n",
    "    'baseline_pass@1_mse': np.mean(baseline_pass1_values) if baseline_pass1_values else None,\n",
    "    'pipeline_pass@1_mse': np.mean(pipeline_pass1_values) if pipeline_pass1_values else None,\n",
    "    'baseline_pass@3_mse': np.mean(baseline_pass3_values) if baseline_pass3_values else None,\n",
    "    'pipeline_pass@3_mse': np.mean(pipeline_pass3_values) if pipeline_pass3_values else None\n",
    "}\n",
    "print(f\"(Baseline: n={len(baseline_pass1_values)}/{len(gemini_df)}, excluded {len(gemini_df)-len(baseline_pass1_values)} unverifiable)\",\n",
    "        f\"(Pipeline: n={len(pipeline_pass1_values)}/{len(gemini_df)}, excluded {len(gemini_df)-len(pipeline_pass1_values)} unverifiable)\")\n",
    "print(f\"(Baseline: n={len(baseline_pass3_values)}/{len(gemini_df)}, excluded {len(gemini_df)-len(baseline_pass3_values)} unverifiable)\",\n",
    "        f\"(Pipeline: n={len(pipeline_pass3_values)}/{len(gemini_df)}, excluded {len(gemini_df)-len(pipeline_pass3_values)} unverifiable)\")\n",
    "\n",
    "# Calculate ordinal metrics using existing MSE columns\n",
    "baseline_pass1_values = mistral_df['baseline_pass1_MSE'].dropna().tolist()\n",
    "pipeline_pass1_values = mistral_df['pipeline_pass1_MSE'].dropna().tolist()\n",
    "baseline_pass3_values = mistral_df['baseline_pass3_MSE'].dropna().tolist()\n",
    "pipeline_pass3_values = mistral_df['pipeline_pass3_MSE'].dropna().tolist()\n",
    "\n",
    "mistral_ordinal_metrics = {\n",
    "    'baseline_pass@1_mse': np.mean(baseline_pass1_values) if baseline_pass1_values else None,\n",
    "    'pipeline_pass@1_mse': np.mean(pipeline_pass1_values) if pipeline_pass1_values else None,\n",
    "    'baseline_pass@3_mse': np.mean(baseline_pass3_values) if baseline_pass3_values else None,\n",
    "    'pipeline_pass@3_mse': np.mean(pipeline_pass3_values) if pipeline_pass3_values else None\n",
    "}\n",
    "\n",
    "print(f\"(Baseline: n={len(baseline_pass1_values)}/{len(mistral_df)}, excluded {len(mistral_df)-len(baseline_pass1_values)} unverifiable)\",\n",
    "    f\"(Pipeline: n={len(pipeline_pass1_values)}/{len(mistral_df)}, excluded {len(mistral_df)-len(pipeline_pass1_values)} unverifiable)\")\n",
    "print(f\"(Baseline: n={len(baseline_pass3_values)}/{len(mistral_df)}, excluded {len(mistral_df)-len(baseline_pass3_values)} unverifiable)\",\n",
    "        f\"(Pipeline: n={len(pipeline_pass3_values)}/{len(mistral_df)}, excluded {len(mistral_df)-len(pipeline_pass3_values)} unverifiable)\")\n",
    "\n",
    "# Plot results and show % improvement (from baseline to pipeline, for both pass@1 and pass@3)\n",
    "fig, axes = plt.subplots(1, 1, figsize = (10, 8), dpi = 300)\n",
    "\n",
    "# Define fall colors\n",
    "colors = ['#D35400', '#E67E22']  # Deep orange and lighter orange\n",
    "bar_width = 0.35\n",
    "index = np.arange(4)  # Two groups for Pass@1 and Pass@3\n",
    "\n",
    "# Create bars\n",
    "baseline_bars = axes.bar(index - bar_width/2, \n",
    "                       [gemini_ordinal_metrics['baseline_pass@1_mse'], gemini_ordinal_metrics['baseline_pass@3_mse'], \n",
    "                        mistral_ordinal_metrics['baseline_pass@1_mse'], mistral_ordinal_metrics['baseline_pass@3_mse'], ], \n",
    "                       bar_width, label='Baseline', color=colors[0])\n",
    "pipeline_bars = axes.bar(index + bar_width/2, \n",
    "                       [gemini_ordinal_metrics['pipeline_pass@1_mse'], gemini_ordinal_metrics['pipeline_pass@3_mse'],\n",
    "                        mistral_ordinal_metrics['pipeline_pass@1_mse'], mistral_ordinal_metrics['pipeline_pass@3_mse']], \n",
    "                       bar_width, label='Pipeline', color=colors[1])\n",
    "\n",
    "# Customize the plot\n",
    "axes.set_ylabel('Mean Squared Error (lower is better)', fontsize=14)\n",
    "axes.yaxis.set_tick_params(labelsize=16)\n",
    "axes.set_title(f'Mean Squared Error (MSE) Comparison: Baseline vs Pipeline', fontsize=16, pad=15)\n",
    "axes.set_xticks(index, [\"Pass@1\", \"Pass@3\", \"Pass@1\", \"Pass@3\"], fontsize=14)\n",
    "axes.text(0.5, -0.3, 'Gemini-1.5-Flash', fontsize = 12, ha='center')\n",
    "axes.annotate('', xy=(0.85, -0.27), xytext=(1, -0.15), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "axes.annotate('', xy=(0.15, -0.27), xytext=(0, -0.15), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "\n",
    "axes.text(2.5, -0.3, 'Mistral-7B-v0.3', fontsize = 12, ha='center')\n",
    "axes.annotate('', xy=(2.85, -0.27), xytext=(3, -0.15), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "axes.annotate('', xy=(2.15, -0.27), xytext=(2, -0.15), textcoords='data', annotation_clip=False,\n",
    "            arrowprops=dict(arrowstyle='-', connectionstyle='angle,angleA=90,angleB=0,rad=10'))\n",
    "\n",
    "axes.legend(fontsize = 16)\n",
    "\n",
    "# Add improvement arrows and percentages\n",
    "for idx, (baseline_val, pipeline_val) in enumerate([\n",
    "    (gemini_ordinal_metrics['baseline_pass@1_mse'], gemini_ordinal_metrics['pipeline_pass@1_mse']),\n",
    "    (gemini_ordinal_metrics['baseline_pass@3_mse'], gemini_ordinal_metrics['pipeline_pass@3_mse']),\n",
    "    (mistral_ordinal_metrics['baseline_pass@1_mse'], mistral_ordinal_metrics['pipeline_pass@1_mse']),\n",
    "    (mistral_ordinal_metrics['baseline_pass@3_mse'], mistral_ordinal_metrics['pipeline_pass@3_mse']),\n",
    "]): \n",
    "    # curlyBrace(fig, axes, [idx+0.05, baseline_val - 0.03], [idx+0.05, pipeline_val + 0.03], 0.03, bool_auto=True, color = 'black', lw=1, int_line_num=1)\n",
    "    improvement = ((baseline_val - pipeline_val) / baseline_val * 100)\n",
    "    # Draw arrow\n",
    "    mid_height = (baseline_val + pipeline_val) / 2\n",
    "    axes.annotate(\n",
    "        f'{improvement:.1f}%\\nimprovment', \n",
    "        xy=(idx + 0.1, mid_height),\n",
    "        xytext=(idx + 0.073, mid_height - 0.1),\n",
    "        fontsize=12,\n",
    "    )\n",
    "    \n",
    "    # Draw arrow\n",
    "    axes.annotate(f'',\n",
    "                xy=(idx + 0.03, pipeline_val),\n",
    "                xytext=(idx + 0.03, baseline_val - 0.05),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                color='green',\n",
    "                fontweight='bold',\n",
    "                arrowprops=dict(\n",
    "                              color='darkolivegreen',\n",
    "                              lw = 1.5,\n",
    "                              connectionstyle='arc3,rad=0'))\n",
    "# Add grid for better readability\n",
    "axes.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mse.png', dpi = 500, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factchecker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
